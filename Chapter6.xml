<!-- Converted by db4-upgrade version 1.0 -->
<chapter xmlns="http://docbook.org/ns/docbook" version="5.0" xml:id="chapter-7" label="7">
    <title>Categorization: Describing Resource Classes and Types</title>
    <info>
        <author>
            <personname><firstname>Robert</firstname><othername>J.</othername><surname>Glushko</surname></personname>
        </author>
        <author>
            <personname><firstname>Rachelle</firstname><surname>Annechino</surname></personname>
        </author>
        <author>
            <personname><firstname>Jess</firstname><surname>Hemerly</surname></personname>
        </author>
        <author>
            <personname><firstname>Robyn</firstname><surname>Perry</surname></personname>
        </author>
        <author>
            <personname><firstname>Longhao</firstname><surname>Wang</surname></personname>
        </author>
        <keywordset>
            <keyword>equivalence classes</keyword>
            <keyword>cultural categories</keyword>
            <keyword>individual categories</keyword>
            <keyword>institutional categories</keyword>
            <keyword>maintenance</keyword>
            <keyword>governance</keyword>
            <keyword>intensional definition</keyword>
            <keyword>tagging</keyword>
            <keyword>classical categories</keyword>
            <keyword>property-based categories</keyword>
            <keyword>typicality</keyword>
            <keyword>centrality</keyword>
            <keyword>family resemblance</keyword>
            <keyword>similarity</keyword>
            <keyword>theory-based categories</keyword>
            <keyword>validation</keyword>
            <keyword>data schemas</keyword>
            <keyword>machine learning</keyword>
        </keywordset>
    </info>

    <!--              -->
    <section xml:id="section-7.1" label="7.1">
        <title>Introduction</title>
        <info>
            <itermset>
                <indexterm zone="section-7.1">
                    <primary>categories</primary>
                </indexterm></itermset>
        </info>


        <sidebar xml:id="ch06-stats" userlevel="Editor" condition="epub3 print" role="statistics">
            <?dbhtml sidebar-width="60%"?>
            <?dbhtml float-type="right"?>
            <?dbfo sidebar-width="60%"?>
            <?dbfo float-type="inside"?>
            <informalfigure xml:id="inf-fig-ch06-stats">
                <mediaobject>
                    <imageobject>
                        <imagedata fileref="Visualisations/ch06-stats.png" format="JPG"/>
                    </imageobject>
                    <textobject>
                        <phrase role="ALT descriptive">This graphic describes the content breakdown
                            of the chapter. A wheel with colored segments depicts core content
                            versus disciplinary content in this chapter, and a bar chart illustrates
                            the disciplinary content distribution. In this chapter CogSci notes
                            predominate by a wide margin, followed by Computing, Business,
                            Linguistics, Philosophy, Law, LIS, and Archives. There are no IA,
                            Museums, or Web notes in this chapter.</phrase>
                    </textobject>
                </mediaobject>
            </informalfigure>
        </sidebar>


        <para audience="CORE" xml:id="para-eky_l4r_lr">For nearly two decades, a TV game show called
                <citetitle>Pyramid</citetitle> aired in North America. The show featured two
            competing teams, each team consisting of two contestants: an ordinary civilian
            contestant and a celebrity. In the show’s first round, both teams’ members viewed a
            pyramid-shaped sign that displayed six category titles, some straightforward like
                <quote>Where You Live</quote> and others less conventional like <quote>Things You
                Need to Feed.</quote> Each team then had an opportunity to compete for points in
            30-second turns. The goal was for one team member to gain points by identifying a word
            or phrase related to the category from clues provided by the other team member. For
            example, a target phrase for the <quote>Where You Live</quote> category might be
                <quote>zip code,</quote> and the clue might be <quote>Mine is 94705.</quote>
            <quote>Things you Need to Feed</quote> might include both <quote>screaming baby</quote>
            and <quote>parking meter.</quote></para>
        <para audience="CORE" xml:id="para-xly_l4r_lr">The team that won the first round advanced to
            the <quote>Winner’s Circle,</quote> where the game was turned around. This time, only
            the clue giver was shown the category name and had to suggest concepts or instances
            belonging to that category so that the teammate could guess the category name. Clues
            like <quote>alto,</quote>
            <quote>soprano,</quote> and <quote>tenor</quote> would be given to prompt the teammate
            to guess <quote>Singing Voices</quote> or <quote>Types of Singers.</quote></para>
        <para audience="CORE" xml:id="para-nny_l4r_lr">As the game progressed, the categories became
            more challenging. It was interesting and entertaining to hear the clue receiver’s
            initial guess and how subsequent guesses changed with more clues. The person giving
            clues would often become frustrated, because to them their clues seemed obvious and
            discriminating but would seem not to help the clue receivers in identifying the
            category. Viewers enjoyed sharing in these moments of vocabulary and category
            confusion.</para>
        <para audience="CORE" xml:id="para-w4y_l4r_lr"><info><itermset>
            <indexterm zone="para-w4y_l4r_lr">
                <primary>categorization</primary>
                <secondary>Pyramid</secondary>
            </indexterm><indexterm zone="para-w4y_l4r_lr">
                <primary>CBS</primary>
            </indexterm></itermset>
        </info>The <citetitle>Pyramid</citetitle> TV game show developers created a
            textbook example for teaching about categories<symbol>&#8212;</symbol>groups or classes
            of things, people, processes, events or anything else that we treat as
                equivalent<symbol>&#8212;</symbol>and categorization<symbol>&#8212;</symbol>the
            process of assigning instances to categories. The game is a useful analog for us to
            illustrate many of the issues we discuss in this chapter. The Pyramid game was
            challenging, and sometimes comical, because people bring their own experiences and
            biases to understanding what a category means, and because not every instance of a
            category is equally typical or suggestive. How we organize reflects our thinking
            processes, which can inadvertently reveal personal characteristics that can be amusing
            in a social context.  Hence, the popularity of the <citetitle>Pyramid</citetitle> franchise,
            which began on <abbrev>CBS</abbrev> in <date>1973</date> and has been produced in 20
            countries.</para>
        
        <para audience="CORE" xml:id="para-eqy_l4r_lr"><info>
                <itermset>
                    <indexterm zone="para-eqy_l4r_lr">
                        <primary>cataloging</primary>
                        <secondary>rules</secondary>
                    </indexterm>
                    <indexterm zone="para-eqy_l4r_lr">
                        <primary>categorization</primary>
                        <secondary>category rules</secondary>
                    </indexterm>
                </itermset>
            </info>Many texts in library science introduce categorization via cataloging rules, a
            set of highly prescriptive methods for assigning resources to categories that some
            describe and others satirize as <quote>mark ’em and park ’em.</quote> Many texts in
            computer science discuss the process of defining the categories needed to create,
            process, and store information in terms of programming language constructs:
                <quote>here’s how to define an abstract type, and here’s the data type
                system.</quote> Machine learning and <glossterm linkend="gloss_data-science">data
                science</glossterm> texts explain how categories are created through statistical
            analysis of the correlations among the values of features in a collection or dataset. We
            take a very different approach in this chapter, but all of these different perspectives
            will find their place in it.<footnote xml:id="endnote-328" label="386" audience="CogSci">
                <para audience="CogSci" xml:id="para-iry_l4r_lr"><info>
                        <itermset>
                            <indexterm audience="Markup" zone="endnote-328">
                                <primary>endnote</primary>
                                <secondary>CogSci</secondary>
                            </indexterm>
                            <indexterm zone="endnote-328">
                                <primary>categories</primary>
                                <secondary>psychological and linguistic</secondary>
                            </indexterm>
                            <indexterm zone="endnote-328">
                                <primary>cognitive science</primary>
                                <secondary>linguistic categories</secondary>
                            </indexterm>
                            <indexterm zone="endnote-328">
                                <primary>cognitive science</primary>
                                <secondary>psychological categories</secondary>
                            </indexterm>
                            <indexterm zone="endnote-328">
                                <primary>cataloging</primary>
                                <secondary>cognition</secondary>
                            </indexterm>
                        </itermset>
                    </info>Cataloging and programming are important activities that need to be done
                    well, and prescriptive advice is often essential. However, we believe that
                    understanding how people create psychological and linguistic categories can help
                    us appreciate that cataloging and information systems design are messier and
                    more intellectually challenging activities than we might otherwise think.</para>
            </footnote></para>
        <important role="editorial" xml:id="nav-ch06">
            <?dbfo float-type="none"?>
            <title>Navigating This Chapter</title>
            <para xml:id="para-qsy_l4r_lr">In the following sections, we discuss how and why we
                create categories, reviewing some important work in philosophy, linguistics, and
                cognitive psychology to better understand how categories are created and used in
                organizing systems. We discuss how the way we organize differs when we act as
                individuals or as members of social, cultural, or institutional groups<phrase
                    role="parenthetical"> (<xref linkend="section-7.2" xrefstyle="short"
                />)</phrase>; later we share principles for creating categories<phrase
                    role="parenthetical">( <xref linkend="section-7.3" xrefstyle="short"
                />)</phrase>, design choices<phrase role="parenthetical"> (<xref
                        linkend="section-7.4" xrefstyle="short"/>)</phrase>, and implementation
                    experience<phrase role="parenthetical"> (<xref linkend="section-7.5"
                        xrefstyle="short"/>)</phrase>. Throughout the chapter, we will compare how
                categories created by people compare with those created by computer algorithms. As
                usual, we close the chapter with a summary of the key points<phrase
                    role="parenthetical"> (<xref linkend="section-7.6" xrefstyle="short"
                />)</phrase>. </para>
        </important>
    </section>
    <!--              -->
    <section xml:id="section-7.2" label="7.2">
        <title>The What and Why of Categories</title>
        <info>
            <itermset>
                <indexterm zone="section-7.2">
                    <primary>categories</primary>
                    <secondary>motivation</secondary>
                </indexterm></itermset>
        </info>

        <para audience="CORE" xml:id="para-h5y_l4r_lr"><phrase role="definition"
                xml:id="def_categories"><glossterm xml:id="term_category">Categories</glossterm> are
                    <firstterm xml:id="first_equivalence_class" linkend="gloss_equivalence_class"
                    >equivalence classes</firstterm>, sets or groups of things or abstract entities
                that we treat the same.</phrase> This does not mean that every instance of a
            category is identical, only that from some perspective, or for some purpose, we are
            treating them as equivalent based on what they have in common. When we consider
            something as a member of a category, we are making choices about which of its properties
            or roles we are focusing on and which ones we are ignoring. We do this automatically and
            unconsciously most of the time, but we can also do it in an explicit and self-aware way.
            When we create categories with conscious effort, we often say that we are creating a
            model, or just modeling. You should be familiar with the idea that a model is a set of
            simplified descriptions or a physical representation that removes some complexity to
            emphasize some features or characteristics and to de-emphasize others.<footnote
                xml:id="endnote-329" label="387" audience="CogSci">
                <para audience="CogSci" xml:id="para-qvy_l4r_lr"><info>
                        <itermset>
                            <indexterm audience="Markup" zone="endnote-329">
                                <primary>endnote</primary>
                                <secondary>CogSci</secondary>
                            </indexterm>
                            <indexterm zone="endnote-329">
                                <primary>cognitive science</primary>
                                <secondary>category knowledge</secondary>
                            </indexterm>
                        </itermset>
                    </info>Cognitive science mostly focuses on the automatic and unconscious
                    mechanisms for creating and using categories. This disciplinary perspective
                    emphasizes the activation of category knowledge for the purpose of making
                    inferences and <quote>going beyond the information given,</quote> to use
                    Bruner’s classic phrase <citation xml:id="cite_Bruner1957" linkend="Bruner1957"
                        >(Bruner 1957)</citation>. In contrast, the discipline of organizing focuses
                    on the explicit and self-aware mechanisms for creating and using categories
                    because by definition, organizing systems serve intentional and often highly
                    explicit purposes. <phrase role="statement">Organizing systems facilitate
                        inferences about the resources they contain, but the more constrained
                        purposes for which resources are described and arranged makes inference a
                        secondary goal. </phrase></para>
                <para audience="CogSci" xml:id="para-wwy_l4r_lr">Cognitive science is also highly
                    focused on understanding and creating computational models of the mechanisms for
                    creating and using categories. These models blend data-driven or bottom-up
                    processing with knowledge-driven or top-down processing to simulate the time
                    course and results of categorization at both fine-grained scales (as in word or
                    object recognition) and over developmental time frames (as in how children learn
                    categories). The discipline of organizing can learn from these models about the
                    types of properties and principles that organizing systems use, but these
                    computational models are not a primary concern to us in this book.</para>
            </footnote></para>
        <para audience="CORE" role="comparative" xml:id="para-gyy_l4r_lr"><phrase role="statement"
                >When we encounter objects or situations, recognizing them as members of a category
                helps us know how to interact with them.</phrase> For example, when we enter an
            unfamiliar building we might need to open or pass through an entryway that we recognize
            as a door. We might never have seen that particular door before, but it has properties
            and affordances that we know that all doors have; it has a doorknob or a handle; it
            allows access to a larger space; it opens and closes. By mentally assigning this
            particular door to the <quote>doors</quote> category we distinguish it from
                <quote>windows,</quote> a category that also contains objects that sometimes have
            handles and that open and close, but which we do not normally pass through to enter
            another space. Categorization judgments are therefore not just about what is included in
            a class, but also about what is excluded from a class. Nevertheless, the category
            boundaries are not sharp; a <quote>Dutch door</quote> is divided horizontally in half so
            that the bottom can be closed like a door while the top can stay open like a
            window.</para>
        
        <para audience="CORE" xml:id="para-uzy_l4r_lr"><phrase role="statement">Categories are
                    <emphasis>cognitive and linguistic models</emphasis> for applying prior
                knowledge; creating and using categories are essential human activities. Categories
                enable us to relate things to each other in terms of similarity and dissimilarity
                and are involved whenever we perceive, communicate, analyze, predict, or classify.
                Without categories, we would perceive the world as an unorganized blur of things
                with no understandable or memorable relation to each other.</phrase> Every
            wall-entry we encounter would be new to us, and we would have to discover its properties
            and supported interactions as though we had never before encountered a door. Of course,
            we still often need to identify something as a particular instance, but categories
            enable us to understand how it is equivalent to other instances. We can interchangeably
            relate to something as specific as <quote>the wooden door to the main conference
                room</quote> or more generally as <quote>any door.</quote></para>
        
        
        <para audience="CogSci Linguistics" xml:id="para-ebz_l4r_lr"><phrase role="statement">All
                human languages and cultures divide up the world into categories.</phrase>
            <phrase role="statement">How and why this takes place has long been debated by
                philosophers, psychologists and anthropologists.</phrase> One explanation for this
            differentiation is that people recognize structure in the world, and then create
            categories of things that <quote>go together</quote> or are somehow similar. An
            alternative view says that human minds make sense of the world by imposing structure on
            it, and that what goes together or seems similar is the outcome rather than a cause of
            categorization. Bulmer framed the contrast in a memorable way by asking which came
            first, the chicken (the objective facts of nature) or the egghead (the role of the human
                intellect).<footnote xml:id="endnote-330" label="388" audience="CogSci">
                <para audience="CogSci" xml:id="para-ncz_l4r_lr"><info>
                        <itermset>
                            <indexterm audience="Markup" zone="endnote-330">
                                <primary>endnote</primary>
                                <secondary>CogSci</secondary>
                            </indexterm>
                            <indexterm zone="endnote-330">
                                <primary>cognitive science</primary>
                                <secondary>Bulmer’s chicken</secondary>
                            </indexterm>
                        </itermset>
                    </info>However, even the way this debate has been framed is a bit controversial.
                    Bulmer’s chicken, the <quote>categories are in the world</quote> position, has
                    been described as empirical, environment-driven, bottom-up, or objectivist, and
                    these are not synonymous. Likewise, the <quote>egghead</quote> position that
                        <quote>categories are in the mind</quote> has been called rational,
                    constructive, top-down, experiential, and embodied<symbol>&#8212;</symbol>and
                    they are also not synonyms. See <citation xml:id="cite_Bulmer1970"
                        linkend="Bulmer1970">(Bulmer 1970)</citation>. See also <citation
                        xml:id="cite_Lakoff1990" linkend="Lakoff1990">(Lakoff 1990)</citation>,
                        <citation xml:id="cite_Malt1995" linkend="Malt1995">(Malt
                    1995)</citation>.</para>
            </footnote></para>
        <para audience="CogSci Linguistics Computing" xml:id="para-xdz_l4r_lr">A secondary and more
            specialized debate going on for the last few decades among linguists, cognitive
            scientists, and computer scientists concerns the extent to which the cognitive
            mechanisms involved in category formation are specialized for that purpose rather than
            more general learning processes.<footnote xml:id="endnote-331" label="389"
                audience="CogSci">
                <para audience="CogSci" xml:id="para-efz_l4r_lr"><info>
                        <itermset>
                            <indexterm audience="Markup" zone="endnote-331">
                                <primary>endnote</primary>
                                <secondary>CogSci</secondary>
                            </indexterm>
                            <indexterm zone="endnote-331" userlevel="Professional Graduate">
                                <primary>Chomsky, Noam</primary>
                            </indexterm>
                            <indexterm zone="endnote-331">
                                <primary>cognitive science</primary>
                                <secondary>Chomsky</secondary>
                            </indexterm>
                            <indexterm zone="endnote-331">
                                <primary>cognitive science</primary>
                                <secondary>universal grammer</secondary>
                            </indexterm>
                            <indexterm zone="endnote-331">
                                <primary>constraints</primary>
                                <secondary>schema</secondary>
                            </indexterm>
                        </itermset>
                    </info><phrase role="interrogative">Is there a <quote>universal grammar</quote>
                        or a <quote>language faculty</quote> that imposes strong constraints on
                        human language and cognition?</phrase>
                    <citation xml:id="cite_Chomsky1965" linkend="Chomsky1965">(Chomsky
                        1965)</citation> and <citation xml:id="cite_Jackendoff1996"
                        linkend="Jackendoff1996">(Jackendoff 1996)</citation> think so. Such
                    proposals imply cognitive representations in which categories are explicit
                    structures in memory with associated instances and properties. In contrast,
                    generalized learning theories model category formation as the adjustment of the
                    patterns and weighting of connections in neural processing networks that are not
                    specialized for language in any way. Computational simulations of semantic
                    networks can reproduce the experimental and behavioral results about language
                    acquisition and semantic judgments that have been used as evidence for explicit
                    category representations without needing anything like them. <citation
                        xml:id="cite_Rogers2008" linkend="Rogers2008">(Rogers and McClelland
                        2008)</citation> thoroughly review the explicit category models and then
                    show how relatively simple learning models can do without them.</para>
            </footnote></para>
        <para audience="CORE" xml:id="para-ngz_l4r_lr">Even before they can talk, children behave in
            ways that suggest they have formed categories based on shape, color, and other
            properties they can directly perceive in physical objects.<footnote xml:id="endnote-332"
                label="390" audience="CogSci">
                <para audience="CogSci" xml:id="para-vhz_l4r_lr"><info>
                        <itermset>
                            <indexterm audience="Markup" zone="endnote-332">
                                <primary>endnote</primary>
                                <secondary>CogSci</secondary>
                            </indexterm>
                            <indexterm zone="endnote-332">
                                <primary>categorization</primary>
                                <secondary>learning methods</secondary>
                            </indexterm>
                            <indexterm zone="endnote-332">
                                <primary>cognitive science</primary>
                                <secondary>correlation</secondary>
                            </indexterm>
                        </itermset>
                    </info>The debates about human category formation also extend to issues of how
                    children learn categories and categorization methods. Most psychologists argue
                    that category learning starts with general learning mechanisms that are very
                    perceptually based, but they do not agree whether to characterize these changes
                    as <quote>stages</quote> or as phases in a more complex dynamical system. Over
                    time more specific learning techniques evolve that focus on correlations among
                    perceptual properties (things with wings tend to have feathers), correlations
                    among properties and roles (things with eyes tend to eat), and ultimately
                    correlations among roles (things that eat tend to sleep). See <citation
                        xml:id="cite_Smith2003" linkend="Smith2003">(Smith and Thelen
                        2003)</citation>.</para>
            </footnote>
            <phrase role="statement">People almost effortlessly learn tens of thousands of
                categories embodied in the culture and language in which they grow up. People also
                rely on their own experiences, preferences, and goals to adapt these <firstterm
                    linkend="gloss_cultural_categories">cultural categories</firstterm> or create
                entirely individual ones that they use to organize resources that they personally
                arrange. Later on, through situational training and formal education, people learn
                to apply systematic and logical thinking processes so that they can create and
                understand categories in engineering, logistics, transport, science, law, business,
                and other institutional contexts.</phrase></para>
        <para audience="CORE" role="comparative" xml:id="para-ljz_l4r_lr"><info>
                <itermset>
                    <indexterm zone="para-ljz_l4r_lr">
                        <primary>categorization</primary>
                        <secondary>contexts</secondary>
                    </indexterm>
                </itermset>
            </info>These three contexts of <emphasis>cultural, individual,</emphasis> and
                <emphasis>institutional categorization</emphasis> share some core ideas but they
            emphasize different processes and purposes for creating categories, so they are a useful
                distinction.<footnote xml:id="endnote-333" label="391" audience="CogSci">
                <para audience="CogSci" xml:id="para-ukz_l4r_lr"><info>
                        <itermset>
                            <indexterm audience="Markup" zone="endnote-333">
                                <primary>endnote</primary>
                                <secondary>CogSci</secondary>
                            </indexterm>
                            <indexterm zone="endnote-333">
                                <primary>cognitive science</primary>
                                <secondary>categorization contexts</secondary>
                            </indexterm>
                            <indexterm zone="endnote-333">
                                <primary>categorization</primary>
                                <secondary>contexts</secondary>
                            </indexterm>
                        </itermset>
                    </info>These three contexts were proposed by <citation xml:id="cite_Glushko2008"
                        linkend="Glushko2008">(Glushko, Maglio, Matlock, and Barsalou
                        2008)</citation>, who pointed out that cognitive science has focused on
                    cultural categorization and largely ignored individual and institutional
                    contexts. They argue that taking a broader view of categorization highlights
                    dimensions on which it varies that are not apparent when only cultural
                    categories are considered. For example, institutional categories are usually
                    designed and maintained using prescriptive methods that have no analogues with
                    cultural categories. There is a difference between institutional categories
                    created for people, and categories created in institutions by computers in the
                    predictive analytics, data mining sense.</para>
            </footnote>
            <phrase role="statement">Cultural categorization can be understood as a natural human
                cognitive ability that serves as a foundation for both informal and formal
                organizing systems. Individual categorization tends to grow spontaneously out of our
                personal activities. Institutional categorization responds to the need for formal
                coordination and cooperation within and between companies, governments, and other
                goal-oriented enterprises.</phrase></para>
        <para xml:id="para-jrh_5xc_sv">In contrast to these three categorization contexts in which
            categories are created by people, <emphasis>computational</emphasis> categories are
            created by computer programs for information retrieval, machine learning, predictive
            analytics, and other applications. Computational categories are similar to those created
            by people in some ways but differ substantially in other ways. </para>
        
        <!--              -->
        <section xml:id="section-7.2.1" label="7.2.1">
            <title>Cultural Categories</title>
            <info>
                <itermset>
                    <indexterm zone="section-7.2.1">
                        <primary>cultural categories</primary>
                    </indexterm>
                    <indexterm zone="section-7.2.1">
                        <primary>categories</primary>
                        <secondary>cultural</secondary>
                    </indexterm></itermset>
        </info>

            
            
            <para audience="CORE" xml:id="para-inz_l4r_lr"><phrase role="definition"
                    xml:id="def_cultural_categories"><glossterm xml:id="term_cultural_categories"
                        >Cultural categories</glossterm> are the archetypical form of categories
                    upon which individual and institutional categories are usually based. Cultural
                    categories tend to describe our everyday experiences of the world and our
                    accumulated cultural knowledge. </phrase>Such categories describe objects,
                events, settings, internal experiences, physical orientation, relationships between
                entities, and many other aspects of human experience. Cultural categories are
                learned primarily, with little explicit instruction, through normal exposure of
                children with their caregivers; they are associated with language acquisition and
                language use within particular cultural contexts. </para>
            <para audience="Philosophy" xml:id="para-r4z_l4r_lr"><info>
                    <itermset>
                        <indexterm zone="para-r4z_l4r_lr" userlevel="Professional Graduate">
                            <primary>Plato</primary>
                        </indexterm>
                        <indexterm zone="para-r4z_l4r_lr">
                            <primary>natural category</primary>
                            <secondary>carving nature at its joints</secondary>
                        </indexterm>
                    </itermset>
                </info>Two thousand years ago <phrase role="statement"><personname>
                        <surname>Plato</surname></personname> wrote that living species could be
                    identified by <quote>carving nature at its joints,</quote> the natural
                    boundaries or discontinuities between types of things where the differences are
                    the largest or most salient.</phrase> Plato’s metaphor is intuitively appealing
                because we can easily come up with examples of perceptible properties or behaviors
                of physical things that go together that make some ways of categorizing them seem
                more natural than others.<footnote xml:id="endnote-334" label="392"
                    audience="Philosophy">
                    <para audience="Philosophy" xml:id="para-eqz_l4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-334">
                                    <primary>endnote</primary>
                                    <secondary>Philosophy</secondary>
                                </indexterm>
                                <indexterm zone="endnote-334">
                                    <primary>philosophy</primary>
                                    <secondary>Phaedrus</secondary>
                                </indexterm>
                                <indexterm zone="endnote-334">
                                    <primary>inference</primary>
                                </indexterm>
                                <indexterm zone="endnote-334">
                                    <primary>perceptual discontinuities</primary>
                                </indexterm>
                                <indexterm zone="endnote-334">
                                    <primary>Plato</primary>
                                    <secondary>Phaedrus</secondary>
                                </indexterm>
                                <indexterm zone="endnote-334">
                                    <primary>Phaedrus</primary>
                                </indexterm>
                                <indexterm zone="endnote-334">
                                    <primary>cognitive science</primary>
                                    <secondary>inference</secondary>
                                </indexterm>
                            </itermset>
                        </info>This quote comes from Plato’s <citetitle xml:id="cite_Plato370BC"
                            linkend="Plato370BC" pubwork="book">Phaedrus</citetitle> dialogue,
                        written around <date>370 BCE</date>. Contemporary philosophers and cognitive
                        scientists commonly invoke it in discussions about whether <quote>natural kinds</quote> exist.
                      . For example, see <citation xml:id="cite_Campbell2011"
                            linkend="Campbell2011">(Campbell, O’Rourke, and Slater 2011)</citation>,
                        and <citation xml:id="cite_Hutchins2010" linkend="Hutchins2010">(Hutchins
                            2010)</citation>, <citation xml:id="cite_Atran1987" linkend="Atran1987"
                            >(Atran 1987)</citation>, and others have argued that the existence of
                        perceptual discontinuities is not sufficient to account for category
                        formation. Instead, people assume that members of a biological category must
                        have an essence of co-occurring properties and these guide people to focus
                        on the salient differences, thereby creating categories. Property clusters
                        enable inferences about causality, which then builds a framework on which
                        additional categories can be created and refined. For example, if
                            <quote>having wings</quote> and <quote>flying</quote> are co-occurring
                        properties that suggest a <quote>bird</quote> category, wings are then
                        inferred as the causal basis of flying, and wings become more
                        salient.</para>
                </footnote></para>

            <para audience="CogSci Linguistics" xml:id="para-nrz_l4r_lr"><phrase role="statement"
                    >Natural languages rely heavily on nouns to talk about categories of things
                    because it is useful to have a shorthand way of referring to a set of properties
                    that co-occur in predictable ways.</phrase><footnote xml:id="endnote-335"
                    label="393" audience="Linguistics">
                    <para audience="Linguistics" xml:id="para-vsz_l4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-335">
                                    <primary>endnote</primary>
                                    <secondary>Linguistics</secondary>
                                </indexterm>
                                <indexterm zone="endnote-335">
                                    <primary>linguistics</primary>
                                    <secondary>parts of speech</secondary>
                                </indexterm>
                                <indexterm zone="endnote-335">
                                    <primary>parts of speech</primary>
                                </indexterm>
                                <indexterm zone="endnote-335">
                                    <primary>cognitive science</primary>
                                    <secondary>nouns</secondary>
                                </indexterm>
                            </itermset>
                        </info>Pronouns, adjectives, verbs, adverbs, prepositions, conjunctions,
                        particles, and numerals and other <quote>parts of speech</quote> are also
                        grammatical categories, but nouns carry most of the semantic weight.</para>
                </footnote> For example, in English (borrowed from Portuguese) we have a word for
                    <quote>banana</quote> because a particular curved shape, greenish-yellow or
                yellow color, and a convenient size tend to co-occur in a familiar edible object, so
                it became useful to give it a name. The word <quote>banana</quote> brings together
                this configuration of highly interrelated perceptions into a unified concept so we
                do not have to refer to bananas by listing their properties.<footnote
                    xml:id="endnote-336" label="394" audience="CogSci">
                    <para audience="CogSci" xml:id="para-d5z_l4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-336">
                                    <primary>endnote</primary>
                                    <secondary>CogSci</secondary>
                                </indexterm>
                                <indexterm zone="endnote-336">
                                    <primary>cognitive science</primary>
                                    <secondary>verbs</secondary>
                                </indexterm>
                            </itermset>
                        </info>In contrast, the set of possible interactions with even a simple
                        object like a banana is very large. We can pick, peel, slice, smash, eat, or
                        throw a banana, so instead of capturing this complexity in the meaning of
                        banana it gets parceled into the verbs that can act on the banana noun.
                        Doing so requires languages to use verbs to capture a broader and more
                        abstract type of meaning that is determined by the nouns with which they are
                        combined. Familiar verbs like <quote>set,</quote>
                        <quote>put,</quote> and <quote>get</quote> have dozens of different senses
                        as a result because they go with so many different nouns. We set fires and
                        we set tables, but fires and tables have little in common. The intangible
                        character of verbs and the complexity of multiple meanings make it easier to
                        focus instead on their associated nouns, which are often physical resources,
                        and create organizing systems that emphasize the latter rather than the
                        former. We create organizing systems that focus on verbs when we are
                        categorizing actions, behaviors, or services where the resources that are
                        involved are less visible or less directly involved in the supported
                        interactions.</para>
                </footnote></para>
            <para audience="CORE" xml:id="para-mvz_l4r_lr"><info><itermset>
                <indexterm significance="preferred"
                    zone="def_linguistic_relativity">
                    <primary>linguistic relativity</primary>
                </indexterm><indexterm zone="def_linguistic_relativity">
                    <primary>language</primary>
                    <secondary>linguistic relativity</secondary>
                </indexterm></itermset>
        </info><phrase role="definition" xml:id="def_linguistic_relativity">Languages
                    differ a great deal in the words they contain and also in more fundamental ways
                    that they require speakers or writers to attend to details about the world or
                    aspects of experience that another language allows them to ignore. This idea is
                    often described as <glossterm xml:id="term_linguistic_relativity">linguistic
                        relativity</glossterm>.</phrase>
                <phrase role="parenthetical">(See the sidebar, <xref linkend="sidebar-LinguisticRelativity"
                    />.)</phrase></para>
            
            <sidebar xml:id="sidebar-LinguisticRelativity">
                <title>Linguistic Relativity</title>
                <info>
                    <itermset>
                        <indexterm zone="sidebar-LinguisticRelativity" condition="print" userlevel="Professional Graduate">
                            <primary>sidebar</primary>
                            <secondary>Linguistic Relativity</secondary>
                        </indexterm>
                        <indexterm zone="sidebar-LinguisticRelativity" userlevel="Professional Graduate">
                            <primary>Linguistic Relativity</primary>
                        </indexterm>
                        <indexterm zone="sidebar-LinguisticRelativity">
                            <primary>language</primary>
                            <secondary>linguistic relativity</secondary>
                        </indexterm>
                        <indexterm zone="sidebar-LinguisticRelativity">
                            <primary>concept</primary>
                            <secondary>linguistic relativity</secondary>
                        </indexterm></itermset>
        </info>

                
                <para audience="CORE" xml:id="para-uwz_l4r_lr"><info><itermset>
                    <indexterm zone="para-uwz_l4r_lr" userlevel="Professional Graduate">
                        <primary>Whorf, Benjamin</primary>
                    </indexterm></itermset>
        </info>Linguistic diversity led
                            <personname><firstname>Benjamin</firstname>
                        <surname>Whorf</surname></personname>, in the <date>mid-20th century</date>,
                    to propose an overly strong statement of the relationships among language,
                    culture, and thought. <phrase role="statement"
                                ><personname><surname>Whorf</surname></personname> argued that the
                        particularities of one’s native language determine how we think and what we
                        can think about. </phrase>Among his extreme ideas was the suggestion that,
                    because some Native American languages lacked words or grammatical forms that
                    refer to what we call <quote>time</quote> in English, they could not understand
                    the concept. More careful language study showed both parts of the claim to be
                    completely false.</para>
                <para audience="CORE" role="contrast" xml:id="para-fyz_l4r_lr">Nevertheless, even
                    though academic linguists have discredited strong versions of Whorf’s ideas,
                    less deterministic versions of <emphasis>linguistic relativity</emphasis> have
                    become influential and help us understand cultural categorization. The more
                    moderate position was crisply characterized by Roman Jakobson, who said that
                        <quote>languages differ essentially in what they <emphasis>must</emphasis>
                        convey and not in what they <emphasis>may</emphasis> convey.</quote> In
                    English one can say <quote>I spent yesterday with a neighbor.</quote> In
                    languages with grammatical gender, one must choose a word that identifies the
                    neighbor as male or female.<footnote xml:id="endnote-337" label="395"
                        audience="Linguistics">
                        <para audience="Linguistics" xml:id="para-pzz_l4r_lr"><info>
                                <itermset>
                                    <indexterm audience="Markup" zone="endnote-337">
                                        <primary>endnote</primary>
                                        <secondary>Linguistics</secondary>
                                    </indexterm>
                                    <indexterm zone="endnote-337">
                                        <primary>linguistics</primary>
                                        <secondary>grammatical gender</secondary>
                                    </indexterm>
                                    <indexterm zone="endnote-337">
                                        <primary>language</primary>
                                        <secondary>grammatical gender</secondary>
                                    </indexterm>
                                    <indexterm zone="endnote-337">
                                        <primary>cognitive science</primary>
                                        <secondary>grammatical gender</secondary>
                                    </indexterm>
                                    <indexterm zone="endnote-337">
                                        <primary>Lakoff, George</primary>
                                    </indexterm>
                                    <indexterm zone="endnote-337">
                                        <primary>Women, Fire, and Dangerous Things</primary>
                                    </indexterm>
                                </itermset>
                            </info>Many languages have a system of grammatical gender in which all
                            nouns must be identified as masculine or feminine using definite
                            articles (<foreignphrase xml:lang="es">el</foreignphrase> and
                                <foreignphrase xml:lang="es">la</foreignphrase> in Spanish,
                                <foreignphrase xml:lang="fr">le</foreignphrase> and <foreignphrase
                                xml:lang="fr">la</foreignphrase> in French, and so on) and
                            corresponding pronouns. Languages also contrast in how they describe
                            time, spatial relationships, and in which things are treated as
                            countable objects (one ox, two oxen) as opposed to substances or mass
                            nouns that do not have distinct singular and plural forms (like water or
                            dirt). <citation xml:id="cite_Deutscher2011" linkend="Deutscher2011"
                                >(Deutscher 2011)</citation> carefully reviews and discredits the
                            strong Whorfian view and makes the case for a more nuanced perspective
                            on linguistic relativity. He also reviews much of
                                    <personname><firstname>Lera</firstname>
                                <surname>Boroditsky</surname></personname>’s important work in this
                            area. <personname><firstname>George</firstname>
                                <surname>Lakoff</surname></personname>’s book with the title
                                <citetitle xml:id="cite_Lakoff1990-6.2a" linkend="Lakoff1990"
                                pubwork="book">Women, Fire, and Dangerous Things</citetitle>
                            <citation xml:id="cite_Lakoff1990-6.2b" linkend="Lakoff1990">(Lakoff
                                1990)</citation> provocatively points out differences in gender
                            rules among languages; in an aboriginal language called Dyirbal many
                            dangerous things, including fire have feminine gender, meanwhile
                                <quote>fire</quote> is masculine in Spanish (<foreignphrase
                                xml:lang="es">el feugo</foreignphrase>) and French (<foreignphrase
                                xml:lang="fr">le feu</foreignphrase>).</para>
                    </footnote></para>
            </sidebar>
            <para audience="CORE" xml:id="para-x11_m4r_lr"><info>
                    <itermset>
                        <indexterm zone="para-x11_m4r_lr" userlevel="Professional Graduate">
                            <primary>Guugu Yimithirr</primary>
                        </indexterm>
                        <indexterm zone="para-x11_m4r_lr">
                            <primary>language</primary>
                            <secondary>aboriginal</secondary>
                            <tertiary>Guugu Yimithirr</tertiary>
                        </indexterm>
                    </itermset>
                </info>For example, speakers of the Australian aboriginal language, Guugu Yimithirr,
                do not use concepts of left and right, but rather use cardinal directions. Where in
                English we might say to a person facing north, <quote>Take a step to your
                    left,</quote> they would use their term for west. If the person faced south, we
                would change our instruction to <quote>right,</quote> but they would still use their
                term for west. Imagine how difficult it would be for a speaker of Guugu Yimithirr
                and a speaker of English to collaborate in organizing a storage room or a
                    closet.<footnote xml:id="endnote-338" label="396" audience="CogSci">
                    <para audience="CogSci" xml:id="para-ec1_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-338">
                                    <primary>endnote</primary>
                                    <secondary>CogSci</secondary>
                                </indexterm>
                                <indexterm zone="endnote-338">
                                    <primary>cognitive science</primary>
                                    <secondary>linguistic relativity</secondary>
                                </indexterm>
                            </itermset>
                        </info>This analysis comes from <citation xml:id="cite_Haviland1998"
                            linkend="Haviland1998">(Haviland 1998)</citation>. More recently,
                                <personname><firstname>Lera</firstname>
                            <surname>Boroditsky</surname></personname> has done many interesting
                        studies and experiments about linguistic relativity. See <citation
                            xml:id="cite_Boroditsky2003" linkend="Boroditsky2003">(Boroditksy
                            2003)</citation> for an academic summary and <citation
                            xml:id="cite_Boroditsky2010" linkend="Boroditsky2010">(Boroditsky
                            2010</citation>, <citation xml:id="cite_Boroditsky2011"
                            linkend="Boroditsky2011">2011)</citation> for more popular
                        treatments.</para>
                </footnote></para>
            <para audience="CORE" xml:id="para-ld1_m4r_lr"><phrase role="statement">It is not
                    controversial to notice that different cultures and language communities have
                    different experiences and activities that give them contrasting knowledge about
                    particular domains.</phrase> No one would doubt that university undergraduates
                in Chicago would think differently about animals than inhabitants of Guatemalan rain
                forests, or even that different types of <quote>tree experts</quote> (taxonomists,
                landscape workers, foresters, and tree maintenance personnel) would categorize trees
                    differently.<footnote xml:id="endnote-339" label="397" audience="CogSci">
                    <para audience="CogSci" xml:id="para-x21_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-339">
                                    <primary>endnote</primary>
                                    <secondary>CogSci</secondary>
                                </indexterm>
                                <indexterm zone="endnote-339" userlevel="Professional">
                                    <primary>cognitive science</primary>
                                    <secondary>citations</secondary>
                                </indexterm>
                            </itermset>
                        </info><citation xml:id="cite_Medin1997" linkend="Medin1997">(Medin et al.
                            1997)</citation>.</para>
                </footnote></para>
            <para revision="3.0" revisionflag="added">On the other hand, despite the wide variation
                in the climates, environments, and cultures that produce them, at a high level
                    <quote>folk taxonomies</quote> that describe natural phenomena are surprisingly
                consistent around the world. Half a century ago the sociologists
                        <personname><firstname>Emile</firstname>
                    <surname>Durkheim</surname></personname> and
                        <personname><firstname>Marcel</firstname>
                    <surname>Mauss</surname></personname> observed that the language and structure
                of folk taxonomies mirrors that of human family relationships (e.g., different types
                of trees might be <quote>siblings,</quote> but animals would be part of another family entirely).
                They suggested that framing the world in terms of familiar human relationships
                allowed people to understand it more easily.<footnote label="398"
                    audience="CogSci" revision="3.0" revisionflag="added" xml:id="endnote-339a">
                    <para audience="CogSci" revision="3.0" revisionflag="added"
                        xml:id="para-tnj_r2w_ps"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-339a">
                                    <primary>endnote</primary>
                                    <secondary>CogSci</secondary>
                                </indexterm>
                                <indexterm zone="endnote-339a" userlevel="Professional">
                                    <primary>cognitive science</primary>
                                    <secondary>citations</secondary>
                                </indexterm>
                            </itermset>
                        </info>This was ultimately reflected in complex mythological systems, such
                        as Greek mythology, where genealogical relationships between gods
                        represented category relationships among the phenomena with which they were
                        associated. As human knowledge grew and the taxonomies became more
                        comprehensive and complex, Durkheim and Mauss argued, they lay the
                        groundwork for scientific classifications and shed their mythological roots.
                            <citation xml:id="cite_Durkheim1963" linkend="Durkheim1963">(Durkheim
                            1963)</citation>.</para>
                </footnote></para>
            <para revision="3.0" revisionflag="added"><jobtitle>Anthropologist</jobtitle>
                <personname><firstname>Brent</firstname>
                    <surname>Berlin</surname></personname>, a more recent
                    <jobtitle>researcher</jobtitle>, concurs with Durkheim and Mauss’s observation
                that kinship relations and folk taxonomies are related, but argues that humans
                patterned their family structures after the natural world, not the other way
                    around.<footnote label="399" audience="CogSci" xml:id="endnote-339b"
                    revision="3.0" revisionflag="added">
                    <para audience="CogSci" revision="3.0" revisionflag="added"
                        xml:id="para-d24_p2w_ps"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-339b">
                                    <primary>endnote</primary>
                                    <secondary>CogSci</secondary>
                                </indexterm>
                                <indexterm zone="endnote-339b" userlevel="Professional">
                                    <primary>cognitive science</primary>
                                    <secondary>citations</secondary>
                                </indexterm>
                            </itermset>
                        </info><citation xml:id="cite_Berlin2014" linkend="Berlin2014">(Berlin
                            2014)</citation></para>
                </footnote></para>

            <sidebar xml:id="sidebar-Whorf">
                <title>Invoking the Whorfian Hypothesis in a Clothing Ad</title>
                <info>
                    <itermset>
                        <indexterm zone="sidebar-Whorf" condition="print" userlevel="Professional Graduate">
                            <primary>sidebar</primary>
                            <secondary>Invoking the Whorfian Hypothesis in a Clothing Ad</secondary>
                        </indexterm>
                        <indexterm zone="sidebar-Whorf" condition="print" userlevel="Professional Graduate">
                            <primary>Invoking the Whorfian Hypothesis in a Clothing Ad</primary>
                        </indexterm>
                        <indexterm zone="sidebar-Whorf" userlevel="Professional Graduate">
                            <primary>Whorf, Benjamin</primary>
                        </indexterm></itermset>
        </info>
                
                <informalfigure xml:id="PICTURE-S6.2.1-Whorf" xreflabel="Whorf">
                    <mediaobject>
                        <imageobject>
                            <imagedata fileref="Pictures/6.2.1-Whorf-OK.gif" format="GIF"/>
                        </imageobject>
                        <textobject>
                            <phrase role="ALT descriptive">An advertisement for <quote>66
                                    North.</quote> The image has three photographic elements and two
                                typographic elements: the top two thirds of the image presents, in
                                the foreground, a young woman wearing a winter jacket with a
                                    <quote>66 North</quote> logo, and, in the background, a scene
                                which includes a waterway, rocks, land, and a lighthouse; the bottom
                                third of the image is a wooden fence. Text in the upper segment
                                reads: <quote>There are over 100 words for snow in Icelandic. Only
                                    one for what to wear.</quote> Text in lower segment details
                                business locations and <uri>www.66north.com</uri>.</phrase>
                        </textobject>
                        <caption><para audience="CORE" xml:id="para-gg1_m4r_lr"><phrase role="caption">An 
                            advertisement for the <quote>66 North</quote> clothing brand invokes
                                    the Whorfian hypothesis to suggest that 
                                    even though Icelanders have more than a hundred words for snow
                                    there is only one kind of winter clothing that matters to them;
                                    the kind that carries this brand name.</phrase>
                            </para>
                            <para audience="CORE" xml:id="para-nh1_m4r_lr"><phrase role="credit"
                                    >(Photo by R. Glushko. Taken in the Reykjavik
                                airport.)</phrase></para></caption>
                    </mediaobject>
                </informalfigure>

            </sidebar>


        </section>
        <!--              -->
        <?dbfo clear ?>
        <?need 7.5cm ?>
        
        <section xml:id="section-7.2.2" label="7.2.2">
            <title>Individual Categories</title>
            <info>
                <itermset>
                    <indexterm zone="section-7.2.2">
                        <primary>individual</primary>
                        <secondary>categories</secondary>
                    </indexterm>
                    <indexterm zone="section-7.2.2">
                        <primary>categories</primary>
                        <secondary>individual</secondary>
                    </indexterm></itermset>
        </info>

            <para audience="CORE" xml:id="para-v31_m4r_lr"><info>
                    <itermset>
                        <indexterm significance="preferred" zone="def_individual_categorization">
                            <primary>individual categorization</primary>
                        </indexterm>
                        <indexterm zone="def_individual_categorization">
                            <primary>categorization</primary>
                            <secondary>individual</secondary>
                        </indexterm>
                    </itermset>
                </info><phrase role="definition" xml:id="def_individual_categorization"><glossterm
                        xml:id="term_individual_categorization">Individual categories</glossterm>
                    are created in an organizing system to satisfy the <foreignphrase
                        xml:lang="Latn">ad hoc</foreignphrase> requirements that arise from a
                    person’s unique experiences, preferences, and resource collections. Unlike
                    cultural categories, which usually develop slowly and last a long time,
                    individual categories are created by intentional activity, in response to a
                    specific situation, or to solve an emerging organizational challenge.</phrase>
                As a consequence, the categories in individual organizing systems generally have
                short lifetimes and rarely outlive the person who created them.<footnote
                    xml:id="endnote-340" label="400" audience="Archives">
                    <para audience="Archives" xml:id="para-ck1_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-340">
                                    <primary>endnote</primary>
                                    <secondary>Archives</secondary>
                                </indexterm>
                                <indexterm zone="endnote-340">
                                    <primary>archives</primary>
                                    <secondary>personal</secondary>
                                </indexterm>
                                <indexterm zone="endnote-340">
                                    <primary>library science</primary>
                                    <secondary>personal archives</secondary>
                                </indexterm>
                            </itermset>
                        </info>The personal archives of people who turn out to be famous or
                        important are the exception that proves this rule. In that case, the
                        individual’s organizing system and its categories are preserved along with
                        their contents.</para>
                </footnote></para>
            <para audience="CORE" xml:id="para-ol1_m4r_lr"><phrase role="statement">Individual
                    categories draw from cultural categories but differ in two important ways.
                    First, individual categories sometimes have an imaginative or metaphorical basis
                    that is meaningful to the person who created them but which might distort or
                    misinterpret cultural categories. Second, individual categories are often
                    specialized or synthesized versions of cultural categories that capture
                    particular experiences or personal history.</phrase> For example, a person who
                has lived in China and Mexico, or lived with people from those places, might have
                highly individualized categories for foods they like and dislike that incorporate
                characteristics of both Chinese and Mexican cuisine.</para>
            <para audience="CORE" xml:id="para-ym1_m4r_lr"><phrase role="statement">Individual
                    categories in organizing systems also reflect the idiosyncratic set of household
                    goods, music, books, website bookmarks, or other resources that a person might
                    have collected over time.</phrase>
                <phrase role="statement">The organizing systems for financial records, personal
                    papers, or email messages often use highly specialized categories that are
                    shaped by specific tasks to be performed, relationships with other people,
                    events of personal history, and other highly individualized
                    considerations.</phrase> Put another way, individual categories are used to
                organize resource collections that are likely not representative samples of all
                resources of the type being collected. If everyone had the same collection of music,
                books, clothes, or toys the world would be a boring place.</para>
            <para audience="CORE" role="comparative" xml:id="para-f41_m4r_lr"><info>
                    <itermset>
                        <indexterm zone="para-f41_m4r_lr">
                            <primary>Twitter</primary>
                        </indexterm>
                    </itermset>
                </info>Traditionally, <glossterm linkend="gloss_individual_categorization"
                    >individual categorization</glossterm> systems were usually not visible to, or
                shared with, others, whereas, this has become an increasingly common situation for
                people using web-based organizing system for pictures, music, or other personal
                resources. On websites like the popular <application>Flickr</application>,
                    <application>Instagram</application>, and <application>YouTube</application>
                sites for photos and videos, people typically use existing cultural categories to
                tag their content as well as individual ones that they invent.<footnote
                    audience="Linguistics" label="401" xml:id="endmote-340a" revision="4.0"
                    revisionflag="added">
                    <para xml:id="para-mnc_fm1_fw" audience="Linguistics">The typical syntactic constraint that tags are
                        delimited by white space encourages the creation of new categories by
                        combining existing category names using concatenation and camel case
                        conventions; photos that could be categorized as <quote>Berkeley</quote> and
                            <quote>Student</quote> are sometimes tagged as
                            <quote>BerkeleyStudent.</quote> Similar generative processes for
                        creating individual category names are used with Twitter
                            <quote>hashtags</quote> where tweets about events are often categorized
                        with an ad hoc tag that combines an event name and a year identifier like
                            <quote>#NBAFinals16.</quote></para>
                </footnote></para>
        </section>
        <!--              -->
        <?dbfo clear ?>
        <?need 7.5cm ?>
        
        <section xml:id="section-7.2.3" label="7.2.3">
            <title>Institutional Categories</title>
            <info>
                <itermset>
                    <indexterm zone="section-7.2.3">
                        <primary>institutional</primary>
                        <secondary>categories</secondary>
                    </indexterm>
                    <indexterm zone="section-7.2.3">
                        <primary>categories</primary>
                        <secondary>institutional</secondary>
                    </indexterm></itermset>
        </info>

            
            <para audience="CORE" xml:id="para-np1_m4r_lr"><info>
                    <itermset>
                        <indexterm zone="para-np1_m4r_lr">
                            <primary>transaction costs</primary>
                        </indexterm>
                        <indexterm zone="para-np1_m4r_lr">
                            <primary>costs</primary>
                            <secondary>transaction</secondary>
                        </indexterm>
                    </itermset>
                </info><phrase role="definition" xml:id="def_institutional_categorization">In
                    contrast to cultural categories that are created and used implicitly, and to
                    individual categories that are used by people acting alone, <glossterm
                        xml:id="term_institutional_categorization">institutional
                        categories</glossterm> are created and used explicitly, and most often by
                    many people in coordination with each other. Institutional categories are most
                    often created in abstract and information-intensive domains where unambiguous
                    and precise categories are needed to regulate and systematize activity, to
                    enable information sharing and reuse, and to reduce transaction costs.</phrase>
                Furthermore, instead of describing the world as it is, institutional categories are
                usually defined to change or control the world by imposing semantic models that are
                more formal and arbitrary than those in cultural categories. Laws, regulations, and
                standards often specify institutional categories, along with decision rules for
                assigning resources to new categories, and behavior rules that prescribe how people
                must interact with them. The rigorous definition of institutional categories enables
                    <emphasis>classification:</emphasis> the systematic assignment of resources to
                categories in an organizing system.<footnote xml:id="endnote-341" label="402"
                    audience="Law">
                    <para audience="Law" xml:id="para-xq1_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-341">
                                    <primary>endnote</primary>
                                    <secondary>Law</secondary>
                                </indexterm>
                                <indexterm zone="endnote-341">
                                    <primary>law</primary>
                                    <secondary>intentionality and planning</secondary>
                                </indexterm>
                                <indexterm zone="endnote-341">
                                    <primary>law</primary>
                                    <secondary>rule-based categorization</secondary>
                                </indexterm>
                            </itermset>
                        </info>Consider how the cultural category of <quote>killing a person</quote>
                        is refined by the legal system to distinguish manslaughter and different
                        degrees of murder based on the amount of intentionality and planning
                        involved (e.g., first and second degree murder) and the roles of people
                        involved with the killing (accessory). In general, the purpose of laws is to
                        replace coarse judgments of categorization based on overall similarity of
                        facts with rule-based categorization based on specific dimensions or
                        properties.</para>
                </footnote></para>
                  <para audience="CORE" xml:id="para-js1_m4r_lr"><info>
                    <itermset>
                        <indexterm zone="para-js1_m4r_lr">
                            <primary>data science</primary>
                            <secondary>gerrymandering</secondary>
                        </indexterm>
                    </itermset>
                </info>Creating institutional categories by more systematic processes than cultural
                or individual categories does not ensure that they will be used in systematic and
                rational ways, because the reasoning and rationale behind institutional categories
                might be unknown to, or ignored by, the people who use them. Likewise, this way of
                creating categories does not prevent them from being biased. Indeed, the goal of
                institutional categories is often to impose or incentivize biases in interpretation
                or behavior. There is no better example of this than the practice of gerrymandering,
                designing the boundaries of election districts to give one political party or ethnic
                group an advantage.<footnote label="403" revision="4.0" revisionflag="added"
                    xml:id="endnote-341a" audience="Linguistics">
                    <para xml:id="para-mrb_wm1_fw" audience="Linguistics">The word was invented in <date>1812</date> in a
                        newspaper article critical of Massachusetts governor
                                <personname><firstname>Elbridge</firstname>
                            <surname>Gerry</surname></personname>, who oversaw the creation of
                        biased electoral districts. One such district was so contorted in shape, it
                        was said to look like a salamander, and thus was called a Gerrymander. The
                        practice remains widespread, but nowadays sophisticated computer programs
                        can select voters on any number of characteristics and create boundaries
                        that either <quote>pack</quote> them into a single district to concentrate
                        their voting power or <quote>crack</quote> them into multiple districts to
                        dilute it.</para>
                </footnote><phrase role="parenthetical">(See the sidebar, <xref
                        linkend="sidebar-Illinois17"/>.)</phrase></para>
                
                  <sidebar xml:id="sidebar-Illinois17">
                <title>Gerrymandering the Illinois 17th Congressional District</title>
                <info>
                    <itermset>
                        <indexterm zone="sidebar-Illinois17" condition="print" userlevel="Professional Graduate">
                            <primary>sidebar</primary>
                            <secondary>Gerrymandering in Illinois 17th Congressional
                                District</secondary>
                        </indexterm>
                        <indexterm zone="sidebar-Illinois17" condition="print" userlevel="Professional Graduate">
                            <primary>bias in institutional categorization</primary>
                        </indexterm>
                        <indexterm zone="sidebar-Illinois17">
                            <primary>gerrymandering</primary>
                        </indexterm>
                    </itermset>
                </info>
                <informalfigure xml:id="PICTURE-S6.2.3-Illinois17" xreflabel="Illinois17">
                    <mediaobject>
                        <imageobject>
                            <imagedata fileref="Pictures/6.2.3-Illinois17.jpg" format="JPG"/>
                        </imageobject>
                        <textobject>
                            <phrase role="ALT descriptive">A map of the 17th Congressional district
                                of the state of Illinois shows a highly contorted shape that
                                interconnects all the major cities in the western part of the state
                                into a shape often described as a rabbit on a skateboard.</phrase>
                        </textobject>
                        <caption>
                            <para audience="CORE" xml:id="para-tt1_m4r_lr"><phrase role="caption"
                                    >The 17th Congressional District in Illinois was dubbed
                                        <quote>the rabbit on a skateboard</quote> from <date>2003
                                        through 2013</date> because of its highly contorted shape.
                                    The bizarre boundary was negotiated to create favorable voting
                                    constituencies for two incumbent legislators from opposing
                                    parties. </phrase>
                            </para>
                            <para audience="CORE" xml:id="para-ev1_m4r_lr"><phrase role="credit"
                                    >(Picture from nationatlas.gov. Not protectable by copyright (17
                                    USC Sec. 105).)</phrase></para>
                        </caption>
                    </mediaobject>
                </informalfigure>
            </sidebar>

                
            <para audience="CORE" role="contrast" xml:id="para-kw1_m4r_lr">Institutional
                categorization stands apart from individual categorization primarily because it
                invariably requires significant efforts to reconcile mismatches between existing
                individual categories, where those categories embody useful working or <glossterm
                    linkend="gloss_contextual_properties">contextual knowledge</glossterm> that is
                lost in the move to a formal institutional system.<footnote xml:id="endnote-342"
                    label="404" audience="Business">
                    <para audience="Business" xml:id="para-ux1_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-342">
                                    <primary>endnote</primary>
                                    <secondary>Business</secondary>
                                </indexterm>
                                <indexterm zone="endnote-342">
                                    <primary>business</primary>
                                    <secondary>worker satisfaction</secondary>
                                </indexterm>
                            </itermset>
                        </info>The particularities or idiosyncrasies of individual categorization
                        systems sometimes capture user expertise and knowledge that is not
                        represented in the institutional categories that replace them. Many of the
                        readers of this book are information professionals whose technological
                        competence is central to their work and which helps them to be creative. But
                        for a great many other people, information technology has enabled the
                        routinization of work in offices, assembly lines, and in other jobs where
                        new institutionalized job categories have <quote>downskilled</quote> or
                            <quote>deskilled</quote> the nature of work, destroying competence and
                        engendering a great deal of resistance from the affected workers.</para>
                </footnote></para>
            <para audience="CORE" role="contrast" xml:id="para-cz1_m4r_lr">Institutional
                categorization efforts must also overcome the vagueness and inconsistency of
                cultural categories because the former must often conform to stricter logical
                standards to support inference and meet legal requirements. Furthermore,
                institutional categorization is usually a process that must be accounted for in a
                budget and staffing plans. While some kinds of institutional categories can be
                devised or discovered by computational processes, most of them are created through
                the collaboration of many individuals, typically from various parts of an
                organization or from different firms. For example, with the gerrymandering case we
                just discussed, it is important to emphasize that the inputs to these programs and
                the decisions about districting are controlled by people, which is why the districts
                are institutional categories; the programs are simply tools that make the process
                more efficient. <footnote xml:id="endnote-343" label="405" audience="Business">
                    <para audience="Business" xml:id="para-k1b_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-343">
                                    <primary>endnote</primary>
                                    <secondary>Business</secondary>
                                </indexterm>
                                <indexterm zone="endnote-343">
                                    <primary>OASIS</primary>
                                    <secondary>technical committee process</secondary>
                                </indexterm>
                                <indexterm zone="endnote-343">
                                    <primary>business</primary>
                                    <secondary>standards</secondary>
                                </indexterm>
                            </itermset>
                        </info>Similar technical concerns arise in within-company and multi-company
                        standardization efforts, but the competitive and potentially
                        anti-competitive character of the latter imposes greater complexity by
                        introducing considerations of business strategy and politics. Credible
                        standards-making in multi-company contexts depends on an explicit and
                        transparent process for gathering and prioritizing requirements, negotiating
                        specifications that satisfy them, and ensuring conformant
                            implementations<symbol>&#8212;</symbol>without at any point giving any
                        participating firm an advantage. See the
                                    <citerefentry><refentrytitle><abbrev>OASIS</abbrev> Technical
                                Committee Process</refentrytitle></citerefentry> for an example
                            (<link xmlns:xlink="http://www.w3.org/1999/xlink"
                            xlink:href="https://www.oasis-open.org/policies-guidelines/tc-process"
                                ><uri>https://www.oasis-open.org/policies-guidelines/tc-process</uri></link>)
                        and <citation xml:id="cite_Rosenthal2004" linkend="Rosenthal2004">(Rosenthal
                            et al. 2004)</citation> for an analysis of best practices.</para>
                </footnote></para>
            <sidebar xml:id="StopAndThink-6.2-color">
                <?dbhtml sidebar-width="40%"?>
                <?dbhtml float-type="right"?>
                <?dbfo sidebar-width="50%"?>
                <?dbfo float-type="outside"?>
                <title>Stop and Think: Color</title>
                <info>
                    <itermset>
                        <indexterm zone="StopAndThink-6.2-color" condition="print"
                            userlevel="Professional Graduate">
                            <primary>Stop and Think</primary>
                            <secondary>Color</secondary>
                        </indexterm>
                        <indexterm zone="StopAndThink-6.2-color" userlevel="Professional Graduate">
                            <primary>Color</primary>
                        </indexterm>
                        <indexterm zone="StopAndThink-6.2-color">
                            <primary>categorization</primary>
                            <secondary>color</secondary>
                        </indexterm>
                    </itermset>
                </info>
                <para xml:id="para-f2v_gzt_qw">Think of the very broad category of
                        <quote>color.</quote> What are a few examples of a <quote>cultural</quote>
                    category of color? How about an <quote>individual</quote> one? And an
                        <quote>institutional</quote> one? </para>
            </sidebar>
            <para audience="CORE" xml:id="para-rbb_m4r_lr">The different business or technical
                perspectives of the participants are often the essential ingredients in developing
                robust categories that can meet carefully identified requirements. And as
                requirements change over time, institutional categories must often change as well,
                implying version control, compliance testing, and other formal maintenance and
                governance processes.</para>
            
            
            
            
            <para audience="CORE" role="contrast" xml:id="para-zcb_m4r_lr"><info>
                    <itermset>
                        <indexterm zone="para-zcb_m4r_lr">
                            <primary>periodic table</primary>
                        </indexterm>
                    </itermset>
                </info>Some institutional categories that initially had narrow or focused
                applicability have found their way into more popular use and are now considered
                cultural categories. A good example is the periodic table in chemistry, which
                Mendeleev developed in <date>1869</date> as a new system of categories for the
                chemical elements. The periodic table proved essential to scientists in
                understanding their properties and in predicting undiscovered ones. Today the
                periodic table is taught in elementary schools, and many things other than elements
                are commonly arranged using a graphical structure that resembles the periodic table
                of elements in chemistry, including sci-fi films and movies, desserts, and
                    superheroes.<footnote xml:id="endnote-344" label="406" audience="CogSci">
                    <para audience="CogSci" xml:id="para-k2b_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-344">
                                    <primary>endnote</primary>
                                    <secondary>CogSci</secondary>
                                </indexterm>
                                <indexterm zone="endnote-344" userlevel="Professional">
                                    <primary>cognitive science</primary>
                                    <secondary>citations</secondary>
                                </indexterm>
                            </itermset>
                        </info> Unfortunately, in this transition from science to popular culture,
                        many of these so-called periodic tables are just <foreignphrase>ad
                            hoc</foreignphrase> collections that ignore the essential idea that the
                        rows and columns capture explanatory principles about resource properties
                        that vary in a periodic manner. A notable exception is Andrew Plotkin's
                        Periodic Table of Dessert. See <citation xml:id="cite_Suehle2012"
                            linkend="Suehle2012">(Suehle 2012)</citation> and Plotkin's table at
                            (<link xmlns:xlink="http://www.w3.org/1999/xlink"
                            xlink:href="http://www.eblong.com/zarf/periodic/">Periodic Table of
                            Dessert</link>).</para>
                </footnote></para>
        </section>
        <!--              -->
        <section xml:id="section-7.2.4" label="7.2.4">
            <title>A <quote>Categorization Continuum</quote></title>
            <info>
                <itermset>
                    <indexterm zone="section-7.2.4">
                        <primary>categorization</primary>
                        <secondary>continuum</secondary>
                    </indexterm></itermset>
        </info>
            
            <para audience="CORE" xml:id="para-sfb_m4r_lr">As we have seen, the concepts of
                cultural, individual, and institutional categorization usefully distinguish the
                primary processes and purposes when people create categories. However, these three
                kinds of categories can fuse, clash, and recombine with each other. Rather than
                viewing them as having precise boundaries, we might view them as regions on a
                continuum of categorization activities and methods.</para>
            <para audience="CORE" role="contrast" xml:id="para-ahb_m4r_lr"><info><itermset>
                <indexterm zone="para-ahb_m4r_lr" userlevel="Professional Graduate">
                <primary>Linnaeus, Carl</primary>
            </indexterm></itermset>
        </info>Consider a few different
                perspectives on categorizing animals as an example. <phrase role="principle statement">Scientific institutions categorize
                    animals according to explicit, principled classification systems, such as the
                    Linnaean taxonomy that assigns animals to a phylum, class, order, family, genus
                    and species.</phrase>
                <phrase role="statement principle">Cultural categorization practices cannot be
                    adequately described in terms of a master taxonomy, and are more fluid,
                    converging with principled taxonomies sometimes, and diverging at other
                    times.</phrase> While human beings are classified within the animal kingdom in
                biological classification systems, people are usually not considered animals in most
                cultural contexts. Sometimes a scientific designation for human beings,
                    <foreignphrase xml:lang="Latn">homo sapiens</foreignphrase> is even applied to
                human beings in cultural contexts, since the genus-species taxonomic designation has
                influenced cultural conceptions of people and (other) animals over the years.</para>
            
            
            
            <para audience="CORE" role="contrast" xml:id="para-j3b_m4r_lr">Animals are also often
                culturally categorized as pets or non-pets. The category <quote>pets</quote>
                commonly includes dogs, cats, and fish. A pet cat might be categorized at multiple
                levels that incorporate individual, cultural, and institutional perspectives on
                    categorization<symbol>&#8212;</symbol>as an <quote>animal</quote>
                (cultural/institutional), as a <quote>mammal</quote> (institutional), as a
                    <quote>domestic short-hair</quote> (institutional) as a <quote>cat</quote>
                (cultural), and as a <quote>troublemaker</quote> or a <quote>favorite</quote>
                (individual), among other possibilities, in addition to being identified
                individually by one or more pet names. Furthermore, not everyone experiences pets as
                just dogs, cats and fish. Some people have relatively unusual pets, like pigs. For
                individuals who have pet pigs or who know people with pet pigs, <quote>pigs</quote>
                may be included in the <quote>pets</quote> category. If enough people have pet pigs,
                eventually <quote>pigs</quote> could be included in mainstream culture’s pet
                category.</para>
            
            
            <!--
            <para audience="CORE"><phrase role="statement">It is not possible to entirely separate
                    individual, cultural and institutional perspectives on categorization.
                    Individuals form subcultures and contribute to institutions; culture influences
                    individuals and institutions; institutions influence individuals and
                    culture.</phrase></para>

-->
            <para audience="CORE" role="contrast" xml:id="para-rjb_m4r_lr">Categorization skewed
                toward cultural perspectives incorporate relatively traditional categories, such as
                those learned implicitly from social interactions, like mainstream understandings of
                what kinds of animals are <quote>pets,</quote> while categorization skewed toward
                institutional perspectives emphasizes explicit, formal categories, like the
                categories employed in biological classification systems.</para>
            
            
            <sidebar xml:id="sidebar-CAFE">
                <title>CAFE Standards: Blurring the Lines Between Categorization Perspectives</title>
                <info>
                    <itermset>
                        <indexterm zone="sidebar-CAFE" condition="print" userlevel="Professional Graduate">
                            <primary>sidebar</primary>
                            <secondary>CAFE Standards</secondary>
                        </indexterm>
                        <indexterm zone="sidebar-CAFE">
                            <primary>CAFE Standards</primary>
                        </indexterm>
                        <indexterm zone="sidebar-CAFE">
                            <primary>categorization</primary>
                        </indexterm></itermset>
        </info>
                
                <para audience="CORE" xml:id="para-dlb_m4r_lr"><info><itermset>
                    <indexterm zone="para-dlb_m4r_lr">
                        <primary>CAFE</primary>
                    </indexterm></itermset>
        </info>The <firstterm xml:id="first_CAFE"
                                ><citerefentry><refentrytitle>Corporate Average Fuel
                                Economy</refentrytitle><manvolnum>CAFE</manvolnum></citerefentry></firstterm>
                    standards sort vehicles into <quote>passenger car</quote> and <quote>light
                        truck</quote> categories and impose higher minimum fuel efficiency
                    requirements for cars because trucks have different typical uses.</para>
                <para audience="CORE" xml:id="para-lmb_m4r_lr">When <abbrev>CAFE</abbrev> standards
                    were introduced, the vehicles classified as light trucks were generally used for
                        <quote>light duty</quote> farming and manufacturing purposes. <quote>Light
                        trucks</quote> might be thought of as a <quote>sort of</quote> in-between
                        category<symbol>&#8212;</symbol>a light truck is not really a car, but
                    sufficiently unlike a prototypical truck to qualify the vehicle’s categorization
                    as <quote>light.</quote> Formalizing this sense of in-between-ness by specifying
                    features that define a <quote>car</quote> and a <quote>light truck</quote> is
                    the only way to implement a consistent, transparent fuel efficiency policy that
                    makes use of informal, graded distinctions between vehicles.</para>
                <para audience="CORE" xml:id="para-tnb_m4r_lr"><info><itermset>
                    <indexterm zone="para-tnb_m4r_lr">
                        <primary>SUV</primary>
                    </indexterm></itermset>
        </info>A manufacturer whose average fuel economy for all the vehicles it
                    sells in a year falls below the <abbrev>CAFE</abbrev> standards has to pay
                    penalties. This encourages them to produce <quote>sport utility vehicles</quote>
                        (<abbrev>SUV</abbrev>s) that adhere to the <abbrev>CAFE</abbrev> definitions
                    of light trucks but which most people use as passenger cars. Similarly, the
                        <hardware>PT Cruiser</hardware>, a retro-styled hatchback produced by
                        <orgname>Chrysler</orgname> from <date>2000-2010</date>, strikes many people
                    as a car. It looks like a car; we associate it with the transport of passengers
                    rather than with farming; and in fact it is formally classified as a car under
                    emissions standards. But like <abbrev>SUV</abbrev>s, in the
                        <abbrev>CAFE</abbrev> classification system, the <hardware>PT
                        Cruiser</hardware> is a light truck.</para>
                <para audience="CORE" xml:id="para-z4b_m4r_lr"><abbrev>CAFE</abbrev> standards have
                    evolved over time, becoming a theater for political clashes between holistic
                    cultural categories and formal institutional categories, which plays out in
                    competing pressures from industry, government, and political organizations.
                    Furthermore, <abbrev>CAFE</abbrev> standards and manufacturers’ response to them
                    are influencing cultural categories, such that our cultural understanding of
                    what a car looks like is changing over time as manufacturers design vehicles
                    like the <hardware>PT Cruiser</hardware> with car functionality in
                    unconventional shapes to take advantage of the <abbrev>CAFE</abbrev> light truck
                        specifications.<footnote xml:id="endnote-345" label="407"
                        audience="Business">
                        <para audience="Business" xml:id="para-iqb_m4r_lr"><info>
                                <itermset>
                                    <indexterm audience="Markup" zone="endnote-345">
                                        <primary>endnote</primary>
                                        <secondary>Business</secondary>
                                    </indexterm>
                                    <indexterm zone="endnote-345">
                                        <primary>CAFE</primary>
                                    </indexterm>
                                    <indexterm zone="endnote-345">
                                        <primary>business</primary>
                                        <secondary>CAFE</secondary>
                                    </indexterm>
                                </itermset>
                            </info>The <citerefentry><refentrytitle>Corporate Average Fuel
                                    Economy</refentrytitle><manvolnum>CAFE</manvolnum></citerefentry>
                            standards have been developed by the <orgname>United States National
                                Highway Traffic Safety Administration</orgname> (<link
                                xmlns:xlink="http://www.w3.org/1999/xlink"
                                xlink:href="http://www.nhtsa.gov/fuel-economy"
                                    ><uri>http://www.nhtsa.gov/fuel-economy</uri></link>) since
                                <date>1975</date>. For a careful and critical assessment of <abbrev
                                xreflabel="CAFE">CAFE</abbrev>, including the politics of
                            categorization for vehicles like the <hardware>PT Cruiser</hardware>,
                            see the <citation xml:id="cite_Board2002" linkend="Board2002"
                                    ><date>2002</date> report</citation> from the <orgname>Committee
                                on the Effectiveness and Impact of Corporate Average Fuel
                                Economy</orgname> (<abbrev xreflabel="CAFE">CAFE</abbrev>)
                            Standards, National Research Council.</para>
                    </footnote></para>
            </sidebar>
            
            
        </section>
        <section label="7.2.5" xml:id="section-7.2.5">
            <title>Computational Categories</title>
            <info>
                <itermset>
                    <indexterm zone="section-7.2.5">
                        <primary>computational categories</primary>
                    </indexterm>
                    <indexterm zone="section-7.2.5">
                        <primary>categorization </primary>
                        <secondary>computational</secondary>
                    </indexterm>
                    <indexterm zone="section-7.2.5">
                        <primary>statistics </primary>
                        <secondary>category creation with</secondary>
                    </indexterm>
                    <indexterm zone="section-7.2.5">
                        <primary>machine learning</primary>
                    </indexterm>
                    <indexterm zone="section-7.2.5">
                        <primary>unsupervised learning</primary>
                    </indexterm>
                    <indexterm zone="section-7.2.5">
                        <primary>supervised learning</primary>
                    </indexterm>
                </itermset>
            </info>
            <para xml:id="para-h12_gcv_sv">Computational categories are created by computer programs
                when the number of resources, or when the number of descriptions or observations
                associated with each resource, are so large that people cannot think about them
                effectively. Computational categories are created for information retrieval,
                predictive analytics, and other applications where information scale or speed
                requirements are critical. The resulting categories are similar to those created by
                people in some ways but differ substantially in other ways. </para>
            <para xml:id="para-zrm_mnl_gw" revision="4.0" revisionflag="added">The simplest kind of
                computational categories can be created using descriptive statistics<phrase
                    role="parenthetical"> (see <xref linkend="section-3.3.4" xrefstyle="short"
                    />)</phrase>. Descriptive statistics do not identify the categories they create
                by giving them familiar cultural or institutional labels. Instead, they create
                implicit categories of items according to how much they differ from the most typical
                or frequent ones. For example, in any dataset where the values follow the normal
                distribution, statistics of central tendency and dispersion serve as standard
                reference measures for any observation. These statistics identify categories of
                items that are very different or statistically unlikely outliers, which could be
                signals of measurement errors, poorly calibrated equipment,  employees who are
                inadequately trained or committing fraud, or other problems. <phrase audience="DS"
                    >The <quote>Six Sigma</quote> methodology for process improvement and quality
                    control rests on this idea that careful and consistent collection of statistics
                    can make any measurable operation better.</phrase></para>
            <para xml:id="para-nbp_snl_gw" revision="4.0" revisionflag="added">Many text processing
                methods and applications use simple statistics to categorize words by their
                frequency in a language, in a collection of documents, or in individual documents,
                and these categories are exploited in many information retrieval applications (see
                    <xref linkend="section-10.4.1" xrefstyle="short"/> and <xref
                    linkend="section-10.4.2" xrefstyle="short"/>).</para>
            <sidebar xml:id="sidebar-SupervisedAndUnsupervised">
                <?dbhtml sidebar-width="50%"?>
                <?dbhtml float-type="right" ?>
                <?dbfo sidebar-width="50%"?>
                <?dbfo float-type="outside"?>
                <title>Supervised and Unsupervised Learning</title>
                <para xml:id="para-wgt_wql_gw"><phrase role="definition">Two subfields of <glossterm
                            linkend="gloss_machine_learning">machine learning</glossterm> that are
                        relevant to organizing systems are <firstterm
                            xml:id="first_supervised_learning">supervised</firstterm> and <firstterm
                            xml:id="first_unsupervised_learning">unsupervised</firstterm>
                        learning.</phrase>
                    <phrase xml:id="def_supervised_learning" role="definition">In <glossterm
                            xml:id="term_supervised_learning">supervised learning</glossterm>, a
                        machine learning program is trained with sample items or documents that are
                        labeled by category, and the program learns to assign new items to the
                        correct categories.</phrase>
                    <phrase xml:id="def_unsupervised_learning" role="definition">In <glossterm
                            xml:id="term_unsupervised_learning">unsupervised learning</glossterm>,
                        the program gets the same items but has to come up with the categories on
                        its own by discovering the underlying correlations between the items; that
                        is why unsupervised learning is sometimes called <glossterm
                            linkend="gloss_statistical_pattern_recognition">statistical pattern
                            recognition</glossterm>.</phrase>
                </para>
            </sidebar>
            <para audience="CORE Computing" xml:id="para-tpj_m4r_lr" revision="4.0"
                revisionflag="changed"><info>
                    <itermset>
                        <indexterm significance="preferred" zone="def_machine_learning">
                            <primary>machine learning</primary>
                        </indexterm>
                    </itermset>
                </info>Categories that people create and label also can be used more explicitly in
                computational algorithms and applications. In particular, a program that can assign
                an item or instance to one or more existing categories is called a classifier. The
                subfield of computer science known as <phrase role="definition"
                    xml:id="def_machine_learning"><glossterm xml:id="term_machine_learning"
                        xreflabel="machine learning" linkend="gloss_machine_learning">machine
                        learning</glossterm> is home to numerous techniques for creating classifiers
                    by training them with already correctly categorized examples. This training is
                    called <glossterm linkend="gloss_supervised_learning">supervised
                        learning</glossterm>; it is supervised because it starts with instances
                    labeled by category, and it involves learning because over time the classifier
                    improves its performance by adjusting the weights for features that distinguish
                    the categories. But strictly speaking, supervised learning techniques do not
                    learn the categories; they implement and apply categories that they inherit or
                    are given to them.</phrase>
                We will further discuss the computational implementation of
                categories created by people in <xref linkend="section-7.5" xrefstyle="short"/>.</para>
            <para audience="CORE Computing" revision="4.0" revisionflag="changed"
                xml:id="para-un2_m4l_gw"><phrase role="definition">In contrast, many computational
                    techniques in machine learning can analyze a collection of resources to discover
                    statistical regularities or correlations among the items, creating a set of
                    categories without any labeled training data. This is called <firstterm
                        linkend="gloss_unsupervised_learning">unsupervised learning</firstterm> or <phrase>
                        <glossterm linkend="gloss_statistical_pattern_recognition">statistical
                            pattern recognition</glossterm>.</phrase></phrase> As we pointed out in
                    <xref linkend="section-7.2.1" xrefstyle="short"/>, we learn most of our cultural
                categories without any explicit instruction about them, so it is not surprising that
                computational models of categorization developed by cognitive scientists often
                employ unsupervised statistical learning methods.</para>
            <para xml:id="para-c3m_vpl_gw" revision="4.0" revisionflag="added">Many computational
                categories are like individual categories because they are tied to specific
                collections of resources or data and are designed to satisfy narrow goals. The
                individual categories you use to organize your email inbox or the files on your
                computer reflect your specific interests, activities, and personal network and are
                surely different than those of anyone else. Similarly, your credit card company
                analyzes your specific transactions to create computational categories of
                    <quote>likely good</quote> and <quote>likely fraudulent</quote> that are
                different for every cardholder. </para>
            <para revision="4.0" revisionflag="added" xml:id="para-mfy_4w5_gw"><info>
                    <itermset>
                        <indexterm zone="para-mfy_4w5_gw">
                            <primary>NAICS</primary>
                        </indexterm>
                        <indexterm zone="para-mfy_4w5_gw">
                            <primary>UNSPC</primary>
                        </indexterm>
                    </itermset>
                </info>This focused scope is obvious when we consider how we might describe a
                computational category. <quote>Fraudulent transaction for cardholder
                    4264123456780123</quote> is not lexicalized with a one-word label as familiar
                cultural categories are. <quote>Door</quote> and <quote>window</quote> have broad
                scopes that are not tied to a single purpose. Put another way, the
                    <quote>door</quote> and <quote>window</quote> cultural categories are highly
                reusable, as are institutional categories like those used to collect economic or
                health data that can be analyzed for many different purposes. The definitions of
                    <quote>door</quote> and <quote>window</quote> might be a little fuzzy, but
                institutional categories are more precisely defined, often by law or regulation.
                Examples are the <firstterm xml:id="first_NAICS"><citerefentry xml:id="ref_NAICS"
                            ><refentrytitle>North American Industry Classification
                            System</refentrytitle><manvolnum>NAICS</manvolnum></citerefentry></firstterm>
                from the <orgname>US Census Bureau</orgname> and the <firstterm xml:id="first_UNSPC"
                        ><citerefentry xml:id="ref_UNSPC"><refentrytitle>United Nations Standard
                            Products and Services
                        Code</refentrytitle><manvolnum>UNSPC</manvolnum></citerefentry></firstterm>.</para>
            <para xml:id="para-aq2_jql_gw" revision="4.0" revisionflag="added">A final contrast
                between categories created by people and those created computationally is that the
                former can almost always be inspected and reasoned about by other people, but only
                some of the latter can. A computational model that categorizes loan applicants as
                good or poor credit risks probably uses properties like age, income, home address,
                and marital status, so that a banker can understand and explain a credit decision.
                However, many other computational categories, especially those that created by
                clustering and deep learning techniques, are inseparable from the mathematical model
                that learned to use them, and as a result are uninterpretable by people. </para>
            <para revision="4.0" revisionflag="added" xml:id="para-nly_hs1_qw" audience="DS">A
                machine learning algorithm for classifying objects in images creates a complex
                multi-layer neural network whose features have no clear relationship to the
                categories, and this network has no other use. Put another way, machine learning
                programs are very general because they can be employed in any domain with high
                dimensional data, but what they learn cannot be applied in any other domain.</para>
        </section>
        
        <!-- ###################### SECTION ########################################### -->
    </section>
    <!--              -->
    <section xml:id="section-7.3" label="7.3">
        <title>Principles for Creating Categories</title>
        <info>
            <itermset>
                <indexterm zone="section-7.3">
                    <primary>categories</primary>
                    <secondary>creating</secondary>
                </indexterm></itermset>
        </info>
        <para audience="CORE" xml:id="para-fyb_m4r_lr"><xref linkend="section-7.2"/> explained what
            categories are and the contrasting cultural, individual, and institutional contexts and
            purposes for which categories are created. In doing so, a number of different principles
            for creating categories were mentioned, mostly in passing.</para>
        <para audience="CORE" xml:id="para-ozb_m4r_lr">We now take a systematic look at principles
            for creating categories, including: enumeration, single properties, multiple properties
            and hierarchy, probabilistic, similarity, and theory- and goal-based categorization.
            These ways of creating categories differ in the information and mechanisms they use to
            determine category membership. </para>
        <!--              -->
        <section xml:id="section-7.3.1" label="7.3.1">
            <title>Enumeration</title>
            <info>
                <itermset>
                    <indexterm zone="section-7.3.1">
                        <primary>enumeration</primary>
                    </indexterm>
                    <indexterm zone="section-7.3.1">
                        <primary>principle</primary>
                        <secondary>category creation</secondary>
                        <tertiary>enumeration</tertiary>
                    </indexterm></itermset>
        </info>

            <para audience="CORE" xml:id="para-x1c_m4r_lr"><info><itermset>
                <indexterm zone="para-x1c_m4r_lr">
                    <primary>ISO</primary>
                    <secondary>currency codes</secondary>
                </indexterm></itermset>
        </info><phrase role="definition" xml:id="def_extensional_definition"><phrase
                        role="definition" xml:id="def_enumeration">The simplest principle for
                        creating a category is <glossterm linkend="gloss_enumeration"
                            >enumeration</glossterm>; any resource in a finite or countable set can
                        be deemed a category member by that fact alone.</phrase> This principle is
                    also known as <glossterm xml:id="term_extensional_definition">extensional
                        definition</glossterm>, and the members of the set are called the <glossterm
                        xml:id="term_extension">extension</glossterm>.</phrase> Many institutional
                categories are defined by enumeration as a set of possible or legal values, like the
                50 United States or the <abbrev>ISO</abbrev> currency codes
                        (<citerefentry><refentrytitle>ISO
                4217</refentrytitle></citerefentry>).</para>
            <para audience="CORE" xml:id="para-fcc_m4r_lr"><phrase role="principle statement"
                    >Enumerative categories enable membership to be unambiguously determined because
                    a value like state name or currency code is either a member of the category or
                    it is not. However, this clarity has a downside; it makes it hard to argue that
                    something not explicitly mentioned in an enumeration should be considered a
                    member of the category, which can make laws or regulations inflexible. Moreover,
                    there comes a size when enumerative definition is impractical or inefficient,
                    and the category either must be sub-divided or be given a definition based on
                    principles other than enumeration.</phrase><footnote audience="Law" label="408"
                    revision="4.0" revisionflag="added" xml:id="endnote-346a">
                    <para xml:id="para-zw4_f41_fw" audience="Law">Legal disputes often reflect different
                        interpretations of category membership and whether a list of category
                        members is exhaustive or merely illustrative. The legal principle of
                            <quote>implied exclusion</quote>—<foreignphrase>expressio unius est
                            exclusio alterius</foreignphrase> —says that if you <quote>expressly
                            name</quote> or <quote>designate</quote> an enumeration of one or more
                        things, any thing that is not named is excluded, by implication. However,
                        prefacing the list with <quote>such as,</quote>
                        <quote>including,</quote> or <quote>like</quote> implies that it is not a
                        strict enumeration because there might be other members.</para>
                </footnote></para>
            
            
            <sidebar xml:id="sidebar-6.3.1-TooManyPlanets">
                <?dbhtml sidebar-width="50%"?>
                <?dbhtml float-type="right" ?>
                <?dbfo sidebar-width="50%"?>
                <?dbfo float-type="outside"?>
                <title>Too Many Planets to Enumerate: Keeping up with Kepler</title>
                <info>
                    <itermset>
                        <indexterm zone="sidebar-6.3.1-TooManyPlanets" condition="print" userlevel="Professional Graduate">
                            <primary>sidebar</primary>
                            <secondary>Too Many Planets to Enumerate</secondary>
                        </indexterm>
                        <indexterm zone="sidebar-6.3.1-TooManyPlanets" userlevel="Professional Graduate">
                            <primary>Too Many Planets to Enumerate</primary>
                        </indexterm>
                        <indexterm zone="sidebar-6.3.1-TooManyPlanets">
                            <primary>planets</primary>
                        </indexterm>
                        <indexterm zone="sidebar-6.3.1-TooManyPlanets" userlevel="Professional Graduate">
                            <primary>Kepler observatory</primary>
                        </indexterm></itermset>
        </info>
                
                <para audience="CORE" xml:id="para-mdc_m4r_lr"><link
                        xmlns:xlink="http://www.w3.org/1999/xlink"
                        xlink:href="http://kepler.nasa.gov/">Kepler</link> is a space observatory
                    launched by NASA in <date>2009</date> to search for Earth-like planets orbiting
                    other stars in our own Milky Way galaxy. Kepler has already discovered and
                    verified a few thousand new planets, and these results have led to estimates
                    that there may be at least as many planets as there are stars, a few hundred
                    billion in the Milky Way alone. Count fast. </para>
                
            </sidebar>
            
            <para audience="CORE" xml:id="para-x2c_m4r_lr"><info userlevel="Professional Graduate"><itermset>
                <indexterm zone="para-x2c_m4r_lr">
                    <primary>planets, enumerated</primary>
                </indexterm><indexterm zone="para-x2c_m4r_lr">
                    <primary>Mercury</primary>
                </indexterm><indexterm zone="para-x2c_m4r_lr">
                    <primary>Venus</primary>
                    <secondary>planet</secondary>
                </indexterm><indexterm zone="para-x2c_m4r_lr">
                    <primary>Earth</primary>
                </indexterm><indexterm zone="para-x2c_m4r_lr">
                    <primary>Mars</primary>
                </indexterm><indexterm zone="para-x2c_m4r_lr">
                    <primary>Jupiter</primary>
                </indexterm><indexterm zone="para-x2c_m4r_lr">
                    <primary>Saturn</primary>
                </indexterm><indexterm zone="para-x2c_m4r_lr">
                    <primary>Uranus</primary>
                </indexterm><indexterm zone="para-x2c_m4r_lr">
                    <primary>Neptune</primary>
                </indexterm><indexterm zone="para-x2c_m4r_lr">
                    <primary>Pluto</primary>
                    <secondary>planet</secondary>
                </indexterm></itermset>
        </info>For example, for millennia we earthlings have had a cultural category
                of <quote>planet</quote> as a <quote>wandering</quote> celestial object, and because
                we only knew of planets in our own solar system, the planet category was defined by
                enumeration: Mercury, Venus, Earth, Mars, Jupiter, and Saturn. When the outer
                planets of Uranus, Neptune, and Pluto were identified as planets in the
                    18<superscript>th</superscript>-20<superscript>th</superscript> centuries, they
                were added to this list of planets without any changes in the cultural category. But
                in the last couple of decades many heretofore unknown planets outside our solar
                system have been detected, making the set of planets unbounded, and definition by
                enumeration no longer works.</para>
            <para audience="CORE" xml:id="para-ggc_m4r_lr"><info><itermset>
                <indexterm zone="para-ggc_m4r_lr">
                    <primary>IAU</primary>
                </indexterm><indexterm zone="para-ggc_m4r_lr">
                <primary>Pluto</primary>
                <secondary>inferior planet</secondary>
            </indexterm></itermset>
        </info>The <firstterm xml:id="first_IAU"><citerefentry xml:id="ref_IAU"
                                ><refentrytitle><orgname>International Astronomical
                            Union</orgname></refentrytitle><manvolnum>IAU</manvolnum></citerefentry></firstterm>
                thought it solved this category crisis by proposing a definition of planet as
                    <quote>a celestial body that is (a) in orbit around a star, (b) has sufficient
                    mass for its self-gravity to overcome rigid body forces so that it assumes a
                    hydrostatic equilibrium (nearly round) shape, and (c) has cleared the
                    neighborhood around its orbit.</quote>
                Unfortunately, Pluto does not satisfy the third requirement, so it no
                longer is a member of the planet category, and instead is now called an
                    <quote>inferior planet.</quote></para>
                        <para audience="CORE" xml:id="para-ohc_m4r_lr">Changing the definition of a
                significant cultural category generated a great deal of controversy and angst among
                ordinary non-scientific people. A typical headline was <quote>Pluto’s demotion has
                    schools spinning,</quote> describing the outcry from elementary school students
                and teachers about the injustice done to Pluto and the disruption on the curriculum.
                    <footnote xml:id="endnote-346b" label="409" audience="LIS" revision="4.0"
                    revisionflag="changed">
                    <para audience="LIS" xml:id="para-y3c_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-346b">
                                    <primary>endnote</primary>
                                    <secondary>LIS</secondary>
                                </indexterm>
                                <indexterm zone="endnote-346b">
                                    <primary>library science</primary>
                                    <secondary>Pluto</secondary>
                                </indexterm>
                                <indexterm zone="endnote-346b">
                                    <primary>IAU</primary>
                                </indexterm>
                                <indexterm zone="endnote-346b">
                                    <primary>Pluto</primary>
                                    <secondary>documentary</secondary>
                                </indexterm>
                            </itermset>
                        </info><citerefentry><refentrytitle><orgname>International Astronomical
                                    Union</orgname></refentrytitle><manvolnum>IAU</manvolnum></citerefentry>
                        (iau.org) published its new definition of planet in <date>August
                        2006</date>. A public television documentary in <date>2011</date> called
                            <citetitle>The Pluto Files</citetitle> retells the story <citation
                            xml:id="cite_Tyson2011" linkend="Tyson2011">(Tyson
                        2011)</citation>.</para>
                </footnote></para>
            
        </section>
        <!--              -->
        <section xml:id="section-7.3.2" label="7.3.2">
            <title>Single Properties</title>
            <info>
                <itermset>
                    <indexterm zone="section-7.3.2">
                        <primary>single properties</primary>
                    </indexterm>
                    <indexterm zone="section-7.3.2">
                        <primary>principle</primary>
                        <secondary>category creation</secondary>
                        <tertiary>single properties</tertiary>
                    </indexterm></itermset>
        </info>

            <para audience="CORE" xml:id="para-kkc_m4r_lr"><info><itermset>
                <indexterm significance="preferred" zone="def_intension">
                    <primary>intensional definition</primary>
                </indexterm><indexterm zone="def_intension">
                    <primary>intension</primary>
                </indexterm><indexterm zone="def_intension">
                    <primary>property</primary>
                    <secondary>intension</secondary>
                </indexterm></itermset>
        </info><phrase role="statement">It is intuitive
                    and useful to think in terms of properties when we identify instances and when
                    we are describing instances (as we saw in <xref linkend="section-4.3"/> and in
                        <xref linkend="chapter-5"/>).</phrase> Therefore, it should also be
                intuitive and useful to consider properties when we analyze more than one instance
                to compare and contrast them so we can determine which sets of instances can be
                treated as a category or <glossterm linkend="gloss_equivalence_class">equivalence
                    class</glossterm>. <phrase role="definition" xml:id="def_intension">Categories whose
                    members are determined by one or more properties or rules follow the principle
                    of <glossterm xml:id="term_intensional_definition">intensional
                        definition</glossterm>, and the defining properties are called the
                        <glossterm xml:id="term_intension" xreflabel="intension"
                        >intension</glossterm>.</phrase></para>
            <para audience="CORE" xml:id="para-rlc_m4r_lr"><phrase role="interrogative"> You might
                    be thinking here that enumeration or extensional definition of a category is
                    also a property test; is not <quote>being a state</quote> a property of
                    California? But statehood is not a property precisely because
                        <quote>state</quote> is defined by extension, which means the only way to
                    test California for statehood is to see if it is in the list of
                    states.</phrase><footnote xml:id="endnote-347" label="410" audience="Philosophy">
                    <para audience="Philosophy" xml:id="para-anc_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-347">
                                    <primary>endnote</primary>
                                    <secondary>Philosophy</secondary>
                                </indexterm>
                                <indexterm zone="endnote-347">
                                    <primary>philosophy</primary>
                                    <secondary>intension and extension</secondary>
                                </indexterm>
                                <indexterm zone="endnote-347">
                                    <primary>Gottlob, Frege</primary>
                                </indexterm>
                            </itermset>
                        </info>The distinction between intension and extension was introduced by
                                <personname><firstname>Gottlob</firstname>
                            <surname>Frege</surname></personname>, a German philosopher and
                        mathematician <citation xml:id="cite_Frege1892" linkend="Frege1892">(Frege
                            1892)</citation>. </para>
                </footnote></para>
            <para audience="CORE" xml:id="para-j4c_m4r_lr"><phrase role="principle statement">Any
                        <emphasis>single property</emphasis> of a resource can be used to create
                    categories, and the easiest ones to use are often the intrinsic static
                    properties.</phrase> As we discussed in <xref linkend="chapter-5"/>, intrinsic
                static properties are those inherent in a resource that never change. The material
                of composition of natural or manufactured objects is an intrinsic and static
                property that can be used to arrange physical resources. For example, an organizing
                system for a personal collection of music that is based on the intrinsic static
                property of physical format might use categories for CDs, DVDs, vinyl albums,
                8-track cartridges, reel-to-reel tape and tape cassettes.<footnote
                    xml:id="endnote-348" label="411" audience="CogSci">
                    <para audience="CogSci" xml:id="para-ppc_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-348">
                                    <primary>endnote</primary>
                                    <secondary>CogSci</secondary>
                                </indexterm>
                                <indexterm zone="endnote-348">
                                    <primary>cognitive science</primary>
                                    <secondary>conflation of properties</secondary>
                                </indexterm>
                            </itermset>
                        </info>The number of resources in each of these categories depends on the
                        age of the collection and the collector. We could be more precise here and
                        say <quote>single atomic property</quote> or otherwise more carefully define
                            <quote>property</quote> in this context as a characteristic that is
                        basic and not easily or naturally decomposable into other characteristics.
                        It would be possible to analyze the physical format of a music resource as a
                        composition of size, shape, weight, and material substance properties, but
                        that is not how people normally think. Instead, they treat physical format
                        as a single property as we do in this example.</para>
                </footnote>
            </para>
            <para audience="CORE" xml:id="para-crc_m4r_lr">Using a single property is most natural
                to do when the properties can take on only a small set of discrete values like music
                formats, and especially when the property is closely related to how the resources
                are used, as they are with the music collection where each format requires different
                equipment to listen to the music. Each value then becomes a subcategory of the music
                category.</para>
            <para audience="CORE" role="comparative" xml:id="para-ksc_m4r_lr"><info><itermset>
                <indexterm zone="para-ksc_m4r_lr">
                    <primary>location</primary>
                    <secondary>resource creation</secondary>
                </indexterm></itermset>
        </info>The author, date, and location of creation of an intellectual resource
                cannot be directly perceived but they are also intrinsic static properties. The
                subject matter or purpose of a resource, its <quote role="interrogative">what it is
                    about</quote> or <quote role="interrogative">what it was originally for,</quote>
                are also intrinsic static properties that are not directly perceivable, especially
                for information resources.</para>
            <para audience="CORE" role="comparative" xml:id="para-utc_m4r_lr"><info>
                    <itermset>
                        <indexterm zone="para-ksc_m4r_lr">
                            <primary>alphabetical ordering</primary>
                        </indexterm>
                        <indexterm zone="para-ksc_m4r_lr">
                            <primary>numerical ordering</primary>
                        </indexterm>
                    </itermset>
                </info><phrase role="principle statement">The name or identifier of a resource is
                    often arbitrary but once assigned normally does not change, making it an
                    extrinsic static property.</phrase> Any collection of resources with alphabetic
                or numeric identifiers as an associated property can use sorting order as an
                organizing principle to arrange spices, books, personnel records, etc., in a
                completely reliable way. Some might argue whether this organizing principle creates
                a category system, or whether it simply exploits the ordering inherent in the
                identifier notation. For example, with alphabetic identifiers, we can think of
                alphabetic ordering as creating a recursive category system with 26 (A-Z) top-level
                categories, each containing the same number of second-level categories, and so on
                until every instance is assigned to its proper place.<footnote xml:id="endnote-349"
                    label="412" audience="CogSci">
                    <para audience="CogSci" xml:id="para-dvc_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-349">
                                    <primary>endnote</primary>
                                    <secondary>CogSci</secondary>
                                </indexterm>
                                <indexterm zone="endnote-349">
                                    <primary>alphabetical ordering</primary>
                                    <secondary>logical versus physical</secondary>
                                </indexterm>
                                <indexterm zone="endnote-349">
                                    <primary>cognitive science</primary>
                                    <secondary>ordering</secondary>
                                </indexterm>
                            </itermset>
                        </info>We need to think of alphabetic ordering or any other organizing
                        principle in a logical way that does not imply any particular physical
                        implementation. Therefore, we do not need to consider which of these
                        alphabetic categories exist as folders, files, or other tangible
                        partitions.</para>
                </footnote></para>
            <para audience="CORE" xml:id="para-lwc_m4r_lr"><info><itermset>
                <indexterm zone="para-lwc_m4r_lr">
                    <primary>location</primary>
                    <secondary>current</secondary>
                </indexterm></itermset>
        </info><phrase role="principle statement">Some resource properties are both
                    extrinsic and dynamic because they are based on usage or behaviors that can be
                    highly context-dependent.</phrase> The current owner or location of a resource,
                its frequency of access, the joint frequency of access with other resources, or its
                current rating or preference with respect to alternative resources are typical
                extrinsic and dynamic properties that can be the basis for arranging resources and
                defining categories.</para>
            <para audience="CORE" role="contrast" xml:id="para-xxc_m4r_lr">These properties can have
                a large number of values or are continuous measures, but as long as there are
                explicit rules for using property values to determine category assignment the
                resulting categories are still easy to understand and use. For example, we naturally
                categorize people we know on the basis of their current profession, the city where
                they live, their hobbies, or their age. Properties with a numerical dimension like
                    <quote>frequency of use</quote> are often transformed into a small set of
                categories like <quote>frequently used,</quote>
                <quote>occasionally used,</quote> and <quote>rarely used</quote> based on the
                numerical property values.<footnote xml:id="endnote-350" label="413"
                    audience="CogSci">
                    <para audience="CogSci" xml:id="para-hzc_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-350">
                                    <primary>endnote</primary>
                                    <secondary>CogSci</secondary>
                                </indexterm>
                                <indexterm zone="endnote-350">
                                    <primary>costs</primary>
                                    <secondary>computed</secondary>
                                </indexterm>
                                <indexterm zone="endnote-350">
                                    <primary>cognitive science</primary>
                                    <secondary>context dependency</secondary>
                                </indexterm>
                            </itermset>
                        </info>Another example: rules for mailing packages might use either size or
                        weight to calculate the shipping cost, and whether these rules are based on
                        specific numerical values or ranges of values, the intent seems to be to
                        create categories of packages.</para>
                </footnote></para>
            <para audience="CORE" role="contrast" xml:id="para-o1d_m4r_lr">While there are an
                infinite number of logically expressible properties for any resource, most of them
                would not lead to categories that would be interpretable and useful for people. If
                people are going to use the categories, it is important to base them on properties
                that are psychologically or pragmatically relevant for the resource domain being
                categorized. Whether something weighs more or less than 5000 pounds is a poor
                property to apply to things in general, because it puts cats and chairs in one
                category, and buses and elephants in another.<footnote xml:id="endnote-351"
                    label="414" audience="CogSci">
                    <para audience="CogSci" xml:id="para-xbd_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-351">
                                    <primary>endnote</primary>
                                    <secondary>CogSci</secondary>
                                </indexterm>
                                <indexterm zone="endnote-351">
                                    <primary>cognitive science</primary>
                                    <secondary>property relevance</secondary>
                                </indexterm>
                            </itermset>
                        </info>If you try hard, you can come up with situations in which this
                        property is important, as when the circus is coming to the island on a ferry
                        or when you are loading an elevator with a capacity limit of 5000 pounds,
                        but it just is not a useful or psychologically salient property in most
                        contexts.</para>
                </footnote></para>
            <para audience="CORE" xml:id="para-ddd_m4r_lr">To summarize: <phrase
                    role="principle statement">The most useful single properties to use for creating
                    categories for an organizing system used by people are those that are formally
                    assigned, objectively measurable and orderable, or tied to well-established
                    cultural categories, because the resulting categories will be easier to
                    understand and describe.</phrase></para>
            <para audience="CORE" role="contrast" xml:id="para-o2d_m4r_lr"><phrase
                    role="principle statement">If only a single property is used to distinguish
                    among some set of resources and to create the categories in an organizing
                    system, the choice of property is critical because different properties often
                    lead to different categories.</phrase> Using the age property,
                        <personname><firstname>Bill</firstname>
                    <surname>Gates</surname></personname> and
                        <personname><firstname>Mark</firstname>
                    <surname>Zuckerberg</surname></personname> are unlikely to end up in the same
                category of people. Using the wealth property, they most certainly would.
                Furthermore, if only one property is used to create a system of categories, any
                category with a large numbers of items in it will lack coherence because differences
                on other properties will be too apparent, and some category members will not fit as
                well as the others.</para>
        </section>
        <!--              -->
        <section xml:id="section-7.3.3" label="7.3.3">
            <title>Multiple Properties</title>
            <info>
                <itermset>
                    <indexterm zone="section-7.3.3">
                        <primary>multiple properties</primary>
                    </indexterm>
                    <indexterm zone="section-7.3.2">
                        <primary>principle</primary>
                        <secondary>category creation</secondary>
                        <tertiary>multiple properties</tertiary>
                    </indexterm></itermset>
        </info>

            <para audience="CORE" xml:id="para-agd_m4r_lr"><phrase role="statement">Organizing
                    systems often use multiple properties to define categories.</phrase> There are
                three different ways in which to do this that differ in the scope of the properties
                and how essential they are in defining the categories.</para>
            <!--              -->
            <section xml:id="section-7.3.3.1" label="7.3.3.1">
                <title>Multi-Level or Hierarchical Categories</title>
                <info>
                    <itermset>
                        <indexterm zone="section-7.3.3.1">
                            <primary>overfitting</primary>
                        </indexterm>
                    </itermset>
                </info>

                <para audience="CORE" xml:id="para-nhd_m4r_lr">If you have many shirts in your
                    closet (and you are a bit compulsive or a <quote>neat freak</quote>), instead of
                    just separating your shirts from your pants using a single property (the part of
                    body on which the clothes are worn) you might arrange the shirts by style, and
                    then by sleeve length, and finally by color. <phrase role="principle statement"
                        >When all of the resources in an organizing system are arranged using the
                        same sequence of resource properties, this creates a <glossterm
                            linkend="gloss_logical_hierarchy">logical hierarchy</glossterm>, a
                        multi-level category system.</phrase></para>
                <para audience="CORE" xml:id="para-v3d_m4r_lr">If we treat all the shirts as the
                    collection being organized, in the shirt organizing system the broad category of
                    shirts is first divided by style into categories like <quote>dress
                        shirts,</quote>
                    <quote>work shirts,</quote>
                    <quote>party shirts,</quote> and <quote>athletic or sweatshirts.</quote> Each of
                    these style categories is further divided until the categories are very narrow
                    ones, like the <quote>white long-sleeve dress shirts</quote> category. A
                    particular shirt ends up in this last category only after passing a series of
                    property tests along the way: it is a dress shirt, it has long sleeves, and it
                    is white. Each test creates more precise categories in the intersections of the
                    categories whose members passed the prior property tests.</para>
                <para audience="CORE" xml:id="para-ekd_m4r_lr"><info>
                        <itermset>
                            <indexterm zone="para-ekd_m4r_lr">
                                <primary>inherited</primary>
                            </indexterm>
                            <indexterm zone="para-ekd_m4r_lr">
                                <primary>property</primary>
                                <secondary>inherited</secondary>
                            </indexterm>
                            <indexterm zone="para-ekd_m4r_lr">
                                <primary>attribute</primary>
                                <secondary>inherited value</secondary>
                            </indexterm>
                        </itermset>
                    </info><phrase role="statement">Put another way, each subdivision of a category
                        takes place when we identify or choose a property that differentiates the
                        members of the category in a way that is important or useful for some intent
                        or purpose.</phrase> Shirts differ from pants in the value of the
                        <quote>part of body</quote> property, and all the shirt subcategories share
                    this <quote>top part</quote> value of that property. However, shirts differ on
                    other properties that determine the subcategory to which they belong. Even as we
                    pay attention to these differentiating properties, it is important to remember
                    the other properties, the ones that members of a category at any level in the
                    hierarchy have in common with the members of the categories that contain it.
                    These properties are often described as <quote>inherited</quote> or
                        <quote>inferred</quote> from the broader category.<footnote
                        xml:id="endnote-352" label="415" audience="Computing">
                        <para audience="Computing" xml:id="para-amd_m4r_lr"><info>
                                <itermset>
                                    <indexterm audience="Markup" zone="endnote-352">
                                        <primary>endnote</primary>
                                        <secondary>Computing</secondary>
                                    </indexterm>
                                    <indexterm zone="endnote-352">
                                        <primary>computing</primary>
                                        <secondary>inherited properties</secondary>
                                    </indexterm>
                                </itermset>
                            </info>Many information systems, applications, and programming languages
                            that work with hierarchical categories take advantage of this logical
                            relationship to infer inherited properties when they are needed rather
                            than storing them redundantly.</para>
                    </footnote> For example, just as every shirt shares the <quote>worn on top part
                        of body</quote> property, every item of clothing shares the <quote>can be
                        worn on the body</quote> property, and every resource in the
                        <quote>shirts</quote> and <quote>pants</quote> category inherits that
                    property.</para>
                <para audience="CORE" xml:id="para-snd_m4r_lr"><phrase role="interrogative">Each
                        differentiating property creates another level in the category hierarchy,
                        which raises an obvious question: How many properties and levels do we
                        need?</phrase> In order to answer this question we must reflect upon the
                    shirt categories in our closet. Our organizing system for shirts arranges them
                    with the three properties of style, sleeve length, and color; some of the
                    categories at the lowest level of the resulting hierarchy might have only one
                    member, or no members at all. You might have yellow or red short-sleeved party
                    shirts, but probably do not have yellow or red long-sleeved dress shirts, making
                    them empty categories. <phrase role="principle statement">Obviously, any
                        category with only one member does not need any additional properties to
                        tell the members apart, so a category hierarchy is logically complete if
                        every resource is in a category by itself.</phrase></para>
                <para audience="CORE" role="contrast" xml:id="para-ipd_m4r_lr"><info>
                        <itermset>
                            <indexterm zone="para-ipd_m4r_lr">
                                <primary>substitution</primary>
                            </indexterm>
                        </itermset>
                    </info>However, even when the lowest level categories of our shirt organizing
                    system have more than one member, we might choose not to use additional
                    properties to subdivide it because the differences that remain among the members
                    do not matter to us for the interactions the organizing system needs to support.
                    Suppose we have two long-sleeve white dress shirts from different shirt makers,
                    but whenever we need to wear one of them, we ignore this property. Instead, we
                    just pick one or the other, treating the shirts as completely equivalent or
                    substitutable. <phrase role="statement">When the remaining differences between
                        members of a category do not make a difference to the users of the category,
                        we can say that the organizing system is pragmatically or practically
                        complete even if it is not yet logically complete.</phrase> That is to say,
                    it is complete <quote>for all intents and purposes.</quote> Indeed, we might
                    argue that it is desirable to stop subdividing a system of categories while
                    there are some small differences remaining among the items in each category
                    because this leaves some flexibility or logical space in which to organize new
                    items. This point might remind you of the concept of overfitting, where models
                    with many parameters can very accurately fit their training data, but as a
                    result generalize less well to new data. <phrase role="parenthetical">(See <xref
                            linkend="section-5.3.2.5" xrefstyle="short"/>.)</phrase></para>
                <para audience="CORE" role="contrast" xml:id="para-ard_m4r_lr">On the other hand,
                    consider the shirt section of a big department store. Shirts there might be
                    organized by style, sleeve length, and color as they are in our home closet, but
                    would certainly be further organized by shirt maker and by size to enable a
                    shopper to find a <orgname>Marc Jacobs</orgname> long-sleeve blue dress shirt of
                    size 15/35. The department store organizing system needs more properties and a
                    deeper hierarchy for the shirt domain because it has a much larger number of
                    shirt instances to organize and because it needs to support many shirt shoppers,
                    not just one person whose shirts are all the same size.</para>

                <sidebar xml:id="sidebar-SwimsuitClassification">
                    <title>Classifying Hawaiian <quote>Boardshorts</quote></title>
                    <info>
                        <itermset>
                            <indexterm zone="sidebar-SwimsuitClassification" condition="print" userlevel="Professional Graduate">
                                <primary>sidebar</primary>
                                <secondary>Classifying Hawaiian Boardshorts</secondary>
                            </indexterm>
                            <indexterm zone="sidebar-SwimsuitClassification" condition="print" userlevel="Professional Graduate">
                                <primary>Classifying Hawaiian Boardshorts</primary>
                            </indexterm>
                            <indexterm zone="sidebar-SwimsuitClassification">
                                <primary>classification</primary>
                            </indexterm></itermset>
        </info>
                    
                    <?dbhtml sidebar-width="50%"?>
                    <?dbhtml float-type="right"?>

                    <informalfigure xml:id="PICTURE-S6.3.3.1-SwimsuitClassification"
                        xreflabel="Swimsuit Classification">
                        <mediaobject>
                            <imageobject>
                                <imagedata fileref="Pictures/6.3.3.1-SwimsuitClassification-OK.JPG"
                                    format="JPG"/>
                            </imageobject>
                            <textobject>
                                <phrase role="ALT descriptive">A museum display of Hawaiian
                                        <quote>boardshorts.</quote></phrase>
                            </textobject>
                            <caption><para audience="CORE" xml:id="para-jsd_m4r_lr"><phrase role="caption">The swimsuits worn by surfers,
                                        called <quote>boardshorts,</quote> have evolved from purely
                                        functional garments to symbols of extreme sports and the
                                        Hawaiian lifestyle. A <link
                                            xmlns:xlink="http://www.w3.org/1999/xlink"
                                            xlink:href="http://honolulumuseum.org/art/exhibitions/12868-board_shorts_perfect_fit"
                                                ><date>2012</date> exhibition</link> at the
                                            <orgname>Honolulu Museum of Art</orgname> captured the
                                        diversity of boardshorts on three facets: their material, 
                                        how they fastened around the
                                        surfer’s fly and waist, and their length.</phrase></para>
                                <para audience="CORE" xml:id="para-ttd_m4r_lr"><phrase role="credit"
                                        >(Photo by R. Glushko.)</phrase></para></caption>
                        </mediaobject>
                    </informalfigure>

                </sidebar>

            </section>
            <!--              -->
            <section xml:id="section-7.3.3.2" label="7.3.3.2">
                <title>Different Properties for Subsets of Resources</title>

                <para audience="CORE" role="contrast" xml:id="para-cvd_m4r_lr"><phrase
                        role="statement principle">A different way to use multiple resource
                        properties to create categories in an organizing system is to employ
                        different properties for distinct subsets of the resources being organized.
                        This contrasts with the strict multi-level approach in which every resource
                        is evaluated with respect to every property.</phrase> Alternatively, we
                    could view this principle as a way of organizing multiple domains that are
                    conceptually or physically adjacent, each of which has a separate set of
                    categories based on properties of the resources in that domain. This principle
                    is used for most folder structures in computer file systems and by many email
                    applications; you can create as many folder categories as you want, but any
                    resource can only be placed in one folder.</para>
                
                
                
                
                <para audience="CORE" role="contrast" xml:id="para-lwd_m4r_lr">The contrasts between
                    intrinsic and extrinsic properties, and between static and dynamic ones, are
                    helpful in explaining this method of creating organizing categories. For
                    example, you might organize all of your clothes using intrinsic static
                    properties if you keep your shirts, socks, and sweaters in different drawers and
                    arrange them by color; extrinsic static properties if you share your front hall
                    closet with a roommate, so you each use only one side of that closet space;
                    intrinsic dynamic properties if you arrange your clothes for ready access
                    according to the season; and, extrinsic dynamic properties if you keep your most
                    frequently used jacket and hat on a hook by the front door.<footnote
                        xml:id="endnote-353" label="416" audience="Business">
                        <para audience="Business" xml:id="para-rxd_m4r_lr"><info>
                                <itermset>
                                    <indexterm audience="Markup" zone="endnote-353">
                                        <primary>endnote</primary>
                                        <secondary>Business</secondary>
                                    </indexterm>
                                    <indexterm zone="endnote-353">
                                        <primary>business</primary>
                                        <secondary>merchandise display</secondary>
                                    </indexterm>
                                </itermset>
                            </info>Similarly, clothing stores use intrinsic static properties when
                            they present merchandise arranged according to color and size; extrinsic
                            static properties when they host branded displays of merchandise;
                            intrinsic dynamic properties when they set aside a display for seasonal
                            merchandise, from bathing suits to winter boots; and extrinsic dynamic
                            properties when a display area is set aside for <quote>Today’s
                                Special.</quote></para>
                    </footnote></para>
                <para audience="CORE" xml:id="para-zyd_m4r_lr"><phrase role="statement principle">If
                        we relax the requirement that different subsets of resources use different
                        organizing properties and allow any property to be used to describe any
                        resource, the loose organizing principle we now have is often called
                            <glossterm linkend="gloss_tagging" xreflabel="tagging"
                            >tagging</glossterm>.</phrase> Using any property of a resource to
                    create a description is an uncontrolled and often unprincipled principle for
                    creating categories, but it is increasingly popular for organizing photos, web
                    sites, email messages in <application>gmail</application>, or other web-based
                    resources. <phrase role="parenthetical">We discuss tagging in more detail in
                            <xref linkend="section-5.2.2.3"/>.</phrase></para>

                <sidebar xml:id="sidebar-StoreMap">
                    <title>A Supermarket Map</title>
                    <info>
                        <itermset>
                            <indexterm zone="sidebar-StoreMap" condition="print" userlevel="Professional Graduate">
                                <primary>sidebar</primary>
                                <secondary>Supermarket Map</secondary>
                            </indexterm>
                            <indexterm zone="sidebar-StoreMap" userlevel="Professional Graduate">
                                <primary>Supermarket Map</primary>
                            </indexterm>
                            <indexterm zone="sidebar-StoreMap">
                                <primary>resource description</primary>
                                <secondary>store map</secondary>
                            </indexterm></itermset>
        </info>
                    
                    <informalfigure xml:id="PICTURE-S6.3.3.2-StoreMap"
                        xreflabel="A Map of Our Store">
                        <mediaobject>
                            <imageobject>
                                <imagedata fileref="Pictures/6.3.3.2-StoreMap-OK.gif" format="GIF"/>
                            </imageobject>
                            <textobject>
                                <phrase role="ALT descriptive">An image of a store map. There are
                                    thirteen numbered aisles, five checkout stands, a deli area at
                                    the front-left of the store, and meats at the back of the store.
                                    Each aisle on the map is annotated with one or more product
                                    categories such as <quote>Dairy,</quote>
                                    <quote>Cleaning Supplies,</quote> and
                                    <quote>Condiments.</quote></phrase>
                            </textobject>
                            <caption><para audience="CORE" xml:id="para-l12_m4r_lr"><phrase role="caption">A typical supermarket embodies
                                        a surprisingly complex classification system. Each section
                                        of the store employs a different set of properties to
                                        arrange its resources, and some properties such as
                                        perishability and onsite preparation are important in more
                                        than one section. </phrase>
                                </para>
                                <para audience="CORE" xml:id="para-cd2_m4r_lr"><phrase role="credit"
                                        >(Photo by R. Glushko.)</phrase></para></caption>
                        </mediaobject>
                    </informalfigure>

                </sidebar>

            </section>
            <!--              -->
            <?need 5cm?>
            <section xml:id="section-7.3.3.3" label="7.3.3.3">
                <title>Necessary and Sufficient Properties</title>
                <info>
                    <itermset>
                        <indexterm significance="preferred" zone="def_classical_categories">
                            <primary>classical categories</primary>
                        </indexterm>
                        <indexterm zone="def_classical_categories">
                            <primary>categories</primary>
                            <secondary>classical</secondary>
                        </indexterm>
                        <indexterm zone="def_classical_categories">
                            <primary>necessary and sufficient properties</primary>
                        </indexterm>
                        <indexterm zone="def_classical_categories">
                            <primary>sufficient properties</primary>
                        </indexterm></itermset>
        </info>

                <para audience="CORE" xml:id="para-j22_m4r_lr"><info>
                        <itermset>
                            <indexterm zone="para-j22_m4r_lr">
                                <primary>Aristotle</primary>
                                <secondary>classical categories</secondary>
                            </indexterm>
                        </itermset>
                    </info><phrase role="statement">A large set of resources does not always require
                        many properties and categories to organize it.</phrase> Some types of
                    categories can be defined precisely with just a few
                        <emphasis>essential</emphasis> properties. For example, a prime number is a
                    positive integer that has no divisors other than 1 and itself, and this category
                    definition perfectly distinguishes prime and not-prime numbers no matter how
                    many numbers are being categorized. <quote>Positive integer</quote> and
                        <quote>divisible only by 1 and itself</quote> are
                        <emphasis>necessary</emphasis> or <emphasis>defining</emphasis> properties
                    for the prime number category; every prime number must satisfy these properties.
                    These properties are also <glossterm linkend="gloss_sufficiency_necessity"
                        xreflabel="sufficient">sufficient</glossterm> to establish membership in the
                    prime number category; any number that satisfies the necessary properties is a
                    prime number. <phrase role="definition" xml:id="def_classical_categories"
                        >Categories defined by necessary and sufficient properties are also called
                            <glossterm xml:id="term_monothetic_categories">monothetic</glossterm>.
                        They are also sometimes called <firstterm
                            xml:id="first_classical_categories">classical categories</firstterm>
                        because they conform to <personname>
                            <surname>Aristotle</surname></personname>’s theory of how categories are
                        used in logical deduction using syllogisms.</phrase><footnote
                        xml:id="endnote-354" label="417" audience="Philosophy">
                        <para audience="Philosophy" xml:id="para-rf2_m4r_lr"><info>
                                <itermset>
                                    <indexterm audience="Markup" zone="endnote-354">
                                        <primary>endnote</primary>
                                        <secondary>Philosophy</secondary>
                                    </indexterm>
                                    <indexterm zone="endnote-354">
                                        <primary>philosophy</primary>
                                        <secondary>classical categories</secondary>
                                    </indexterm>
                                    <indexterm zone="endnote-354">
                                        <primary>Aristotle</primary>
                                        <secondary>classical categories</secondary>
                                    </indexterm>
                                </itermset>
                            </info><personname>
                                <surname>Aristotle</surname></personname> did not call them
                            classical categories. That label was bestowed about <date>2300 years
                                later</date> by <citation xml:id="cite_Smith1981"
                                linkend="Smith1981">(Smith and Medin 1981)</citation>.</para>
                    </footnote>
                    <phrase role="parenthetical"> (See the sidebar, <xref
                            linkend="sidebar-ClassicalViewCategories"/>.)</phrase></para>
                <para audience="CORE" xml:id="para-bh2_m4r_lr"><phrase role="statement">Theories of
                        categorization have evolved a great deal since Plato and Aristotle proposed
                        them over two thousand years ago, but in many ways we still adhere to
                        classical views of categories when we create organizing systems because they
                        can be easier to implement and maintain that way.</phrase></para>
                <para audience="CORE" xml:id="para-m32_m4r_lr"><phrase role="statement">An important
                        implication of necessary and sufficient category definition is that every
                        member of the category is an equally good member or example of the category;
                        every prime number is equally prime.</phrase> Institutional category systems
                    often employ necessary and sufficient properties for their conceptual simplicity
                    and straightforward implementation in  <firstterm linkend="gloss_decision_tree"
                        xml:id="first_decision_tree">decision trees</firstterm>, database <glossterm
                        linkend="gloss_schema">schemas</glossterm>,and programming language
                        <glossterm linkend="gloss_classes">classes</glossterm>.</para>
                <sidebar xml:id="sidebar-ClassicalViewCategories">
                    <title>The Classical View of Categories</title>
                    <info>
                        <itermset>
                            <indexterm zone="sidebar-ClassicalViewCategories" condition="print" userlevel="Professional Graduate">
                                <primary>sidebar</primary>
                                <secondary>Classical View of Categories</secondary>
                            </indexterm>
                            <indexterm zone="sidebar-ClassicalViewCategories" userlevel="Professional Graduate">
                                <primary>Classical View of Categories</primary>
                            </indexterm>
                            <indexterm zone="sidebar-ClassicalViewCategories">
                                <primary>categories</primary>
                                <secondary>classical</secondary>
                            </indexterm>
                            <indexterm zone="sidebar-ClassicalViewCategories">
                                <primary>categories</primary>
                                <secondary>necessary and sufficient properties</secondary>
                            </indexterm>
                            <indexterm zone="sidebar-ClassicalViewCategories">
                                <primary>necessary and sufficient properties</primary>
                            </indexterm></itermset>
        </info>

                    <para audience="CORE" xml:id="para-uj2_m4r_lr"><phrase
                            role="principle statement">The classical view is that categories are
                            defined by necessary and sufficient properties.</phrase> This theory has
                        been enormously influential in Western thought, and is embodied in many
                        organizing systems, especially those for information resources. However, as
                        we will explain, we cannot rely on this principle to create categories in
                        many domains and contexts because there are not necessary and sufficient
                        properties. As a result, many psychologists, cognitive scientists, and
                        computer scientists who think about categorization have criticized the
                        classical theory.</para>
                    <para audience="CORE" xml:id="para-jm2_m4r_lr"><info><itermset>
                        <indexterm zone="para-jm2_m4r_lr">
                            <primary>Aristotle</primary>
                            <secondary>classical categories</secondary>
                        </indexterm><indexterm zone="para-jm2_m4r_lr" userlevel="Professional Graduate">
                            <primary>Socrates</primary>
                        </indexterm></itermset>
        </info>We think this is unfair to <personname>
                            <surname>Aristotle</surname></personname>, who proposed what we now call
                        the classical theory primarily to explain how categories underlie the logic
                        of deductive reasoning: All men are mortal; <personname>
                            <surname>Socrates</surname></personname> is a man; Therefore, <personname>
                            <surname>Socrates</surname></personname> is mortal. People are wrong to
                        turn Aristotle’s thinking around and apply it to the problem of inductive
                        reasoning, how categories are created in the first place. But this is not <personname>
                            <surname>Aristotle</surname></personname>’s fault; he was not trying to
                        explain how natural cultural categories arise.</para>
                </sidebar>
                <para audience="CORE" role="comparative" xml:id="para-tn2_m4r_lr">Consider the
                    definition of an address as requiring a street, city, governmental region, and
                    postal code. Anything that has all of these <glossterm
                        linkend="gloss_information_component">information components</glossterm> is
                    therefore considered to be a valid address, and anything that lacks any of them
                    will not be considered to be a valid address. If we refine the properties of an
                    address to require the governmental region to be a state, and specifically one
                    of the <orgname>United States Postal Service</orgname>’s list of official state
                    and territory codes, we create a subcategory for US addresses that uses an
                    enumerated category as part of its definition. Similarly, we could create a
                    subcategory for Canadian addresses by exchanging the name
                        <quote>province</quote> for state, and using an enumerated list of Canadian
                    province and territory codes.</para>
            </section>
        </section>
        <!--              -->
        <?need 5cm ?>
        <section xml:id="section-7.3.4" label="7.3.4">
            <title>The Limits of Property-Based Categorization</title>
            <info>
                <itermset>
                    <indexterm zone="section-7.3.4">
                        <primary>categorization</primary>
                        <secondary>limits</secondary>
                    </indexterm></itermset>
        </info>

            <para audience="CORE" xml:id="para-fp2_m4r_lr"><phrase role="definition"
                    xml:id="def_property-based_categorization"><glossterm
                        xml:id="term_property-based_categorization">Property-based
                        categorization</glossterm> works tautologically well for categories like
                        <quote>prime number</quote> where the category is defined by necessary and
                    sufficient properties. Property-based categorization also works well when
                    properties are conceptually distinct and the value of a property is easy to
                    perceive and examine, as they are with man-made physical resources like
                    shirts.</phrase></para>
            <para audience="CORE" xml:id="para-nq2_m4r_lr"><phrase role="statement">Historical
                    experience with organizing systems that need to categorize information resources
                    has shown that basing categories on easily perceived properties is often not
                    effective.</phrase> There might be indications <quote>on the surface</quote>
                that suggest the <quote>joints</quote> or boundaries between types of information
                resources, but these are often just presentation or packaging choices, That is to
                say, neither the size of a book nor the color of its cover are reliable cues for
                what it contains. Information resources have numerous descriptive properties like
                their title, author, and publisher that can be used more effectively to define
                categories, and these are certainly useful for some kinds of interactions, like
                finding all of the books written by a particular author or published by the same
                publisher. <phrase role="principle statement">However, for practical purposes, the
                    most useful property of an information resource is its <glossterm
                        linkend="gloss_aboutness" xreflabel="aboutness">aboutness</glossterm>, which
                    may not be objectively perceivable and which is certainly hard to
                    characterize.</phrase><footnote xml:id="endnote-355" label="418" audience="LIS">
                    <para audience="LIS" xml:id="para-yr2_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-355">
                                    <primary>endnote</primary>
                                    <secondary>LIS</secondary>
                                </indexterm>
                                <indexterm zone="endnote-355">
                                    <primary>aboutness</primary>
                                </indexterm>
                                <indexterm zone="endnote-355">
                                    <primary>library science</primary>
                                    <secondary>aboutness</secondary>
                                </indexterm>
                            </itermset>
                        </info>We all use the word <quote>about</quote> with ease in ordinary
                        discourse, but <quote>aboutness</quote> has generated a surprising amount of
                        theoretical commentary about its typically implicit definition, starting
                        with <citation xml:id="cite_Hutchins1977" linkend="Hutchins1977">(Hutchins
                            1977)</citation> and <citation xml:id="cite_Maron1977"
                            linkend="Maron1977">(Maron 1977)</citation> and relentlessly continued
                        by <citation xml:id="cite_Hjorland1992" linkend="Hjorland1992">(Hjørland
                            1992</citation>, <citation xml:id="cite_Hjorland2001"
                            linkend="Hjorland2001">2001)</citation>.</para>
                </footnote> Any collection of information resources in a library or document filing
                system is likely to be about many subjects and topics, and when an individual
                resource is categorized according to a limited number of its content properties, it
                is at the same time not being categorized using the others.</para>
            
            <sidebar xml:id="sidebar-6.3.4-Yahoo">
                
                <?dbhtml sidebar-width="50%"?>
                <?dbhtml float-type="right"?>
                
                <title>Classifying the Web: Yahoo! in 1996</title>
                <info>
                    <itermset>
                        <indexterm zone="sidebar-6.3.4-Yahoo" condition="print" userlevel="Professional Graduate">
                            <primary>sidebar</primary>
                            <secondary>Classifying the Web</secondary>
                        </indexterm>
                        <indexterm zone="sidebar-6.3.4-Yahoo" condition="print" userlevel="Professional Graduate">
                            <primary>Classifying the Web</primary>
                        </indexterm>
                        <indexterm zone="sidebar-6.3.4-Yahoo">
                            <primary>classification</primary>
                            <secondary>Yahoo!</secondary>
                        </indexterm></itermset>
        </info>
                
                <informalfigure xml:id="PICTURE-6.3.4-YahooClassification">
                    <mediaobject>
                        <imageobject>
                            <imagedata fileref="Pictures/6.3.4-YahooClassification.jpg"
                                format="JPEG"/>
                        </imageobject>
                        <textobject>
                            <phrase role="ALT descriptive">Screenshot of Yahoo! home page in
                                <date>1996</date>.</phrase>
                        </textobject>
                        <caption>
                            <para audience="CORE" xml:id="para-jt2_m4r_lr"><phrase role="caption"
                                    >Their goal was to manually assign every web page to a
                                    category.</phrase>
                            </para>
                            <para audience="CORE" xml:id="para-s52_m4r_lr"><phrase role="credit"
                                    >(Screenshot by R. Glushko. Source: <link
                                        xmlns:xlink="http://www.w3.org/1999/xlink"
                                        xlink:href="https://archive.org/web/">Internet Archive
                                        wayback machine</link>.)</phrase></para>
                        </caption>
                    </mediaobject>
                </informalfigure>
            </sidebar>
            <para audience="CORE" role="contrast" xml:id="para-lx2_m4r_lr">When the web first
                started, there were many attempts to create categories of web sites, most notably by
                    <orgname>Yahoo!</orgname> As the web grew, it became obvious that search engines
                would be vastly more useful because their near real-time text indexes obviate the
                need for <foreignphrase xml:lang="Latn">a priori</foreignphrase> assignment of web
                pages to categories. Rather, web search engines represent each web page or document
                in a way that treats each word or term they contain as a separate property.</para>
            <para audience="CORE" xml:id="para-ty2_m4r_lr" revision="4.0" revisionflag="changed"
                >Considering every distinct word in a document stretches our notion of property to
                make it very different from the kinds of properties we have discussed so far, where
                properties were being explicitly used by people to make decisions about category
                membership and resource organization. It is just not possible for people to pay
                attention to more than a few properties at the same time even if they want to,
                because that is how human perceptual and cognitive machinery works. But computers
                have no such limitations, and algorithms for information retrieval and machine
                learning can use huge numbers of properties, as we will see later in this chapter
                and in <xref linkend="chapter-8" xrefstyle="short"/> and <xref linkend="chapter-10"
                    xrefstyle="short"/>.</para>
            <?need 2.5cm ?>
            <?dbfo clear ?>
        </section>
        <!--              -->
        <?need 5cm ?>
        <section xml:id="section-7.3.5" label="7.3.5">
            <title>Probabilistic Categories and <quote>Family Resemblance</quote></title>
            <info>
                <itermset>
                    <indexterm zone="section-7.3.5">
                        <primary>categories</primary>
                        <secondary>family resemblance</secondary>
                    </indexterm>
                    <indexterm zone="section-7.3.5">
                        <primary>categories</primary>
                        <secondary>probabilistic</secondary>
                    </indexterm>
                    <indexterm zone="section-7.3.5">
                        <primary>natural selection</primary>
                    </indexterm>
                    <indexterm zone="section-7.3.5">
                        <primary>principle</primary>
                        <secondary>category creation</secondary>
                        <tertiary>family resemblance</tertiary>
                    </indexterm>
                    <indexterm zone="section-7.3.5">
                        <primary>family resemblance</primary>
                    </indexterm>
                </itermset>
            </info>
            <para xml:id="para-a2n_yzc_sv">As we have seen, some categories can be precisely defined
                using necessary and sufficient features, especially when the properties that
                determine category membership are easy to observe and evaluate. Something is either
                a prime number or it isn’t. A person cannot be a registered student and not
                registered at the same time.</para>

            <para audience="CORE" xml:id="para-d1f_m4r_lr"><phrase role="principle statement"
                    >However, categorization based on explicit and logical consideration of
                    properties is much less effective, and sometimes not even possible for domains
                    where properties lack one or more of the characteristics of separability,
                    perceptibility, and necessity.</phrase> Instead, we need to categorize using
                properties in a probabilistic or statistical way to come up with some measure of
                resemblance or similarity between the resource to be categorized and the other
                members of the category.</para>
            
            <para audience="CORE" role="comparative" xml:id="para-jbf_m4r_lr">Consider a familiar
                category like <quote>bird.</quote> All birds have feathers, wings, beaks, and two
                legs. But there are thousands of types of birds, and they are distinguished by
                properties that some birds have that other birds lack: most birds can fly, most are
                active in the daytime, some swim, some swim underwater; some have webbed feet. These
                properties are correlated  or clustered, a consequence of natural selection that
                conveys advantages to particular configurations of characteristics, and there are
                many different clusters; birds that live in trees have different wings and feet than
                those that swim, and birds that live in deserts have different colorations and
                metabolisms that those that live near water. So instead of being defined by a single
                set of properties that are both necessary and sufficient, the bird category is
                defined probabilistically, which means that decisions about category membership are
                made by accumulating evidence from the properties that are more or less
                characteristic of the category.</para>
            <para xml:id="para-ibm_k1d_sv">Categories of information resources often have the same
                probabilistic character. The category of spam messages is suggested by the presence
                of particular words (beneficiary, pharmaceutical) but these words also occur in
                messages that are not spam. A spam classifier uses the probabilities of each word in
                a message in spam and non-spam contexts to calculate an overall likelihood that the
                message is spam.</para>
            
            
            
            <para audience="CORE" xml:id="para-qcf_m4r_lr">There are three related consequences for
                categories when their characteristic properties have a probabilistic
                distribution:</para>
            <itemizedlist>
                <listitem>
            <para audience="CORE" xml:id="para-zdf_m4r_lr"><info>
                            <itermset>
                                <indexterm zone="para-zdf_m4r_lr">
                                    <primary>gradience</primary>
                                </indexterm>
                                <indexterm zone="para-zdf_m4r_lr">
                                    <primary>property</primary>
                                    <secondary>gradience</secondary>
                                </indexterm>
                            </itermset>
                        </info>The first is an effect of <firstterm linkend="gloss_typicality"
                            xml:id="first_typicality">typicality</firstterm> or <firstterm
                            linkend="gloss_centrality" xml:id="first_centrality"
                            >centrality</firstterm> that makes some members of the category better
                        examples than others. Membership in probabilistic categories is not all or
                        none, so even if they share many properties, an instance that has more of
                        the characteristic properties will be judged as better or more
                            typical.<footnote xml:id="endnote-356" label="419" audience="CogSci">
                            <para audience="CogSci" xml:id="para-iff_m4r_lr"><info>
                                    <itermset>
                                        <indexterm audience="Markup" zone="endnote-356">
                                            <primary>endnote</primary>
                                            <secondary>CogSci</secondary>
                                        </indexterm>
                                        <indexterm zone="endnote-356">
                                            <primary>Rosch, Eleanor</primary>
                                        </indexterm>
                                        <indexterm zone="endnote-356">
                                            <primary>cognitive science</primary>
                                            <secondary>typicality and centrality</secondary>
                                        </indexterm>
                                    </itermset>
                                </info>Typicality and centrality effects were studied by Rosch and
                                others in numerous highly influential experiments in the
                                    <date>1970s</date> and <date>1980s</date>
                                <citation xml:id="cite_Rosch1975" linkend="Rosch1975">(Rosch
                                    1975)</citation>. Good summaries can be found in <citation
                                    xml:id="cite_Mervis1981" linkend="Mervis1981">(Mervis and Rosch
                                    1981)</citation>, <citation xml:id="cite_Rosch1999"
                                    linkend="Rosch1999">(Rosch 1999)</citation>, and in Chapter 1 of
                                    <citation xml:id="cite_Rogers2008-6.2" linkend="Rogers2008"
                                    >(Rogers and McClelland 2008)</citation>.</para>
                        </footnote> Try to define <quote>bird</quote> and then ask yourself if all
                        of the things you classify as birds are equally good examples of the
                        category (look at the six birds in <xref linkend="sidebar-6.3.5-SixBirds"
                        />). This effect is also described as <glossterm linkend="gloss_gradience"
                            >gradience</glossterm> in category membership and reflects the extent to
                        which the most characteristic properties are shared.</para>
                </listitem>
                <listitem>
                    
            <para audience="CORE" xml:id="para-rgf_m4r_lr"><info>
                            <itermset>
                                <indexterm significance="preferred" zone="def_family_resemblance">
                                    <primary>family resemblance</primary>
                                </indexterm>
                                <indexterm zone="para-rgf_m4r_lr" userlevel="Professional Graduate">
                                    <primary>Wittgenstein, Ludwig</primary>
                                </indexterm>
                            </itermset>
                        </info><phrase xml:id="def_family_resemblance">A second consequence is that
                            the sharing of some but not all properties creates what we call
                                <glossterm xml:id="term_family_resemblance">family
                                resemblances</glossterm> among the category members; just as
                            biological family members do not necessarily all share a single set of
                            physical features but still are recognizable as members of the same
                            family.</phrase> This idea was first proposed by the
                            <date>20th-century</date> philosopher
                                <personname><firstname>Ludwig</firstname>
                            <surname>Wittgenstein</surname></personname>, who used
                            <quote>games</quote> as an example of a category whose members resemble
                        each other according to shifting property subsets.<footnote
                            xml:id="endnote-357" label="420" audience="Philosophy">
                            <para audience="Philosophy" xml:id="para-a3f_m4r_lr"><info>
                                    <itermset>
                                        <indexterm audience="Markup" zone="endnote-357">
                                            <primary>endnote</primary>
                                            <secondary>Philosophy</secondary>
                                        </indexterm>
                                        <indexterm zone="endnote-357">
                                            <primary>philosophy</primary>
                                            <secondary>Wittgenstein</secondary>
                                        </indexterm>
                                        <indexterm zone="endnote-357">
                                            <primary>Wittgenstein, Ludwig</primary>
                                        </indexterm>
                                    </itermset>
                                </info>An easy to find source for Wittgenstein’s discussion of
                                    <quote>game</quote> is <citation xml:id="cite_Wittgenstein2002"
                                    linkend="Wittgenstein2002">(Wittgenstein 2002)</citation> in a
                                collection of core readings for cognitive psychology <citation
                                    xml:id="cite_Levitin2002" linkend="Levitin2002">(Levitin
                                    2002)</citation>.</para>
                        </footnote></para>
                </listitem>
                <listitem>
            <para audience="CORE" xml:id="para-dmf_m4r_lr">The third consequence, when categories do
                        not have necessary features for membership, is that the boundaries of the
                        category are not fixed; the category can be stretched and new members
                        assigned as long as they resemble incumbent members. Personal video games
                        and multiplayer online games like <application>World of
                            Warcraft</application> did not exist in Wittgenstein’s time but we have
                        no trouble recognizing them as games and neither would Wittgenstein, were he
                        alive. Recall that in <xref linkend="chapter-1" xrefstyle="short"/> we
                        pointed out that the cultural category of <quote>library</quote> has been
                        repeatedly extended by new properties, as when
                            <application>Flickr</application> is described as a web-based
                        photo-sharing library. <phrase xml:id="def_polythetic">Categories defined by
                            family resemblance or multiple and shifting property sets are termed
                                <glossterm xml:id="term_polythetic"
                        >polythetic</glossterm>.</phrase></para>
                </listitem>
            </itemizedlist>
            <sidebar xml:id="sidebar-WhatIsGame">
                <title>What Is a Game?</title>
                <info>
                    <itermset>
                        <indexterm zone="sidebar-WhatIsGame" condition="print" userlevel="Professional Graduate">
                            <primary>sidebar</primary>
                            <secondary>What Is a Game?</secondary>
                        </indexterm>
                        <indexterm zone="sidebar-WhatIsGame" userlevel="Professional Graduate">
                            <primary>What Is a Game?</primary>
                        </indexterm>
                        <indexterm zone="sidebar-WhatIsGame">
                            <primary>categorization</primary>
                            <secondary>games</secondary>
                        </indexterm>
                    </itermset>
                </info>
                <?dbhtml sidebar-width="60%"?>
                <?dbhtml float-type="right"?>
                <para audience="CORE" role="interrogative" xml:id="para-ljf_m4r_lr"><info>
                        <itermset>
                            <indexterm zone="para-ljf_m4r_lr" userlevel="Professional Graduate">
                                <primary>Wittgenstein, Ludwig</primary>
                                <secondary>What Is a Game?</secondary>
                            </indexterm>
                        </itermset>
                    </info><personname><firstname>Ludwig</firstname>
                        <surname>Wittgenstein</surname></personname>
                    <date>(1889-1951)</date> was a philosopher who thought deeply about mathematics,
                    the mind, and language. In <date>1999</date>, his <citetitle
                        linkend="Wittgenstein2002" pubwork="book">Philosophical
                        Investigations</citetitle> was ranked as the most important book of
                        <date>20th-century</date> philosophy in a poll of philosophers.<footnote
                        xml:id="endnote-358" label="421" audience="Philosophy">
                        <para audience="Philosophy" xml:id="para-vkf_m4r_lr"><info>
                                <itermset>
                                    <indexterm audience="Markup" zone="endnote-358">
                                        <primary>endnote</primary>
                                        <secondary>Philosophy</secondary>
                                    </indexterm>
                                    <indexterm zone="endnote-358">
                                        <primary>philosophy</primary>
                                        <secondary>Wittgenstein</secondary>
                                    </indexterm>
                                </itermset>
                            </info>The philosopher’s poll that ranked Wittgenstein’s book #1 is
                            reported by <citation xml:id="cite_Lackey1999" linkend="Lackey1999"
                                >(Lackey 1999)</citation>.</para>
                    </footnote> In that book, Wittgenstein uses <quote>game</quote> to argue that
                    many concepts have no defining properties, and that instead there is a
                        <quote>complicated network of similarities overlapping and criss-crossing:
                        sometimes overall similarities, sometimes similarities of detail.</quote> He
                    contrasts board games, card games, ball games, games of skill, games of luck,
                    games with competition, solitary games, and games for amusement. Wittgenstein
                    notes that not all games are equally good examples of the category, and jokes
                    about teaching children a gambling game with dice because he knows that this is
                    not the kind of game that the parents were thinking of when they asked him to
                    teach their children a game.<footnote label="422" xml:id="endnote-358a"
                        revision="3.0" revisionflag="added" audience="Philosophy">
                        <para audience="Philosophy" revision="3.0" revisionflag="added"
                            xml:id="para-lj4_g45_xs"><info>
                                <itermset>
                                    <indexterm audience="Markup" zone="endnote-358a">
                                        <primary>endnote</primary>
                                        <secondary>Philosophy</secondary>
                                    </indexterm>
                                    <indexterm zone="endnote-358a">
                                        <primary>philosophy</primary>
                                        <secondary>game definition</secondary>
                                    </indexterm>
                                </itermset>
                            </info>It might be possible to define <quote>game,</quote>but it
                            requires a great deal of abstraction that obscures the <quote>necessary
                                and sufficient</quote> tests. <quote audience="Philosophy"
                                revision="3.0" revisionflag="added" xml:id="para-r3s_m2w_ps">To play
                                a game is to engage in activity directed toward bringing about a
                                speciﬁc state of affairs, using only means permitted by speciﬁc
                                rules, where the means permitted by the rules are more limited in
                                scope than they would be in the absence of the rules, and where the
                                sole reason for accepting such limitation is to make possible such
                                activity.</quote>
                            <citation linkend="Suits1967" xml:id="cite_Suits1967">(Suits
                                1967)</citation></para>
                    </footnote></para>
            </sidebar>
            
            <para audience="CORE" role="comparative" xml:id="para-mnf_m4r_lr">We conclude that
                instead of using properties one at a time to assign category membership, we can use
                them in a composite or integrated way where together a co-occurring cluster of
                properties provides evidence that contributes to a  <emphasis><glossterm
                        linkend="gloss_similarity" xreflabel="similarity"
                    >similarity</glossterm></emphasis> calculation. Something is categorized as an A
                and not a B if it is more similar to A’s best or most typical member rather than it
                is to B’s.<footnote xml:id="endnote-359" label="423" audience="CogSci">
                    <para audience="CogSci" xml:id="para-u4f_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-359">
                                    <primary>endnote</primary>
                                    <secondary>CogSci</secondary>
                                </indexterm>
                                <indexterm zone="endnote-359">
                                    <primary>categories</primary>
                                    <secondary>prototypes</secondary>
                                </indexterm>
                                <indexterm zone="endnote-359">
                                    <primary>categories</primary>
                                    <secondary>exemplars</secondary>
                                </indexterm>
                                <indexterm zone="endnote-359">
                                    <primary>cognitive science</primary>
                                    <secondary>prototype vs exemplar</secondary>
                                </indexterm>
                            </itermset>
                        </info>The exact nature of the category representation to which the
                        similarity comparison is made is a subject of ongoing debate in cognitive
                        science. Is it a <emphasis>prototype</emphasis>, a central tendency or
                        average of the properties shared by category members, or it one or more
                            <emphasis>exemplars,</emphasis> particular members that typify the
                        category. <phrase role="interrogative">Or is it neither, as argued by
                            connectionist modelers who view categories as patterns of network
                            activation without any explicitly stored category
                            representation?</phrase> Fortunately, these distinctions do not matter
                        for our discussion here. A recent review is <citation xml:id="cite_Rips2012"
                            linkend="Rips2012">(Rips, Smith, and Medin 2012)</citation>.</para>
                </footnote></para>
            
            <sidebar xml:id="sidebar-6.3.5-SixBirds">
                
                <title>Family Resemblance and Typicality</title>
                <info>
                    <itermset>
                        <indexterm zone="sidebar-6.3.5-SixBirds" condition="print" userlevel="Professional Graduate">
                            <primary>sidebar</primary>
                            <secondary>Family Resemblance and Typicality</secondary>
                        </indexterm>
                        <indexterm zone="sidebar-6.3.5-SixBirds" condition="print" userlevel="Professional Graduate">
                            <primary>Family Resemblance and Typicality</primary>
                        </indexterm>
                        <indexterm zone="sidebar-6.3.5-SixBirds">
                            <primary>family resemblance</primary>
                        </indexterm>
                        <indexterm zone="sidebar-6.3.5-SixBirds">
                            <primary>typicality</primary>
                        </indexterm></itermset>
        </info>
                
                <para xml:id="para-hqf_m4r_lr">These six animals have some physical features in
                    common but not all of them, yet they resemble each other enough to be easily
                    recognizable as birds. Most people consider a pigeon to be a more typical bird
                    than a penguin.</para>
                <informalfigure xml:id="PICTURE-6.3.5-SixBirds">
                    <mediaobject>
                        <imageobject>
                            <imagedata fileref="Pictures/6.3.5-SixBirds.jpg"
                                format="JPG"/>
                        </imageobject>
                        <textobject>
                            <phrase role="ALT descriptive">Six images of birds. 
                                A penguin, a pigeon, a swan, a frigate bird,
                                a flamingo, and a stork.</phrase>
                        </textobject>
                        <caption>
                            <para audience="CORE" xml:id="para-orf_m4r_lr"><phrase role="caption">A
                                    penguin, a pigeon, a swan, a stork, a flamingo, and a frigate
                                    bird. (Clockwise from top-left.)</phrase></para>
                            <para audience="CORE" xml:id="para-wsf_m4r_lr"><phrase role="credit"
                                    >(Photos by R. Glushko.)</phrase></para>
                        </caption>
                    </mediaobject>
                </informalfigure>
                
            </sidebar>
            <?need 2.5cm ?>
            <?dbfo clear ?>
        </section>
        <!--              -->
        <?need 5cm ?>
        <section xml:id="section-7.3.6" label="7.3.6">
            <title>Similarity</title>
            <info>
                <itermset>
                    <indexterm zone="section-7.3.6">
                        <primary>similarity</primary>
                    </indexterm>
                    <indexterm zone="section-7.3.2">
                        <primary>principle</primary>
                        <secondary>category creation</secondary>
                        <tertiary>similarity</tertiary>
                    </indexterm></itermset>
        </info>

            <para audience="CORE" xml:id="para-c5f_m4r_lr"><phrase role="definition"
                xml:id="def_similarity"><glossterm
                        xml:id="term_similarity">Similarity</glossterm> is a measure of the
                    resemblance between two things that share some characteristics but are not
                    identical. It is a very flexible notion whose meaning depends on the domain
                    within which we apply it.</phrase> Some people consider that the concept of
                similarity is itself meaningless because there must always be some basis, some
                unstated set of properties, for determining whether two things are similar. If we
                could identify those properties and how they are used, there would not be any work
                for a similarity mechanism to do.<footnote xml:id="endnote-360" label="424"
                    audience="CogSci">
                    <para audience="CogSci" xml:id="para-mvf_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-360">
                                    <primary>endnote</primary>
                                    <secondary>CogSci</secondary>
                                </indexterm>
                                <indexterm zone="endnote-360">
                                    <primary>similarity</primary>
                                </indexterm>
                                <indexterm zone="endnote-360">
                                    <primary>cognitive science</primary>
                                    <secondary>similarity</secondary>
                                </indexterm>
                            </itermset>
                        </info>Another situation where similarity has been described as a
                            <quote>mostly vacuous</quote> explanation for categorization is with
                        abstract categories or metaphors. Goldstone says <quote>an unrewarding job
                            and a relationship that cannot be ended may both be metaphorical
                            prisons... and may seem similar in that both conjure up a feeling of
                            being trapped... but this feature is almost as abstract as the category
                            to be explained.</quote>
                        <citation xml:id="cite_Goldstone1994" linkend="Goldstone1994">(Goldstone
                            1994)</citation>, p. 149.</para>
                </footnote></para>
            <para audience="CORE" xml:id="para-ywf_m4r_lr"><phrase role="statement">To make
                    similarity a useful mechanism for categorization we have to specify how the
                    similarity measure is determined. There are four psychologically-motivated
                    approaches that propose different functions for computing similarity: feature-
                    or property-based, geometry-based, transformational, and alignment- or
                    analogy-based.</phrase>
                <phrase audience="DS">The big contrast here is between models that represent items
                    as sets of properties or discrete conceptual features, and those that assume
                    that properties vary on a continuous metric space.</phrase><footnote
                    xml:id="endnote-361" label="425" audience="CogSci">
                    <para audience="CogSci" xml:id="para-iyf_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-361">
                                    <primary>endnote</primary>
                                    <secondary>CogSci</secondary>
                                </indexterm>
                                <indexterm zone="endnote-361" userlevel="Professional">
                                    <primary>cognitive science</primary>
                                    <secondary>citations</secondary>
                                </indexterm>
                            </itermset>
                        </info>
                        <citation xml:id="cite_Medin1993" linkend="Medin1993">(Medin, Goldstone, and
                            Gentner 1993)</citation> and <citation linkend="Tenenbaum2001"
                            xml:id="cite_Tenenbaum2001">(Tenenbaum and Griffiths
                        2001)</citation>.</para>
                </footnote></para>
            <section label="7.3.6.1" xml:id="section-7.3.6.1" revision="4.0" revisionflag="changed">
                <title>Feature-based Models of Similarity</title>
                <info>
                    <itermset>
                        <indexterm zone="section-7.3.6.1">
                            <primary>similarity</primary>
                            <secondary>feature-based models</secondary>
                        </indexterm>
                    </itermset>
                </info>
                <para audience="CORE" xml:id="para-rzf_m4r_lr"><phrase role="principle statement">An
                        influential model of feature-based similarity calculation is
                                <personname><firstname>Amos</firstname>
                            <surname>Tversky</surname></personname>’s contrast model, which matches
                        the features or properties of two things and computes a similarity measure
                        according to three sets of features:</phrase>
                    <itemizedlist>
                        <listitem>
                            <para xml:id="para-abg_m4r_lr">those features they share,</para>
                        </listitem>
                        <listitem>
                            <para xml:id="para-icg_m4r_lr">those features that the first has that
                                the second lacks, and</para>
                        </listitem>
                        <listitem>
                            <para xml:id="para-vdg_m4r_lr">those features that the second has that
                                the first lacks.</para>
                        </listitem>
                    </itemizedlist> The similarity based on the shared features is reduced by the
                    two sets of distinctive ones. The weights assigned to each set can be adjusted
                    to explain judgments of category membership. Another commonly feature-based
                    similarity measure is the Jaccard coefficient, the ratio of the common features
                    to the total number of them. This simple calculation equals zero if there are no
                    overlapping features and one if all features overlap. Jaccard's measure is often
                    used to calculate document similarity by treating each word as a
                        feature.<footnote audience="CogSci" label="426" xml:id="endnote-361b"
                        revision="4.0" revisionflag="added">
                        <para xml:id="para-ygk_mbd_sv" audience="CogSci"> Because Tversky's model separately considers
                            the sets of non-overlapping features, it is possible to accurately
                            capture similarity judgments when they are not symmetric, i.e., when A
                            is judged more similar to B than B is to A. This framing effect is
                            well-established in the psychological literature and many machine
                            learning algorithms now employ asymmetric measures. <citation
                                linkend="Tversky1974" xml:id="cite_Tversky1974">(Tversky
                                1974)</citation>
                        </para>
                    </footnote></para>
                <para audience="CORE" xml:id="para-gfg_m4r_lr"><info>
                        <itermset>
                            <indexterm zone="para-gfg_m4r_lr">
                                <primary>abstraction</primary>
                                <secondary>level</secondary>
                            </indexterm>
                        </itermset>
                    </info><phrase role="principle statement">We often use a heuristic version of
                        feature-based similarity calculation when we create multi-level or
                        hierarchical category systems to ensure that the categories at each level
                        are at the same level of abstraction or breadth.</phrase> For example, if we
                    were organizing a collection of musical instruments, it would not seem correct
                    to have subcategories of <quote>woodwind instruments,</quote>
                    <quote>violins,</quote> and <quote>cellos</quote> because the feature-based
                    similarity among the categories is not the same for all pairwise comparisons
                    among the categories; violins and cellos are simply too similar to each other to
                    be separate categories given woodwinds as a category.</para>
            </section>
            <?need 5cm?>
            <section label="7.3.6.2" revision="4.0" revisionflag="changed" xml:id="section-7.3.6.2">
                <title>Geometric Models of Similarity</title>
                <info>
                    <itermset>
                        <indexterm zone="section-7.3.6.2">
                            <primary>similarity</primary>
                            <secondary>geometric models</secondary>
                        </indexterm>
                    </itermset>
                </info>
                <para xml:id="para-vrp_3cd_sv">Geometric models are a type of similarity framework
                    in which items whose property values are metric are represented as points in a
                    multi-dimensional feature- or property-space. The property values are the
                    coordinates, and similarity is calculated by measuring the distance between the
                    items.</para>
                <sidebar xml:id="sidebar-DocumentSimilarity" revision="4.0" revisionflag="added">
                    <?dbhtml sidebar-width="50%"?>
                    <?dbhtml float-type="right"?>
                    <?dbfo sidebar-width="60%"?>
                    <?dbfo float-type="outside"?>
                    <title>Document Similarity</title>
                    <info userlevel="Professional Graduate">
                        <itermset>
                            <indexterm zone="sidebar-DocumentSimilarity">
                                <primary>sidebar</primary>
                                <secondary>Document Similarity</secondary>
                            </indexterm>
                            <indexterm zone="sidebar-DocumentSimilarity">
                                <primary>Document Similarity</primary>
                            </indexterm>
                        </itermset>
                    </info>
                    <informalfigure>
                        <mediaobject>
                            <imageobject>
                                <imagedata fileref="figs/DocumentSimilarity.jpg" format="JPG"/>
                            </imageobject>
                            <textobject>
                                <phrase role="ALT descriptive">Vector diagram defies
                                    description.</phrase>
                            </textobject>
                            <caption>
                                <para audience="CORE" xml:id="para-wmk_bvq_nw">Documents represented
                                    as vectors in term space, with the angles between them as a
                                    measure of their similarity.</para>
                            </caption>
                        </mediaobject>
                    </informalfigure>
                </sidebar>
                <para xml:id="para-srw_4tq_nw" revision="4.0" revisionflag="added">Geometric
                    similarity functions are commonly used by search engines; if a query and
                    document are each represented as a vector of search terms, relevance is
                    determined by the distance between the vectors in the <quote>term space.</quote>
                    The simplified diagram in the sidebar, <xref
                        linkend="sidebar-DocumentSimilarity"/>, depicts four documents whose
                    locations in the term space are determined by how many of each of three terms
                    they contain. The document vectors are normalized to length 1, which makes it
                    possible to use the cosine of the angle between any two documents as a measure
                    of their similarity. Documents d1 and d2 are more similar to each other than
                    documents d3 and d4, because angle between the former pair (Θ) is smaller than
                    the angle between the latter (Φ). We will discuss how this works in greater
                    detail in <xref linkend="chapter-10"/>.</para>
                <para xml:id="para-r1q_xtq_nw" revision="4.0" revisionflag="added">If the vectors
                    that represent items in a multi-dimensional property space are of different
                    lengths, instead of calculating similarity using cosines we need to calculate
                    similarity in a way that more explicitly considers the differences on each
                    dimension.</para>
                <?need 7cm?>
                
                <sidebar xml:id="sidebar-GeometricDistance" revision="4.0" revisionflag="added">
                    <?dbhtml sidebar-width="50%"?>
                    <?dbhtml float-type="left"?>
                    <?dbfo sidebar-width="60%"?>
                    <?dbfo float-type="inside"?>
                    <title>Geometric Distance Functions</title>
                    <info userlevel="Professional Graduate">
                        <itermset>
                            <indexterm zone="sidebar-GeometricDistance">
                                <primary>sidebar</primary>
                                <secondary>Geometric Distance Functions</secondary>
                            </indexterm>
                            <indexterm zone="sidebar-GeometricDistance">
                                <primary>Geometric Distance Functions</primary>
                            </indexterm>
                        </itermset>
                    </info>
                    <informalfigure>
                        <mediaobject>
                            <imageobject>
                                <imagedata fileref="figs/GeometricDistance.jpg" format="JPG"/>
                            </imageobject>
                            <textobject>
                                <phrase role="ALT descriptive">Diagram defies description.</phrase>
                            </textobject>
                            <caption>
                                <para audience="CORE" xml:id="para-p15_pfs_3w">The distance between
                                    points 1 and 2 depends on how the distance function combines the
                                    differences in values (A and B) on each dimension.</para>
                            </caption>
                        </mediaobject>
                    </informalfigure>
                </sidebar>
                <para xml:id="para-slp_3jr_nw" revision="4.0" revisionflag="added">The diagram in
                    the sidebar, <xref linkend="sidebar-GeometricDistance"/> shows two different
                    ways of calculating the distance between points 1 and 2 using the differences A
                    and B. The Euclidean distance function takes the square root of the sum of the
                    squared differences on each dimension; in two dimensions, this is the familiar
                    Pythagorean Theorem to calculate the length of the hypotenuse of a right
                    triangle, where the exponent applied to the differences is 2. In contrast, the
                    City Block distance function, so-named because it is the natural way to measure
                    distances in cities with <quote>gridlike</quote> street plans,  simply adds up
                    the differences on each dimension, which is equivalent to an exponent of
                    1.</para>
                <para xml:id="para-k2v_1vt_3w" revision="4.0" revisionflag="added">We can interpret
                    the exponent as a weighting function that determines the relative contribution
                    of each property to the overall distance or similarity calculation. The choice
                    of exponent depends on the type of properties that characterize a domain and how
                    people make category judgments within it. The exponent of 1 in the City Block
                    function ensures that each property contributes its full amount. As the exponent
                    grows larger, it magnifies the impact of the properties on which differences are
                    the largest. </para>
                <para revision="4.0" revisionflag="added" xml:id="para-gn1_bxx_pw" audience="DS">The
                    Chebyshev function takes this to the limit (where the exponent would be
                    infinity) and defines the distance between two items as the difference of their
                    values on the single property with the greatest difference. What this means in
                    practice is that two items could have similar or even identical values on most
                    properties, but if they differ much on just one property, they will be treated
                    as very dissimilar. We can make an analogy to stereotyping or prejudice when a
                    person is just like you in all ways except for the one property you view as
                    negative, which then becomes the only one that matters to you.</para>
                <para audience="DS" xml:id="para-f5f_v5q_nw" revision="4.0" revisionflag="added">At
                    the other extreme, if the exponent is reduced to zero, this treats each property
                    as binary, either present or absent, and the distance function becomes a count
                    of the number of times that the value of the property for one item is different
                    from the value for the other one. This is called the <quote>Hamming
                        distance.</quote></para>
            </section>
            <section label="7.3.6.3" revision="4.0" revisionflag="changed" xml:id="section-7.3.6.3">
                <title> Transformational Models of Similarity</title>
                <info>
                    <itermset>
                        <indexterm zone="section-7.3.6.3">
                            <primary>similarity</primary>
                            <secondary>transformational models</secondary>
                        </indexterm>
                    </itermset>
                </info>
                <para xml:id="para-mjg_m4r_lr" revision="4.0" revisionflag="changed"
                    >Transformational models assume that the similarity between two things is
                    inversely proportional to the complexity of the transformation required to turn
                    one into the other. <phrase role="statement">The simplest transformational model
                        of similarity counts the number of properties that would need to change
                        their values.</phrase> More generally, one way to perform the <glossterm
                        linkend="gloss_name_matching">name matching</glossterm> task of determining
                    when two different strings denote the same person, object, or other named entity
                    is to calculate the <quote>edit distance</quote> between them; the number of
                    changes required to transform one into the other. </para>
                <para audience="DS" revision="4.0" revisionflag="changed" xml:id="para-zdb_c1t_3w"
                    >The simplest calculation just counts the number of insertion, deletion, and
                    substitution operations and is called the Levenshtein distance; for example, the
                    distance between <quote>bob</quote> and <quote>book</quote> is two: insert
                        <quote>o</quote> and change the second <quote>b</quote> to <quote>k</quote>.
                    Two strings with a short edit distance might be variant spellings or
                    misspellings of the same name, and transformational models that are sensitive to
                    common typing errors like transposed or duplicated letters are very effective at
                    spelling correction. Transformational models of similarity are also commonly
                    used to detect plagiarism and duplicate web pages.<footnote xml:id="endnote-362"
                        label="427" audience="Computing">
                        <para audience="Computing" xml:id="para-wkg_m4r_lr"><info>
                                <itermset>
                                    <indexterm audience="Markup" zone="endnote-362">
                                        <primary>endnote</primary>
                                        <secondary>Computing</secondary>
                                    </indexterm>
                                    <indexterm zone="endnote-362">
                                        <primary>computing</primary>
                                        <secondary>transformation</secondary>
                                    </indexterm>
                                </itermset>
                            </info>For a detailed explanation of distance and transformational
                            models of similarity, see <citation xml:id="cite_Flach2012"
                                linkend="Flach2012">(Flach 2012)</citation>, Chapter 9. There are
                            many online calculators for Levenshein distance;
                            http://www.let.rug.nl/kleiweg/lev/ also has a compelling visualization.
                            The <quote>strings</quote> to be matched can themselves be
                            transformations. The <quote>soundex</quote> function is very commonly
                            used to determine if two words could be different spellings of the same
                            name. It <quote>hashes</quote> the names into phonetic encodings that
                            have fewer characters than the text versions. See <citation
                                xml:id="cite_Christen2006" linkend="Christen2006">(Christen
                                2006)</citation> and <link
                                xmlns:xlink="http://www.w3.org/1999/xlink"
                                xlink:href="http://www.searchforancestors.com/utility/soundex.html"
                                    ><uri>http://www.searchforancestors.com/utility/soundex.html</uri></link>
                            to try it yourself.</para>
                    </footnote></para>
            </section>
            <section label="7.3.6.4" revision="4.0" revisionflag="changed" xml:id="section-7.3.6.4">
                <title>Alignment or Analogy Models of Similarity</title>
                <info>
                    <itermset>
                        <indexterm zone="section-7.3.6.4">
                            <primary>similarity</primary>
                            <secondary>alignemt models</secondary>
                        </indexterm>
                        <indexterm zone="section-7.3.6.4">
                            <primary>similarity</primary>
                            <secondary>analogy models</secondary>
                        </indexterm>
                    </itermset>
                </info>
                <para audience="CORE" xml:id="para-yhg_m4r_lr">None of the previous types of
                    similarity models works very well when comparing things that have lots of
                    internal or relational structure. In these cases, calculations based on matching
                    features is insufficient; you need to compare features that align because they
                    have the same role in structures or relationships. For example, a car with a
                    green wheel and a truck with a green hood both share the feature green, but this
                    matching feature does not increase their similarity much because the car's wheel
                    does not align with the truck's hood. On the other hand, analogy lets us say
                    that an atom is like the solar system. They have no common properties, but they
                    share the relationship of having smaller objects revolving around a large
                    one.</para>
                <para xml:id="para-bcb_wxq_nw">This kind of analogical comparison is especially
                    important in problem solving. You might think that experts are good at solving
                    problems in their domain of expertise because they have organized their
                    knowledge and experience in ways that enable efficient search for and evaluation
                    of possible solutions. For example, it is well known that chess masters search
                    their memories of previous winning positions and the associated moves to decide
                    what to play. However, top chess players also organize their knowledge and
                    select moves on the basis of abstract similarities that cannot be explained in
                    terms of specific positions of chess pieces. This idea that experts represent
                    and solve problems at deeper levels than novices do by using more abstract
                    principles or domain structure has been replicated in many areas. Novices tend
                    to focus more on surface properties and rely more on literal
                        similarity.<footnote audience="CogSci" revision="4.0" revisionflag="added"
                        label="428" xml:id="endnote-0000428">
                        <para xml:id="para-aq1_zxq_nw" audience="CogSci">This explanation for
                            expert-novice differences in categorization and problem solving was
                            proposed in <citation linkend="Chi1981" xml:id="cite_Chi1981">(Chi et al
                                1981)</citation>. See <citation xml:id="cite_Linhares2007"
                                linkend="Linhares2007">(Linhares 2007)</citation> for studies of
                            abstract reasoning by chess experts. </para>
                    </footnote></para>
            </section>
        </section>
        <!--              -->
        <?dbfo need 6cm?>
        <section xml:id="section-7.3.7" label="7.3.7">
            <title>Goal-Derived Categories</title>
            <info>
                <itermset>
                    <indexterm zone="section-7.3.7">
                        <primary>goal-derived categories</primary>
                    </indexterm>
                    <indexterm zone="section-7.3.7">
                        <primary>principle</primary>
                        <secondary>category creation</secondary>
                        <tertiary>goal-derived</tertiary>
                    </indexterm>
                    <indexterm zone="section-7.3.7" userlevel="Professional Graduate">
                        <primary>Barsalou, Lawrence</primary>
                    </indexterm>
                </itermset>
            </info>
            <para audience="CORE" xml:id="para-hqg_m4r_lr"><phrase role="principle statement"
                    >Another psychological principle for creating categories is to organize
                    resources that go together in order to satisfy a goal.</phrase> Consider the
                category <quote>Things to take from a burning house,</quote> an example that
                cognitive scientist <personname><firstname>Lawrence</firstname>
                    <surname>Barsalou</surname></personname> termed an <firstterm
                    xml:id="first_ad_hoc_category">ad hoc</firstterm> or
                    <emphasis>goal-derived</emphasis> category.<footnote xml:id="endnote-364"
                    label="429" audience="CogSci">
                    <para audience="CogSci" xml:id="para-qrg_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-364">
                                    <primary>endnote</primary>
                                    <secondary>CogSci</secondary>
                                </indexterm>
                                <indexterm zone="endnote-364">
                                    <primary>cognitive science</primary>
                                    <secondary>Barsalou</secondary>
                                </indexterm>
                            </itermset>
                        </info><citation xml:id="cite_Barsalou1983-6.1" linkend="Barsalou1983"
                            >(Barsalou 1983)</citation>.</para>
                </footnote></para>
            <sidebar xml:id="sidebar-6.3.8-TakeToTheGym">
                <?dbhtml sidebar-width="50%"?>
                <?dbhtml float-type="right"?>
                <?dbfo sidebar-width="50%"?>
                <?dbfo float-type="outside"?>
                <title>Things Used at the Gym</title>
                <info>
                    <itermset>
                        <indexterm zone="sidebar-6.3.8-TakeToTheGym" condition="print"
                            userlevel="Professional Graduate">
                            <primary>sidebar</primary>
                            <secondary>Things Used at the Gym</secondary>
                        </indexterm>
                        <indexterm zone="sidebar-6.3.8-TakeToTheGym"
                            userlevel="Professional Graduate">
                            <primary>Things Used at the Gym</primary>
                        </indexterm>
                        <indexterm zone="sidebar-6.3.8-TakeToTheGym">
                            <primary>categorization</primary>
                            <secondary>goal-derived</secondary>
                        </indexterm>
                    </itermset>
                </info>
                <informalfigure xml:id="PICTURE-S6.3.8-TakeToTheGym" xreflabel="Illinois17">
                    <mediaobject>
                        <imageobject>
                            <imagedata fileref="Pictures/6.3.8-TakeToTheGym.jpg" format="JPG"/>
                        </imageobject>
                        <textobject>
                            <phrase role="ALT descriptive">A hand towel, a music player with
                                headphones, and a bottle of water.</phrase>
                        </textobject>
                        <caption>
                            <para audience="CORE" xml:id="para-zsg_m4r_lr"><phrase role="caption">A
                                    hand towel, a music player with headphones, and a bottle of
                                    water have no properties in common but they go together because
                                    they are members of the <quote>things used at the gym when
                                        working out</quote> category.<phrase
                                        userlevel="Professional Graduate"> This type of ad hoc or
                                        goal-derived category gave contestants trouble on the
                                            <citetitle>Pyramid</citetitle> game
                                    show.</phrase></phrase><phrase userlevel="Professional Graduate"
                                > </phrase></para>
                            <para audience="CORE" xml:id="para-g5g_m4r_lr"><phrase role="credit"
                                    >(Photo by R. Glushko.)</phrase></para>
                        </caption>
                    </mediaobject>
                </informalfigure>
            </sidebar>
            <para audience="CORE" xml:id="para-jqz_pvc_tw"><phrase role="interrogative">What things
                    would you take from your house if a fire threatened it??</phrase> Possibly your
                cat, your wallet and checkbook,  important papers like birth certificates and
                passports, and grandma’s old photo album, and anything else you think is important,
                priceless, or irreplaceable<symbol>&#8212;</symbol>as long as you can carry it.
                These items have no discernible properties in common, except for being your most
                precious possessions. The category is derived or induced by a particular goal in
                some specified context.</para>
        </section>
        <?need 6cm?>
        <section xml:id="section-7.3.8" label="7.3.8">
            <title>Theory-Based Categories</title>
            <info>
                <itermset>
                    <indexterm zone="section-7.3.8">
                        <primary>theory-based categories</primary>
                    </indexterm>
                    <indexterm zone="section-7.3.8">
                        <primary>principle</primary>
                        <secondary>category creation</secondary>
                        <tertiary>theory-based</tertiary>
                    </indexterm>
                </itermset>
            </info>
            <para audience="CORE" xml:id="para-fmg_m4r_lr"><phrase role="definition"
                    xml:id="def_theory-based_category">A final psychological principle for creating
                    categories is organizing things in ways that fit a theory or story that makes a
                    particular categorization sensible. A <glossterm
                        xml:id="term_theory-based_category">theory-based category</glossterm> can
                    win out even if probabilistic categorization, on the basis of <glossterm
                        linkend="gloss_family_resemblance">family resemblance</glossterm> or
                        <glossterm linkend="gloss_similarity">similarity</glossterm> with respect to
                    visible properties, would lead to a different category assignment.</phrase> For
                example, a theory of phase change explains why liquid water, ice, and steam are all
                the same chemical compound even though they share few visible properties. </para>
            <para audience="CORE" xml:id="para-ong_m4r_lr"><phrase role="principle statement"
                    >Theory-based categories based on origin or causation are especially important
                    with highly inventive and computational resources because unlike natural kinds
                    of physical resources, little or none of what they can do or how they behave is
                    visible on the surface (see <xref linkend="section-3.4.1"/>).</phrase> Consider
                all of the different appearances and form factors of the resources that we
                categorize as <quote>computers</quote>
                <symbol>&#8212;</symbol>their essence is that they all compute, an invisible or
                theory-like principle that does not depend on their visible properties.<footnote
                    xml:id="endnote-363" label="430" audience="CogSci">
                    <para audience="CogSci" xml:id="para-v4g_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-363">
                                    <primary>endnote</primary>
                                    <secondary>CogSci</secondary>
                                </indexterm>
                                <indexterm zone="endnote-363">
                                    <primary>cognitive science</primary>
                                    <secondary>theory-based categories</secondary>
                                </indexterm>
                            </itermset>
                        </info>The emergence of theory-based categorization is an important event in
                        cognitive development that has been characterized as a shift from
                            <quote>holistic</quote> to <quote>analytic</quote> categories or from
                            <quote>surface properties</quote> to <quote>principles.</quote> See
                            <citation xml:id="cite_Carey1991" linkend="Carey1991">(Carey and Gelman
                            1991)</citation>
                        <citation xml:id="cite_Rehder2004" linkend="Rehder2004">(Rehder and Hastie
                            2004)</citation>.</para>
                </footnote></para>
        </section>
        <!--              -->
        
        <!-- ###################### SECTION ########################################### -->
        <?dbfo clear ?>
    </section>
    <!--              -->
    <?need 5cm?>
    <section xml:id="section-7.4" label="7.4">
        <title>Category Design Issues and Implications</title>
        <info>
            <itermset>
                <indexterm zone="section-7.4">
                    <primary>categories</primary>
                    <secondary>design choices</secondary>
                </indexterm></itermset>
        </info>

        <para audience="CORE" role="principle" xml:id="para-och_m4r_lr"><info>
                <itermset>
                    <indexterm zone="para-och_m4r_lr">
                        <primary>abstraction</primary>
                        <secondary>level</secondary>
                    </indexterm>
                </itermset>
            </info><phrase role="statement principle">We have previously discussed the most
                important principles for creating categories: resource properties, similarity, and
                goals. When we use one or more of these principles to develop a system of
                categories, we must make decisions about its depth and breadth.</phrase> Here, we
            examine the idea that some levels of abstraction in a system of categories are more
            basic or natural than others. We also consider how the choices we make affect how we
            create the organizing system in the first place, and how they shape our interactions
            when we need to find some resources that are categorized in it.</para>
        <note userlevel="Editor">
            <para xml:id="para-c2p_ssg_hv">Add a section here on maintaining categories over time;
                some of this is in 11.6.2, but needs to be mentioned here with more specific content
                about model maintenance</para>
            <para xml:id="para-eql_zsg_hv">7.4.4 could mention the debate about black box
                transparency; some use cases don't need to understand the categories</para>
        </note>
        <!--              -->
        <section xml:id="section-7.4.1" label="7.4.1">
            <title>Category Abstraction and Granularity</title>
            <info>
                <itermset>
                    <indexterm zone="section-7.4.1">
                        <primary>abstraction</primary>
                        <secondary>category</secondary>
                    </indexterm>
                    <indexterm zone="section-7.4.1">
                        <primary>granularity</primary>
                        <secondary>category</secondary>
                    </indexterm>
                    <indexterm zone="section-7.4.1">
                        <primary>concept</primary>
                        <secondary>size principle</secondary>
                    </indexterm>
                </itermset>
            </info>

            <para audience="CORE" xml:id="para-ydh_m4r_lr"><phrase role="statement">We can identify
                    any resource as a unique instance or as a member of a class of
                    resources.</phrase>
                <phrase role="statement">The size of this class<symbol>&#8212;</symbol>the number of
                    resources that are treated as equivalent<symbol>&#8212;</symbol>is determined by
                    the properties or characteristics we consider when we examine the resources in
                    some domain.</phrase>
                <phrase role="statement">The way we think of a resource domain depends on context
                    and intent, so the same resource can be thought of abstractly in some situations
                    and very concretely in others.</phrase> As we discussed in <xref
                    linkend="chapter-5"/>, this influences the nature and extent of resource
                description, and as we have seen in this chapter, it then influences the nature and
                extent of categories we can create.</para>
            <para audience="CORE" xml:id="para-gfh_m4r_lr">Consider the regular chore of putting
                away clean clothes. We can consider any item of clothing as a member of a broad
                category whose members are any kind of garment that a person might wear. Using one
                category for all clothing, that is, failing to distinguish among the various items
                in any useful or practical way would likely mean that we would keep our clothes in a
                big unorganized pile.</para>
            <para audience="CORE" xml:id="para-mgh_m4r_lr">However, we cannot wear any random
                combination of clothing items<symbol>&#8212;</symbol>we need a shirt, a pair of
                pants, socks, and so on. Clearly, our indiscriminate clothing category is too broad
                for most purposes. So instead, most people organize their clothes in more
                fine-grained categories that fit the normal pattern of how they wear clothes.</para>
            <para xml:id="para-emt_htk_gw" audience="CogSci"><phrase role="principle">This tendency
                    to use specific categories instead of broader ones is a general principle that
                    reflects how people organize their experience when they see similar, but not
                    identical, examples or events.</phrase> This <quote>size principle</quote> for
                concept learning, as <jobtitle>cognitive scientist</jobtitle>
                <personname><firstname>Josh</firstname>
                    <surname>Tenenbaum</surname></personname> describes it, is a preference for the
                most specific rules or descriptions that fit the observations. For example, if you
                visit a zoo and see many different species of animals, your conception of what you
                saw is different than if you visited a kennel that only contained dogs. You might
                say <quote>I saw animals at the zoo,</quote> but would be more likely to say
                    <quote>I saw dogs at the kennel</quote> because using the broad
                    <quote>animal</quote> category to describe your kennel visit conveys less of
                what you learned from your observations there.<footnote audience="CogSci"
                    label="431" xml:id="endnote-365b" revision="4.0" revisionflag="added">
                    <para xml:id="para-cpw_ktk_gw" audience="CogSci"><citation
                            linkend="Tenenbaum2000" xml:id="cite_Tenenbaum2000">(Tenenbaum
                            2000)</citation> argues that this preference for the most specific
                        hypothesis that fits the data is a general principle of Bayesian learning
                        with random samples. </para>
                </footnote></para>
            <para audience="CORE" xml:id="para-whh_m4r_lr">In <xref linkend="section-7.3.2"/> we
                described an organizing system for the shirts in our closet, so let us talk about
                socks instead. When it comes to socks, most people think that the basic unit is a
                pair because they always wear two socks at a time. If you are going to need to find
                socks in pairs, it seems sensible to organize them into pairs when you are putting
                them away. Some people might further separate their dress socks from athletic ones,
                and then sort these socks by color or material, creating a hierarchy of sock
                categories analogous to the shirt categories in our previous example.</para>
            
            <para audience="CORE" xml:id="para-gjh_m4r_lr">Questions of resource abstraction and
                granularity also emerge whenever the information systems of different firms, or
                different parts of a firm, need to exchange information or be merged into a single
                system. All parties must define the identity of each thing in the same way, or in
                ways that can be related or mapped to each other either manually or
                electronically.</para>
            <para audience="CORE" xml:id="para-pkh_m4r_lr"><phrase role="interrogative">For example,
                    how should a business system deal with a customer’s address?</phrase> Printed on
                an envelope, <quote>an address</quote> typically appears as a comprehensive,
                multi-line text object. Inside an information system, however, an address is best
                stored as a set of distinctly identifiable information components. This fine-grained
                organization makes it easier to sort customers by city or postal codes, for sales
                and marketing purposes. Incompatibilities in the abstraction and granularity of
                these information components, and the ways in which they are presented and reused in
                documents, will cause interoperability problems when businesses need to share
                    information.<footnote xml:id="endnote-365" label="432" audience="Computing">
                    <para audience="Computing" xml:id="para-wlh_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-365">
                                    <primary>endnote</primary>
                                    <secondary>Computing</secondary>
                                </indexterm>
                                <indexterm zone="endnote-365">
                                    <primary>modeling</primary>
                                    <secondary>address</secondary>
                                </indexterm>
                                <indexterm zone="endnote-365">
                                    <primary>computing</primary>
                                    <secondary>granularity</secondary>
                                </indexterm>
                            </itermset>
                        </info>Consider what happens if two businesses model the concept of
                            <quote>address</quote> in a customer database with different
                        granularity. One may have a coarse <quote>Address</quote> field in the
                        database, which stores a street address, city, state, and Zip code all in
                        one block, while the other stores the components
                            <quote>StreetAddress,</quote>
                        <quote>City,</quote> and <quote>PostalCode</quote> In separate fields. The
                        more granular model can be automatically transformed into the less granular
                        one, but not vice versa <citation xml:id="cite_Glushko2005-6.1"
                            linkend="Glushko2005">(Glushko and McGrath 2005)</citation>.</para>
                </footnote></para>
            <para audience="Computing" role="contrast" xml:id="para-zfj_m4r_lr"><info>
                    <itermset>
                        <indexterm zone="para-zfj_m4r_lr">
                            <primary>abstraction</primary>
                            <secondary>level</secondary>
                        </indexterm>
                        <indexterm zone="para-zfj_m4r_lr">
                            <primary>UBL</primary>
                        </indexterm>
                    </itermset>
                </info>The <firstterm xml:id="first_UBL"><citerefentry><refentrytitle>Universal
                            Business
                        Language</refentrytitle><manvolnum>UBL</manvolnum></citerefentry></firstterm>
                <phrase role="parenthetical">(mentioned briefly in <xref linkend="section-8.1.5.2"
                        xrefstyle="short"/>)</phrase> is a library of information components
                designed to enable the creation of business document models that span a range of
                category abstraction. UBL comes equipped with XML schemas that define document
                categories like orders, invoices, payments, and receipts that many people are
                familiar with from their personal experiences of shopping and paying bills. However,
                UBL can also be used to design very specific or subordinate level transactional
                document types like <quote>purchase order for industrial chemicals when buyer and
                    seller are in different countries,</quote> or document types at the other end of
                the abstraction hierarchy like <quote>fill-in-the-blank</quote> legal forms for any
                kind of contract.</para>
            <para audience="LIS" revision="3.0" revisionflag="added" xml:id="para-xmg_mzx_rs"><info>
                    <itermset>
                        <indexterm zone="para-xmg_mzx_rs">
                            <primary>precision and validity</primary>
                        </indexterm>
                    </itermset>
                </info>Bowker and Star point out that there is often a pragmatic tradeoff between
                precision and validity when defining categories and assigning resources to them,
                particularly in scientific and other highly technical domains. More granular
                categories make more precise classification possible in principle, but highly
                specialized domains might contain instances that are so complex or hard to
                understand that it is difficult to decide where to organize them.<footnote
                    audience="LIS" revision="3.0" revisionflag="added" xml:id="endnote-365a"
                    label="433">
                    <para audience="LIS" xml:id="para-s2q_f2w_ps" revision="3.0"
                        revisionflag="added"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-365a">
                                    <primary>endnote</primary>
                                    <secondary>LIS</secondary>
                                </indexterm>
                                <indexterm zone="endnote-365a" userlevel="Professional">
                                    <primary>library science</primary>
                                    <secondary>citations</secondary>
                                </indexterm>
                            </itermset>
                        </info><citation xml:id="cite_Bowker2000-6.4.1" linkend="Bower2001">(Bowker and Star
                        2000)</citation></para>
                </footnote></para>
            <para audience="LIS" revision="3.0" revisionflag="added">As an example of this real-world messiness
                that resists precise classification, Bowker and Star turn to medicine and the World
                Health Organization's International Classification of Diseases (ICD), a system of
                categories for cause-of-death reporting. The ICD requires that every death be
                assigned to one and only one category out of thousands of possible choices, which
                facilitates important uses such as statistical reporting for public health research. </para>
            <para audience="LIS" revision="3.0" revisionflag="added">In practice, however, doctors often lack
                conclusive evidence about the cause of a particular death, or they identify a number
                of contributing factors, none of which could properly be described as the sole
                cause. In these situations, less precise categories would better accommodate the
                ambiguity, and the aggregate data about causes of death would have greater validity.
                But doctors have to use the ICD's precise categories when they sign a death
                certificate, which means they sometimes record the wrong cause of death just to get
                their work done.</para>
            <para xml:id="para-n3v_xhs_3w" revision="4.0" revisionflag="added">It might seem
                counterintuitive, but when a system of human-generated categories is too complex for
                people to interpret and apply reliably, computational classifiers that compute
                statistical similarity between new and already classified items can outperform
                    people.<footnote audience="DS" revision="4.0" revisionflag="added" label="434"
                    xml:id="endnote-0000434">
                    <para xml:id="para-thq_d3s_3w" audience="DS">Statistician and baseball fan
                                <personname><firstname>Nate</firstname>
                            <surname>Silver</surname></personname> rejected a complex system that
                        used twenty-six player categories for predicting baseball performance
                        because <quote>it required as much art as science to figure out what group a
                            player belonged in.</quote><citation linkend="Silver2012"
                            xml:id="cite_Silver2012"> (Silver 2012, p, 83).</citation> His improved
                        system used the technique of <quote>nearest neighbor</quote> analysis to
                        identify current baseball players whose minor league statistics were most
                        similar to the current minor league players being evaluated. <phrase
                            role="parenthetical">(See <xref linkend="section-7.5.3.3"/>)</phrase>. </para>
                    <para xml:id="para-hdm_23s_3w" audience="DS">Silver later became famous for his
                        extremely accurate predictions of the 2008 US presidential elections. He is
                        the founder and editor of the FiveThirtyEight blog, so named because there
                        are 538 senators and representatives in the US Congress. </para>
                </footnote></para>
        </section>
        <!--              -->
        <section xml:id="section-7.4.2" label="7.4.2">
            <title>Basic or Natural Categories</title>
            <info>
                <itermset>
                    <indexterm zone="section-7.4.2">
                        <primary>categories</primary>
                        <secondary>basic</secondary>
                    </indexterm>
                    <indexterm zone="section-7.4.2">
                        <primary>categories</primary>
                        <secondary>natural</secondary>
                    </indexterm>
                    <indexterm zone="section-7.4.2">
                        <primary>basic category</primary>
                    </indexterm>
                    <indexterm zone="section-7.4.2">
                        <primary>natural category</primary>
                    </indexterm></itermset>
        </info>

            <para audience="CORE" xml:id="para-enh_m4r_lr"><phrase role="statement">Category
                    abstraction is normally described in terms of a hierarchy of superordinate,
                    basic, and subordinate category levels.</phrase>
                <quote>Clothing,</quote> for example, is a superordinate category,
                    <quote>shirts</quote> and <quote>socks</quote> are basic categories, and
                    <quote>white long-sleeve dress shirts</quote> and <quote>white wool hiking
                    socks</quote> are subordinate categories. Members of basic level categories like
                    <quote>shirts</quote> and <quote>socks</quote> have many perceptual properties
                in common, and are more strongly associated with motor movements than members of
                superordinate categories. Members of subordinate categories have many common
                properties, but these properties are also shared by members of other subordinate
                categories at the same level of abstraction in the category hierarchy. That is,
                while we can identify many properties shared by all <quote>white long-sleeve dress
                    shirts,</quote> many of them are also properties of <quote>blue long-sleeve
                    dress shirts</quote> and <quote>black long-sleeve pullover
                shirts.</quote></para>
            <para audience="CogSci" xml:id="para-q4h_m4r_lr">Psychological research suggests that
                some levels of abstraction in a system of categories are more basic or natural than
                others.  Anthropologists have also observed that folk taxonomies invariably classify
                natural phenomena into a five- or six-level hierarchy, with one of the levels being
                the psychologically basic or <quote>real</quote> name (such as <quote>cat</quote> or
                    <quote>dog</quote>), as opposed to more abstract names (e.g.
                    <quote>mammal</quote>) that are used less in everyday life.  An implication for
                organizing system design is that basic level categories are highly efficient in
                terms of the cognitive effort they take to create and use. A corollary is that
                classifications with many levels at different abstraction levels may be difficult
                for users to navigate effectively.<footnote xml:id="endnote-366" label="435"
                    audience="CogSci">
                    <para audience="CogSci" xml:id="para-zph_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-366">
                                    <primary>endnote</primary>
                                    <secondary>CogSci</secondary>
                                </indexterm>
                                <indexterm zone="endnote-366">
                                    <primary>principle</primary>
                                    <secondary>cognitive economy</secondary>
                                </indexterm>
                                <indexterm zone="endnote-366">
                                    <primary>Rosch, Eleanor</primary>
                                </indexterm>
                                <indexterm zone="endnote-366">
                                    <primary>cognitive science</primary>
                                    <secondary>cognitive economy</secondary>
                                </indexterm>
                            </itermset>
                        </info><citation xml:id="cite_Rosch1999-6.2" linkend="Rosch1999">(Rosch
                            1999)</citation> calls this the principle of cognitive economy, that
                            <quote>what one wishes to gain from one’s categories is a great deal of
                            information about the environment while conserving finite resources as
                            much as possible. [...] It is to the organism’s advantage not to
                            differentiate one stimulus from another when that differentiation is
                            irrelevant to the purposes at hand.</quote> (Pages 3-4.)</para>
                </footnote></para>
            
        </section>
        <!--              -->
        <?need 5cm ?>
        <section xml:id="section-7.4.3" label="7.4.3">
            <title>The Recall / Precision Tradeoff</title>
            <info>
                <itermset>
                    <indexterm zone="section-7.4.3">
                        <primary>precision</primary>
                        <secondary>tradeoffs</secondary>
                    </indexterm>
                    <indexterm zone="section-7.4.3">
                        <primary>recall</primary>
                        <secondary>tradeoffs</secondary>
                    </indexterm>
                    <indexterm zone="section-7.4.3">
                        <primary>tradeoffs</primary>
                        <secondary>recall/precision</secondary>
                    </indexterm></itermset>
        </info>

            <para audience="CORE" role="interrogative" xml:id="para-irh_m4r_lr"><info><itermset>
                <indexterm zone="para-irh_m4r_lr">
                    <primary>abstraction</primary>
                    <secondary>level</secondary>
                </indexterm></itermset>
        </info><phrase role="statement">The abstraction level we choose determines how
                    precisely we identify resources.</phrase> When we want to make a general claim,
                or communicate that the scope of our interest is broad, we use superordinate
                categories, as when we ask, <quote>How many animals are in the <orgname>San Diego
                        Zoo</orgname>?</quote> But we use precise subordinate categories when we
                need to be specific: <quote>How many adult emus are in the <orgname>San Diego
                        Zoo</orgname> today?</quote></para>
            <para audience="CORE" xml:id="para-psh_m4r_lr">If we return to our clothing example,
                finding a pair of white wool hiking socks is very easy if the organizing system for
                socks creates fine-grained categories. When resources are described or arranged with
                this level of detail, a similarly detailed specification of the resources you are
                looking for yields precisely what you want. When you get to the place where you keep
                white wool hiking socks, you find all of them and nothing else. On the other hand,
                if all your socks are tossed unsorted into a sock drawer, when you go sock hunting
                you might not be able to find the socks you want and you will encounter lots of
                socks you do not want. But you will not have put time into sorting them, which many
                people do not enjoy doing; you can spend time sorting or searching depending on your
                preferences.</para>
            <para audience="CORE" xml:id="para-b5h_m4r_lr">If we translate this example into the
                jargon of information retrieval, we say that more fine-grained organization reduces
                    <glossterm linkend="gloss_recall">recall</glossterm>, the number of resources
                you find or retrieve in response to a query, but increases the <glossterm
                    linkend="gloss_precision">precision</glossterm> of the recalled set, the
                proportion of recalled items that are relevant. <phrase role="statement">Broader or
                    coarse-grained categories increase recall, but lower precision.</phrase> We are
                all too familiar with this hard bargain when we use a web search engine; a quick
                one-word query results in many pages of mostly irrelevant sites, whereas a carefully
                crafted multi-word query pinpoints sites with the information we seek. We will
                discuss recall, precision, and evaluation of information retrieval more extensively
                in <xref linkend="chapter-10"/>.</para>
            <para audience="CORE" role="contrast" xml:id="para-kvh_m4r_lr"><info><itermset>
                <indexterm zone="para-kvh_m4r_lr">
                    <primary>costs</primary>
                    <secondary>accounting</secondary>
                </indexterm><indexterm zone="para-kvh_m4r_lr">
                    <primary>tradeoffs</primary>
                    <secondary>organization versus retrieval</secondary>
                </indexterm></itermset>
        </info>This mundane example illustrates the fundamental tradeoff between
                organization and retrieval. <phrase role="statement">A tradeoff between the
                    investment in organization and the investment in retrieval persists in nearly
                    every organizing system.</phrase> The more effort we put into organizing
                resources, the more effectively they can be retrieved. The more effort we are
                willing to put into retrieving resources, the less they need to be organized first.
                The allocation of costs and benefits between the organizer and retriever differs
                according to the relationship between them. <phrase role="interrogative">Are they
                    the same person? Who does the work and who gets the benefit?</phrase></para>
        </section>
        <?dbfo clear ?>
        <?need 5cm ?>
        <!--              -->
        <section xml:id="section-7.4.4" label="7.4.4">
            <title>Category Audience and Purpose</title>
            <info>
                <itermset>
                    <indexterm zone="section-7.4.4">
                        <primary>audience</primary>
                    </indexterm>
                    <indexterm zone="section-7.4.4">
                        <primary>purpose</primary>
                        <secondary>category</secondary>
                    </indexterm></itermset>
        </info>

            <para audience="CORE" xml:id="para-rwh_m4r_lr"><phrase role="statement">The ways in
                    which people categorize depend on the goals of categorization, the breadth of
                    the resources in the collection to be categorized, and the users of the
                    organizing system.</phrase> Suppose that we want to categorize languages. Our
                first step might be determining what constitutes a language, since there is no
                widespread agreement on what differentiates a language from a dialect, or even on
                whether such a distinction exists.</para>

            <para audience="CORE" xml:id="para-ayh_m4r_lr">What we mean by <quote>English</quote>
                and <quote>Chinese</quote> as categories can change depending on the audience we are
                addressing and what our purpose is, however.<footnote xml:id="endnote-367"
                    label="436" audience="Linguistics">
                    <para audience="Linguistics" xml:id="para-hzh_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-367">
                                    <primary>endnote</primary>
                                    <secondary>Linguistics</secondary>
                                </indexterm>
                                <indexterm zone="endnote-367">
                                    <primary>linguistics</primary>
                                    <secondary>language variants</secondary>
                                </indexterm>
                                <indexterm zone="endnote-367">
                                    <primary>language</primary>
                                    <secondary>variants</secondary>
                                </indexterm>
                                <indexterm zone="endnote-367">
                                    <primary>English language</primary>
                                    <secondary>variants</secondary>
                                </indexterm>
                                <indexterm zone="endnote-367">
                                    <primary>cognitive science</primary>
                                    <secondary>dialects</secondary>
                                </indexterm>
                            </itermset>
                        </info>For example, some linguists think of <quote>English</quote> as a
                        broad category encompassing multiple languages or dialects, such as
                            <quote>Standard British English,</quote>
                        <quote>Standard American English,</quote> and <quote>Appalachian
                            English.</quote></para>
                    <para audience="Linguistics" xml:id="para-xf3_m4r_lr">If we are concerned with
                        linguistic diversity and the survival of minority languages, we might
                        categorize some languages as endangered in order to mobilize language
                        preservation efforts. We could also categorize languages in terms of shared
                        linguistic ancestors (<quote>Romance languages,</quote> for example), in
                        terms of what kinds of sounds they make use of, by how well we speak them,
                        by regions they are commonly spoken in, whether they are signed or unsigned,
                        and so on. We could also expand our definition of the languages category to
                        include artificial computer languages, or body language, or languages shared
                        by people and their pets<symbol>&#8212;</symbol>or thinking more
                        metaphorically, we might include the language of fashion.</para>
                </footnote> A language learning school’s representation of <quote>English</quote>
                might depend on practical concerns such as how the school’s students are likely to
                use the language they learn, or which teachers are available. For the purposes of a
                school teaching global languages, and one of the standard varieties of English
                (i.e., those associated with political power), or an amalgamation of several
                standard varieties, might be thought of as a single instance
                (<quote>English</quote>) of the category <quote>Languages.</quote></para>
            
            
            
            <para audience="CORE" xml:id="para-k23_m4r_lr">Similarly, the category structure in
                which <quote>Chinese</quote> is situated can vary with context. While some schools
                might not conceptualize <quote>Chinese</quote> as a category encompassing multiple
                linguistic varieties, but rather as a single instance within the
                    <quote>Languages</quote> category, another school might teach its students
                Mandarin, Wu, and Cantonese as dialects within the language category
                    <quote>Chinese,</quote> that are unified by a single standard <glossterm
                    linkend="gloss_writing_system">writing system</glossterm>. In addition, a
                linguist might consider Mandarin, Wu, and Cantonese to be mutually unintelligible,
                making them separate languages within the broader category <quote>Chinese</quote>
                for the purpose of creating a principled language classification system.</para>
            <para audience="CORE" xml:id="para-fh3_m4r_lr">If people could only categorize in a
                single way, the <citetitle>Pyramid</citetitle> game show, where contestants guess
                what category is illustrated by the example provided by a clue giver, would pose no
                challenge. The creative possibilities provided by categorization allow people to
                order the world and refer to interrelationships among conceptions through a kind of
                allusive shorthand. When we talk about the language of fashion, we suggest that in
                the context of our conversation, instances like <quote>English,</quote>
                <quote>Chinese,</quote> and <quote>fashion</quote> are alike in ways that
                distinguish them from other things that we would not categorize as languages.</para>
            
            
            
        </section>
        
        <!-- ###################### SECTION ########################################### -->
    </section>
    <!--              -->
    <?dbfo clear ?>
    <?need 5cm ?>
    <section xml:id="section-7.5" label="7.5">
        <title>Implementing Categories</title>
        <info>
            <itermset>
                <indexterm zone="section-7.5">
                    <primary>implementing</primary>
                    <secondary>categories</secondary>
                </indexterm>
                <indexterm zone="section-7.5">
                    <primary>categories</primary>
                    <secondary>implementing</secondary>
                </indexterm></itermset>
        </info>
        <note userlevel="Editor">
            <para xml:id="para-irc_nvl_gw">Murray, test all xrefs into 7.5, especially 7.5.1, 7.5.2,
                and 7.5.X. </para>
        </note>

        <para audience="CORE" xml:id="para-gp3_m4r_lr">Categories are conceptual constructs that we
            use in a mostly invisible way when we talk or think about them. When we organize our
            kitchens, closets, or file cabinets using shelves, drawers, and folders, these physical
            locations and containers are visible implementations of our personal category system,
            but they are not the categories. This distinction between category design and
            implementation is obvious when we follow signs and labels in libraries or grocery stores
            to find things, search a product catalog or company personnel directory, or analyze a
            set of economic data assembled by the government from income tax forms. These
            institutional categories were designed by people prior to the assignment of resources to
            them. </para>
        <para audience="CORE" xml:id="para-pq3_m4r_lr">This separation between category creation and
            category implementation prompts us to ask how a system of categories can be implemented.
            We will not discuss the implementation of categories in the literal sense of building
            physical or software systems that organize resources. Instead, we will take a
            higher-level perspective that analyzes the implementation problem to be solved for the
            different types of categories discussed in <xref linkend="section-7.3" xrefstyle="short"
            />, and then explain the logic followed to assign resources correctly to them.</para>
        <!--              -->
        <?dbfo clear ?>
        <?need 5cm ?>
        <section label="7.5.1" xml:id="section-7.5.1">
            <title>Implementing Enumerated Categories</title>
            <para xml:id="para-ety_vyk_gw">Categories defined by enumeration are easy to implement.
                The members or legal values in a set define the category, and testing an item for
                membership means looking in the set for it. Enumerated category definitions are
                familiar in drop-down menus and form-filling. You scroll through a list of all the
                countries in the world to search for the one you want in a shipping address, and
                whatever you select will be a valid country name, because the list is fixed until a
                new country is born. Enumerated categories can also be implemented with associative
                arrays (also known as hash tables or dictionaries). With these data structures, a
                test for set membership is even more efficient than searching, because it takes the
                same time for sets of any size (see <xref linkend="section-9.2.1"/>).</para>
        </section>
        <?dbfo clear ?>
        <?need 5cm ?>
        <section xml:id="section-7.5.2" label="7.5.2">
            <title>Implementing Categories Defined by Properties</title>
            <info>
                <itermset>
                    <indexterm zone="section-7.5.2">
                        <primary>implementing</primary>
                        <secondary>classical categories</secondary>
                    </indexterm>
                    <indexterm zone="section-7.5.2">
                        <primary>decision tree</primary>
                        <secondary>simple</secondary>
                    </indexterm>
                </itermset>
            </info>
            <para audience="CORE" xml:id="para-zr3_m4r_lr" revision="4.0" revisionflag="changed"
                    ><phrase role="statement" revision="4.0">The most conceptually simple and
                    straightforward implementation of categories defined by properties adopts the
                        <glossterm linkend="gloss_classical_categories"
                        xreflabel="classical view of">classical view of categories</glossterm> based
                    on necessary and sufficient features. Because such categories are prescriptive
                    with explicit and clear boundaries, classifying items into the categories is
                    objective and deterministic, and supports a well-defined notion of <glossterm
                        linkend="gloss_validation">validation</glossterm> to determine unambiguously
                    whether some instance is a member of the category. Items are classified by
                    testing them to determine if they have the required properties and property
                    values.</phrase>
                <phrase role="statement"> Tests can be expressed as rules:</phrase></para>
            <itemizedlist revision="4.0" revisionflag="added">
                <listitem>
                    <para xml:id="para-qrm_qdl_gw">If instance X has property P, then X is in
                        category Y.</para>
                </listitem>
                <listitem>
                    <para xml:id="para-eyy_rdl_gw">If a home mortgage loan in San Francisco exceeds
                        $625,000, then it is classified as a <quote>jumbo</quote> loan by the
                            <orgname>US Office of Federal Housing Oversight</orgname>. </para>
                </listitem>
                <listitem>
                    <para xml:id="para-ngy_sdl_gw">For a number to be classified as prime it must
                        satisfy two rules: It must be greater than 1, and have no positive divisors
                        other than 1 and itself.</para>
                </listitem>
            </itemizedlist>
            <para xml:id="para-cbf_j2l_gw" revision="4.0" revisionflag="added">This doesn’t mean the
                property test is always easy; validation might require special equipment or
                calculations, and tests for the property might differ in their cost or efficiency.
                But given the test results, the answer is unambiguous. The item is either a member
                of the category or it isn’t.<footnote label="437" xml:id="endnote-368a"
                    revision="4.0" revisionflag="added" audience="Computing">
                    <para xml:id="para-yxv_m2l_gw" audience="Computing">For example, you can test whether a number is
                        prime by dividing it by every number smaller than its square root, but this
                        algorithm is ridiculously impractical for any useful application. Many
                        cryptographic systems multiply prime numbers to create encryption keys,
                        counting on the difficulty of factoring them to protect the keys; so,
                        proving that ever larger numbers are prime is very important. See <citation
                            linkend="Crandall2006" xml:id="cite_Crandall2006">(Crandall and
                            Pomerance 2006)</citation>.</para>
                    <para xml:id="para-kvf_p2l_gw" audience="Computing">If you are wondering why prime numbers aren’t
                        considered an enumerative category given that every number that is prime
                        already exists, it is because we have not found all of them yet, and we need
                        to test through to infinity.</para>
                </footnote></para>
            <para xml:id="para-obm_1fl_gw" revision="4.0" revisionflag="added">A system of
                hierarchical categories is defined by a sequence of property tests in a particular
                order. The most natural way to implement multi-level category systems is with
                    <glossterm linkend="gloss_decision_tree">decision trees</glossterm>. <phrase
                    role="definition" xml:id="def_decision_tree">A simple <glossterm
                        xml:id="term_decision_tree">decision tree</glossterm> is an algorithm for
                    determining a decision by making a sequence of logical or property
                    tests.</phrase> Suppose a bank used a sequential rule-based approach to decide
                whether to give someone a mortgage loan.</para>
            <itemizedlist revision="4.0" revisionflag="added">
                <listitem>
                    <para xml:id="para-gyc_cfl_gw"> If applicant’s annual income exceeds $100,000,
                        and if the monthly loan payment is less than 25% of monthly income, approve
                        the mortgage application.</para>
                </listitem>
                <listitem>
                    <para xml:id="para-w2s_dfl_gw">Otherwise, deny the loan application.</para>
                </listitem>
            </itemizedlist>
            <para xml:id="para-erh_hfl_gw" revision="4.0" revisionflag="added">This simple decision
                tree is depicted in <xref linkend="figure-7.1"/>. The rules used by the bank to
                classify loan applications as <quote>Approved</quote> or <quote>Denied</quote> have
                a clear representation in the tree. The easy interpretation of decision trees makes
                them a common formalism for implementing classification models.</para>
            <figure xml:id="figure-7.1" revision="4.0" revisionflag="added">
                <title>Rule-based Decision Tree</title>
                <mediaobject>
                    <imageobject>
                        <imagedata fileref="figs/RulesDecisionTree.png" format="JPG" scalefit="1"/>
                    </imageobject>
                    <textobject>
                        <phrase role="ALT descriptive">Flow chart shows decision points. Deny loan
                            if income below $100k; otherwise, deny if loan payment above 25% of
                            monthly income; otherwise approve loan.</phrase>
                    </textobject>
                    <caption>
                        <para audience="CORE" xml:id="para-mym_km2_nw">In this simple decision tree,
                            a sequence of two tests for the borrower's annual income and the
                            percentage of monthly income required to make the loan payment classify
                            the applicants into the <quote>deny</quote> and <quote>approve</quote>
                            categories.</para>
                    </caption>
                </mediaobject>
            </figure>
            <para xml:id="para-nkf_4gl_gw" revision="4.0" revisionflag="added">Nevertheless, any
                implementation of a category is only interpretable to the extent that the properties
                and tests it uses in its definition and implementation can be understood. Because
                natural language is inherently ambiguous, it is not the optimal representational
                format for formally defined institutional categories. Categories defined using
                natural language can be incomplete, inconsistent, or ambiguous because words often
                have multiple meanings. This implementation of the bank’s procedure for evaluating
                loans would be hard to interpret reliably:</para>
            <itemizedlist revision="4.0" revisionflag="added">
                <listitem>
                    <para xml:id="para-kh4_pgl_gw">If applicant is wealthy, and then if the monthly
                        payment is an amount that the applicant can easily repay, then applicant is
                        approved. </para>
                </listitem>
            </itemizedlist>
            <para xml:id="para-n2b_tgl_gw" revision="4.0" revisionflag="added">To ensure their
                interpretability, decision trees are sometimes specified using the controlled
                vocabularies and constrained syntax of <quote>simplified writing</quote> or
                    <quote>business rule</quote> systems.</para>
            <sidebar xml:id="sidebar-6.5.1-DecisionTree" revision="4.0" revisionflag="deleted"
                audience="DELETED" userlevel="Editor">
                <?dbhtml sidebar-width="50%"?>
                <?dbhtml float-type="right"?>
                <title>Everyday Decision Trees</title>
                <info>
                    <itermset>
                        <indexterm zone="sidebar-6.5.1-DecisionTree" condition="print"
                            userlevel="Professional Graduate">
                            <primary>sidebar</primary>
                            <secondary>Everyday Decision Trees</secondary>
                        </indexterm>
                        <indexterm zone="sidebar-6.5.1-DecisionTree" condition="print"
                            userlevel="Professional Graduate">
                            <primary>Everyday Decision Trees</primary>
                        </indexterm>
                        <indexterm zone="sidebar-6.5.1-DecisionTree">
                            <primary>decision tree</primary>
                            <secondary>everyday</secondary>
                        </indexterm>
                    </itermset>
                </info>
                <informalfigure xml:id="PICTURE-6.5.1-DecisionTree">
                    <mediaobject>
                        <imageobject>
                            <imagedata fileref="Pictures/6.5.1-DecisionTree.jpg" format="JPG"/>
                        </imageobject>
                        <textobject>
                            <phrase role="ALT descriptive">Detail of a complex decision tree
                                offering different typeface suggestions based on the nature of one's
                                graphic design project.</phrase>
                        </textobject>
                        <caption>
                            <para audience="CORE" xml:id="para-fw3_m4r_lr"><phrase role="caption">
                                    Decision trees full of snarky or humorous descriptions of
                                    everyday situations have become a popular Internet meme in the
                                    past few years. Good examples include designer <link
                                        xmlns:xlink="http://www.w3.org/1999/xlink"
                                        xlink:href="http://julianhansen.com/">Julian Hansen</link>'s
                                        <quote>So You Need A Typeface</quote> infographic (pictured)
                                    and Jessica Hische's <quote>Should I Work for Free?</quote>
                                    <link xmlns:xlink="http://www.w3.org/1999/xlink"
                                        xlink:href="http://www.shouldiworkforfree.com/clean.html"
                                        >website</link> for freelance workers.</phrase></para>
                            <para audience="CORE" xml:id="para-mx3_m4r_lr"><phrase role="credit"
                                    >(Screenshot by <personname><firstname>Ian</firstname>
                                        <surname>MacFarland</surname></personname>.)</phrase></para>
                        </caption>
                    </mediaobject>
                </informalfigure>
            </sidebar>
            <para audience="CORE" xml:id="para-shb_rnt_ts" revision="4.0" revisionflag="changed"><info>
                    <itermset>
                        <indexterm zone="para-shb_rnt_ts">
                            <primary>artificial language</primary>
                            <secondary>concision</secondary>
                        </indexterm>
                    </itermset>
                </info>Artificial languages are a more ambitious way to enable precise specification
                of property-based categories. An artificial language expresses ideas concisely by
                introducing new terms or symbols that represent complex ideas along with syntactic
                mechanisms for combining and operating on them. Mathematical notation, programming
                languages, schema languages that define valid document instances (see <xref
                    linkend="section-9.2.3.1" xrefstyle="short"/>), and regular expressions that
                define search and selection patterns  (see <xref linkend="section-9.2.3.2"
                    xrefstyle="short"/>) are familiar examples of artificial languages. It is
                certainly easier to explain and understand the Pythagorean Theorem when it is
                efficiently expressed as <quote>H<superscript>2</superscript> =
                        A<superscript>2</superscript> + B<superscript>2</superscript></quote> than
                with a more verbose natural language expression: <quote>In all triangles with an
                    angle such that the sides forming the angle are perpendicular, the product of
                    the length of the side opposite the angle such that the sides forming the angle
                    are perpendicular with itself is equal to the sum of the products of the lengths
                    of the other two sides, each with itself.</quote><footnote label="438"
                    audience="CogSci" xml:id="endnote-369a" revision="3.0" revisionflag="added">
                    <para audience="CogSci" xml:id="para-sq3_22w_ps" revision="3.0"
                        revisionflag="added"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-369a">
                                    <primary>endnote</primary>
                                    <secondary>CogSci</secondary>
                                </indexterm>
                                <indexterm zone="endnote-369a">
                                    <primary>cognitive science</primary>
                                    <secondary>natural artificial languages</secondary>
                                </indexterm>
                                <indexterm zone="endnote-369a">
                                    <primary>artificial language</primary>
                                    <secondary>natural</secondary>
                                </indexterm>
                            </itermset>
                        </info>This example comes from <citation xml:id="cite_Perlman1984"
                            linkend="Perlman1984">(Perlman 1984)</citation>, who introduced the idea
                        of <quote>natural artificial languages</quote> as those designed to be easy
                        to learn and use because they employ mnemonic symbols, suggestive names, and
                        consistent syntax.</para>
                </footnote></para>
            <para audience="CORE"><phrase revision="3.0" revisionflag="added">Artificial languages
                    for defining categories have a long history in philosophy and science. (See the
                    sidebar, <xref linkend="sidebar-6.5.1-ArtificialLangDescClassif"/>).</phrase>
                However, the vast majority of institutional category systems are still specified
                with natural language, despite its ambiguities because people usually understand the
                languages they learned naturally better than artificial ones. Sometimes this is even
                intentional to allow institutional categories embodied in laws to evolve in the
                courts and to accommodate technological advances.<footnote xml:id="endnote-370"
                    label="439" audience="Law">
                    <para audience="Law" xml:id="para-sbj_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-370">
                                    <primary>endnote</primary>
                                    <secondary>Law</secondary>
                                </indexterm>
                                <indexterm zone="endnote-370">
                                    <primary>law</primary>
                                    <secondary>fair use</secondary>
                                </indexterm>
                                <indexterm zone="endnote-370">
                                    <primary>fair use doctrine</primary>
                                </indexterm>
                                <indexterm zone="endnote-370">
                                    <primary>fair use doctrine</primary>
                                </indexterm>
                                <indexterm zone="endnote-370">
                                    <primary>copyright</primary>
                                    <secondary>fair use doctrine</secondary>
                                </indexterm>
                                <indexterm zone="endnote-370">
                                    <primary>Samuelson, Pamela</primary>
                                </indexterm>
                            </itermset>
                        </info>When the <orgname>US Congress</orgname> revised copyright law in
                            <date>1976</date> it codified a <quote>fair use</quote> provision to
                        allow for some limited uses of copyrighted works, but fair use in the
                        digital era is vastly different today; website caching to improve
                        performance and links that return thumbnail versions of images are fair uses
                        that were not conceivable when the law was written. A law that precisely
                        defined fair uses using contemporary technology would have quickly become
                        obsolete, but one written more qualitatively to enable interpretation by the
                        courts has remained viable. See <citation xml:id="cite_Samuelson2009"
                            linkend="Samuelson2009">(Samuelson 2009)</citation>.</para>
                </footnote></para>
            <sidebar xml:id="sidebar-6.5.1-ArtificialLangDescClassif">
                <title>Artificial Languages for Description and Classification</title>
                <info>
                    <itermset>
                        <indexterm zone="sidebar-6.5.1-ArtificialLangDescClassif" condition="print" userlevel="Professional Graduate">
                            <primary>sidebar</primary>
                            <secondary>Artificial Languages for Description and
                                Classification</secondary>
                        </indexterm>
                        <indexterm zone="sidebar-6.5.1-ArtificialLangDescClassif" condition="print" userlevel="Professional Graduate">
                            <primary>Artificial Languages for Description and
                                Classification</primary>
                        </indexterm>
                        <indexterm zone="sidebar-6.5.1-ArtificialLangDescClassif">
                            <primary>artificial language</primary>
                            <secondary>description and classification</secondary>
                        </indexterm>
                    </itermset>
                </info>
                <para><personname><firstname>John</firstname>
                        <surname>Wilkins</surname></personname> was one of the founders of the
                        <orgname>British Royal Society</orgname>. In 1668 he published <citetitle
                        linkend="Wilkins1668" xml:id="cite_Wilkins1668">An Essay towards a Real
                        Character and a Philosophical Language</citetitle> in which he proposed an
                    artificial language for describing a universal taxonomy of knowledge that used
                    symbol composition to specify a location in the category hierarchy. There were
                    forty top level genus categories, which were further subdivided into differences
                    within the genus, which were then subdivided into species. Each genus was a
                    monosyllable of two letters; each difference added a consonant, and each species
                    added a vowel. </para>
                <para>This artificial language conveys the meaning of categories directly from the
                    composition of the category name. For instance, <wordasword>zi</wordasword>
                    indicates the genus of beasts, <wordasword>zit</wordasword> would be
                        <quote>rapacious beasts of the dog kind</quote> whereas
                        <wordasword>zid</wordasword> would be <quote>cloven-footed beast.</quote>
                    Adding for the fourth character an <wordasword>a</wordasword> for species,
                    indicating the second species in the difference, would give
                        <wordasword>zita</wordasword> for dog and <wordasword>zida</wordasword> for
                    sheep. </para>
                <para>In <citetitle linkend="Borges1952" xml:id="cite_Borges1952">The Analytical
                        Language of John Wilkins,</citetitle> <personname><firstname>Jorge
                            Luis</firstname>
                        <surname>Borges</surname></personname> remarks that Wilkins has many
                        <quote>ambiguities, redundancies and deficiencies</quote> in the language
                    and presents as a foil and parody an imagined <quote>Celestial Empire of
                        Benevolent Knowledge.</quote></para>
                <blockquote>
                    <para>In its remote pages it is written that the animals are divided into: (a)
                        belonging to the emperor, (b) embalmed, (c) tame, (d) sucking pigs, (e)
                        sirens, (f) fabulous, (g) stray dogs, (h) included in the present
                        classification, (i) frenzied, (j) innumerable, (k) drawn with a very fine
                        camel hair brush, (l) et cetera, (m) having just broken the water pitcher,
                        (n) that from a long way off look like flies. </para>
                </blockquote>
                <para>Borges compliments Wilkins for inventing names that might signify in
                    themselves some meaning to those who know the system, but notes that <quote>it
                        is clear that there is no classification of the Universe not being arbitrary
                        and full of conjectures.</quote><footnote label="440" xml:id="endnote-370a"
                        audience="Linguistics" revision="3.0" revisionflag="added">
                        <para audience="Linguistics" xml:id="para-fns_d2w_ps" revision="3.0"
                            revisionflag="added"><info>
                                <itermset>
                                    <indexterm audience="Markup" zone="endnote-370a">
                                        <primary>endnote</primary>
                                        <secondary>Linguistics</secondary>
                                    </indexterm>
                                    <indexterm zone="endnote-370a">
                                        <primary>linguistics</primary>
                                        <secondary>Wilkins and Borges</secondary>
                                    </indexterm>
                                    <indexterm zone="endnote-370a">
                                        <primary>artificial language</primary>
                                        <secondary>Wilkins and Borges</secondary>
                                    </indexterm>
                                </itermset>
                            </info><citation linkend="Wilkins1668" xml:id="cite_Wilkins1668-fn370a"
                                >(Wilkins 1668)</citation> and <citation
                                xml:id="cite_Borges1952-fn370a" linkend="Borges1952">(Borges
                                1952)</citation>
                        </para>
                    </footnote></para>
            </sidebar>
            <para xml:id="para-bdj_m4r_lr"><info>
                    <itermset>
                        <indexterm significance="preferred" zone="def_data_schema">
                            <primary>data</primary>
                            <secondary>schema</secondary>
                        </indexterm>
                        <indexterm zone="def_data_schema">
                            <primary>schema</primary>
                            <secondary>data</secondary>
                        </indexterm>
                        <indexterm zone="def_data_schema">
                            <primary>XML</primary>
                            <secondary>data schema</secondary>
                        </indexterm>
                        <indexterm zone="para-bdj_m4r_lr">
                            <primary>Document Type Spectrum</primary>
                        </indexterm>
                    </itermset>
                </info><phrase role="definition" xml:id="def_data_schema"><glossterm
                        xml:id="term_data_schema">Data schemas</glossterm> that specify data
                    entities, elements, identifiers, attributes, and relationships in databases and
                        <abbrev>XML</abbrev> document types on the transactional end of the Document
                    Type Spectrum <phrase role="parenthetical">(<xref linkend="section-4.2.1"
                            xrefstyle="short"/>)</phrase> are implementations of the categories
                    needed for the design, development and maintenance of information organization
                    systems. Data schemas tend to rigidly define categories of resources.</phrase>
                <footnote xml:id="endnote-371" label="441" audience="Computing">
                    <para audience="Computing" xml:id="para-s2j_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-371">
                                    <primary>endnote</primary>
                                    <secondary>Computing</secondary>
                                </indexterm>
                                <indexterm zone="endnote-371">
                                    <primary>primary key</primary>
                                </indexterm>
                                <indexterm zone="endnote-371">
                                    <primary>computing</primary>
                                    <secondary>primary key</secondary>
                                </indexterm>
                            </itermset>
                        </info><quote>Rigid</quote> might sound negative, but a rigidly defined
                        resource is also precisely defined. Precise definition is essential when
                        creating, capturing, and retrieving data and when information about
                        resources in different organizing systems needs to be combined or compared.
                        For example, in a traditional relational database, each table contains a
                        field, or combination of fields, known as a primary key, which is used to
                        define and restrict membership in the table. A table of email messages in a
                        database might define an email message as a unique combination of sender
                        address, recipient address, and date/time when the message was sent, by
                        enforcing a primary key on a combination of these fields. Similar to
                        category membership based on a single, monothetic set of properties,
                        membership in this email message table is based on a single set of required
                        criteria. An item without a recipient address cannot be admitted to the
                        table. In categorization terms, the item is not a member of the <quote>email
                            message</quote> class because it does not have all the properties
                        necessary for membership.</para>
                </footnote></para>
            <para xml:id="para-ihj_m4r_lr"><info>
                    <itermset>
                        <indexterm significance="preferred" zone="def_classes">
                            <primary>classes</primary>
                        </indexterm>
                        <indexterm zone="def_classes">
                            <primary>data structures</primary>
                            <secondary>classes</secondary>
                        </indexterm>
                    </itermset>
                </info><phrase role="definition" xml:id="def_classes">In object-oriented programming
                    languages, <glossterm xml:id="term_classes" xreflabel="classes"
                        >classes</glossterm> are schemas that serve as templates for the creation of
                    objects. A class in a programming language is analogous to a database schema
                    that specifies the structure of its member instances, in that the class
                    definition specifies how instances of the class are constructed in terms of data
                    types and possible values. Programming classes may also specify whether data in
                    a member object can be accessed, and if so, how.</phrase><footnote
                    xml:id="endnote-372" label="442" audience="Computing">
                    <para audience="Computing" xml:id="para-r3j_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-372">
                                    <primary>endnote</primary>
                                    <secondary>Computing</secondary>
                                </indexterm>
                                <indexterm zone="endnote-372">
                                    <primary>classes</primary>
                                </indexterm>
                                <indexterm zone="endnote-372">
                                    <primary>computing</primary>
                                    <secondary>classes</secondary>
                                </indexterm>
                            </itermset>
                        </info>Like <glossterm linkend="gloss_data_schema">data schemas</glossterm>,
                        programming classes specify and enforce rules in the construction and
                        manipulation of data. However, programming classes, like other
                        implementations that are characterized by specificity and rule enforcement,
                        can vary widely in the degree to which rules are specified and enforced.
                        While some class definitions are very rigid, others are more flexible. Some
                        languages have abstract types that have no instances but serve to provide a
                        common ancestor for specific implemented types.</para>
                </footnote></para>
            <para audience="CORE" xml:id="para-fkj_m4r_lr"><phrase role="statement">Unlike
                    transactional document types, which can be prescriptively defined as <glossterm
                        linkend="gloss_classical_categories">classical categories</glossterm>
                    because they are often produced and consumed by automated processes, narrative
                    document types are usually descriptive in character.</phrase> We do not classify
                something as a novel because it has some specific set of properties and content
                types. Instead, we have a notion of typical novels and their characteristic
                properties, and some things that are considered novels are far from typical in their
                structure and content.<footnote xml:id="endnote-373" label="443" audience="CogSci">
                    <para audience="CogSci" xml:id="para-plj_m4r_lr"><info>
                            <itermset>
                                <indexterm audience="Markup" zone="endnote-373">
                                    <primary>endnote</primary>
                                    <secondary>CogSci</secondary>
                                </indexterm>
                                <indexterm zone="endnote-373">
                                    <primary>Joyce, James</primary>
                                </indexterm>
                                <indexterm zone="endnote-373">
                                    <primary>Beckett, Samuel</primary>
                                </indexterm>
                                <indexterm zone="endnote-373">
                                    <primary>cognitive science</primary>
                                    <secondary>stream of consciousness</secondary>
                                </indexterm>
                            </itermset>
                        </info>The existence of chapters might suggest that an item is a novel;
                        however, a lack of chapters need not automatically indicate that an item is
                        not a novel. Some novels are hypertexts that encourage readers to take
                        alternative paths. Many of the writings by
                                <personname><firstname>James</firstname>
                            <surname>Joyce</surname></personname> and
                                <personname><firstname>Samuel</firstname>
                            <surname>Beckett</surname></personname> are <quote>stream of
                            consciousness</quote> works that lack a coherent plot, yet they are
                        widely regarded as novels.</para>
                </footnote></para>
            <para audience="CORE" role="comparative" xml:id="para-ymj_m4r_lr"><info>
                    <itermset>
                        <indexterm zone="para-ymj_m4r_lr">
                            <primary>constraints</primary>
                            <secondary>schema</secondary>
                        </indexterm>
                    </itermset>
                </info>Nevertheless, categories like narrative document types can sometimes be
                implemented using document schemas that impose only a few constraints on structure
                and content. A schema for a purchase order is highly prescriptive; it uses
                    <glossterm linkend="gloss_regular_expressions">regular expressions</glossterm>,
                strongly data typed content, and enumerated code lists to validate the value of
                required elements that must occur in a particular order. In contrast, a schema for a
                narrative document type would have much optionality, be flexible about order, and
                expect only text in its sections, paragraphs and headings. <phrase role="statement"
                    >Even very lax document schemas can be useful in making content management,
                    reuse, and formatting more efficient.</phrase></para>
        </section>
        <!--              -->
        <?dbfo clear ?>
        <?need 5cm ?>
        <section label="7.5.3" xml:id="section-7.5.3" revision="4.0" revisionflag="added">
            <title>Implementing Categories Defined by Probability and Similarity</title>
            <para xml:id="para-tpq_jjl_gw" revision="4.0" revisionflag="added">Many categories
                cannot be defined in terms of required properties, and instead must be defined
                probabilistically, where category membership is determined by properties that
                resources are likely to share. Consider the category <quote>friend.</quote> You probably consider
                many people to be your friends, but you have longtime friends, school friends,
                workplace friends, friends you see only at the gym, and friends of your parents.
                Each of these types of friends represents a different cluster of common properties.
                If someone is described to you as a potential friend or date, how accurately can you
                predict that the person will become a friend? <phrase audience="DS"
                    role="parenthetical">(See the sidebar, <xref linkend="sidebar-FindingFriends"
                    />)</phrase></para>
            <sidebar xml:id="sidebar-FindingFriends" audience="DS" revision="4.0"
                revisionflag="added">
                <title>Finding Friends and Dates: Lessons for Learning Categories</title>
                <info userlevel="Professional Graduate">
                    <itermset>
                        <indexterm zone="sidebar-FindingFriends">
                            <primary>sidebar</primary>
                            <secondary>Finding Friends</secondary>
                        </indexterm>
                        <indexterm zone="sidebar-FindingFriends">
                            <primary>Finding Friends</primary>
                        </indexterm>
                        <indexterm zone="sidebar-FindingFriends">
                            <primary>machine learning</primary>
                            <secondary>curse of dimensionality</secondary>
                        </indexterm>
                        <indexterm zone="sidebar-FindingFriends">
                            <primary>data science</primary>
                            <secondary>curse of dimensionality</secondary>
                        </indexterm>
                        <indexterm zone="sidebar-FindingFriends">
                            <primary>curse of dimensionality</primary>
                        </indexterm>
                        <indexterm zone="sidebar-FindingFriends">
                            <primary>concept</primary>
                            <secondary>size principle</secondary>
                        </indexterm>
                    </itermset>
                </info>
                <para xml:id="para-nk4_sjl_gw">Online dating or matchmaking sites use many of the
                    same features to describe people, but also have additional features to make more
                    accurate matches for their targeted users. As the number of features grows,
                    there are exponentially more combinations of shared properties. For example, the
                        <application>matchmaking site</application>
                    <application>eHarmony</application> employs 29 <quote>Dimensions of
                        Compatibility</quote> and more than 200 questions to create a user profile.
                    Even if the 29 dimensions were Boolean (would you describe yourself as x?) this
                    yields 2<superscript>29</superscript> or over 500,000,000 different
                    combinations. Using these complex resource descriptions to predict the
                    probability of a good match requires matchmaking sites to use proprietary
                    machine learning algorithms to propose matches, which are ranked with
                    unexplained measures and precision (what does an 80% match mean?). Not
                    surprisingly, many people who try online dating give up after less success than
                    they expected. </para>
                <para xml:id="para-uk2_wjl_gw">With such a large number of features in user
                    profiles, any matching algorithm confronts what machine learning calls the curse
                    of dimensionality. With high-dimensional data, there can never be enough
                    instances to learn which features are really the most important. Neither you nor
                    the online dating algorithm will ever meet enough different kinds of people to
                    reliably predict the outcome of a possible match. </para>
                <para xml:id="para-lnw_wjl_gw">But all is not hopeless. Machine learning programs
                    attack the curse of dimensionality using statistical techniques that use
                    correlations among features to combine them or adjust the weights given to
                    features to reflect their value in making predictions or classifications. For
                    example, <application>OKCupid</application> asks people to rate how much
                    importance they assign to match questions. You might prefer cats to dogs, and
                    you might either never consider dating a dog lover or you might not care at all. </para>
                <para xml:id="para-qcx_xjl_gw">Another way to reduce the number of features needed
                    to classify accurately is to reduce the scope of the category being learned. The
                    matchmaking model for sites that target people with particular professions,
                    religions, or political views would be less complex than the
                        <application>eHarmony</application> one, because the former will have fewer
                    relevant features, and hence fewer random correlations and noise that will
                    undermine its accuracy. All other things being equal, the lower the variability
                    in a set of examples, the better a model that learns from that data will
                    perform.</para>
            </sidebar>
            <para xml:id="para-w2p_4j5_gw">Probabilistic categories can be challenging to define and
                use because it can be difficult to keep in mind the complex feature correlations and
                probabilities exhibited by different clusters of instances from some domain.
                Furthermore, when the category being learned is broad with a large number of
                members, the sample from which you learn strongly shapes what you learn. For
                example, people who grow up in high-density and diverse urban areas may have less
                predictable ideas of what an acceptable potential date looks like than someone in a
                remote rural area with a more homogeneous population. </para>
            <para xml:id="para-zw3_bjs_3w" audience="DS CORE" revision="4.0" revisionflag="added"
                >More generally, if you are organizing a domain where the resources are active,
                change their state, or are measurements of properties that vary and co-occur
                probabilistically, the sample you choose strongly affects the accuracy of models for
                classification or prediction. In <citetitle linkend="Silver2012">The Signal and the
                    Noise,</citetitle> <jobtitle>statistician</jobtitle>
                <personname><firstname>Nate</firstname>
                    <surname>Silver</surname></personname> explains how many notable predictions
                failed because of poor sampling techniques. One common sampling mistake is to use
                too short a historical window to assemble the training dataset; this is often a
                corollary of a second mistake, an over reliance on recent data because it is more
                available. For example, the collapse of housing prices and the resulting financial
                crisis of 2008 can be explained in part because the models that lenders used to
                predict mortgage foreclosures were based on data from 1980-2005, when house prices
                tended to grow higher. As a result, when mortgage foreclosures increased rapidly,
                the results were <quote>out of sample</quote> and were initially misinterpreted,
                delaying responses to the crisis. </para>
            <para xml:id="para-jbw_ljs_3w" audience="DS" revision="4.0" revisionflag="added">Samples
                from dynamic and probabilistic domains result in models that capture this
                variability. Unfortunately, because many forecasters want to seem authoritative, and
                many people do not understand probability, classifications or predictions that are
                inherently imprecise are often presented with certainty and exactness even though
                they are probabilistic with a range of outcomes. Silver tells the story of a
                disastrous 1997 flood caused when the Red River crested at 54 feet when the levees
                protecting the town of Grand Forks were at 51 feet. The weather service had
                predicted a crest between 40 and 58 feet, but emphasized the midpoint of the range,
                which was 49 feet. Unfortunately, most people interpreted this probabilistic
                prediction as if it were a binary classification, <quote>flood</quote> versus
                    <quote>no flood,</quote> ignored the range of the forecast, and failed to
                prepare for a flood that had about a 35% chance of occurring.<footnote audience="DS"
                    label="444" revision="4.0" revisionflag="added" xml:id="endnote-0000444">
                    <para audience="DS" xml:id="para-w2k_tjs_3w">See <citation linkend="Silver2012">(Silver
                            2012)</citation>. Over reliance on data that is readily available is a
                        decision-making heuristic proposed by <citation linkend="Tversky1974"
                            >(Tversky and Kahneman 1974)</citation>, who developed the psychological
                        foundations for behavioral economics. <phrase role="parenthetical">(See the
                            sidebar, <xref linkend="sidebar-10.2.1-BehavEcon"/>.) </phrase></para>
                </footnote></para>
            <?dbfo clear ?>
            <?need 7cm ?>
            <section label="7.5.3.1" xml:id="section-7.5.3.1" revision="4.0" revisionflag="added">
                <title>Probabilistic Decision Trees</title>
                <info>
                    <itermset>
                        <indexterm zone="section-7.5.3.1" audience="DS">
                            <primary>decision tree</primary>
                            <secondary>probabilistic</secondary>
                        </indexterm>
                        <indexterm zone="section-7.5.3.1" audience="DS">
                            <primary>data science</primary>
                            <secondary>decision tree</secondary>
                        </indexterm>
                        <indexterm zone="section-7.5.3.1" audience="DS">
                            <primary>information gain</primary>
                        </indexterm>
                        <indexterm zone="section-7.5.3.1" audience="DS">
                            <primary>overfitting</primary>
                        </indexterm>
                    </itermset>
                </info>
                <para xml:id="para-pck_bj5_gw">In <xref linkend="section-7.5.2" xrefstyle="short"/>,
                    we showed how a rule-based decision tree could be used to implement a strict
                    property-based classification in which a bank uses tests for the properties of
                        <quote>annual income</quote> and <quote>monthly loan payment</quote> to
                    classify applicants as approved or denied. We can adapt that example to
                    illustrate probabilistic decision trees, which are better suited for
                    implementing categories in which category membership is probabilistic rather
                    than absolute.</para>
                <para xml:id="para-m5k_p2y_mw">Banks that are more flexible about making loans can
                    be more profitable because they can make loans to people that a stricter bank
                    would reject but who still are able to make loan payments. Instead of enforcing
                    conservative and fixed cutoffs on income and monthly payments, these banks
                    consider more properties and look at applications in a more probabilistic way.
                    These banks recognize that not every loan applicant who is likely to repay the
                    loan looks exactly the same; <quote>annual income</quote> and <quote>monthly loan payment</quote> remain
                    important properties, but other factors might also be useful predictors, and
                    there is more than one configuration of values that an applicant could satisfy
                    to be approved for a loan.</para>
                <para xml:id="para-tq5_q2y_mw">Which properties of applicants best predict whether
                    they will repay the loan or default? A property that predicts each at 50% isn’t
                    helpful because the bank might as well flip a coin, but a property that splits
                    the applicants into two sets, each with very different probabilities for
                    repayment and defaulting, is very helpful in making a loan decision.</para>
                <para xml:id="para-h2q_r2y_mw">A data-driven bank relies upon historical data about
                    loan repayment and defaults to train algorithms that create decision trees by
                    repeatedly splitting the applicants into subsets that are most different in
                    their predictions. Subsets of applicants with a high probability of repayment
                    would be approved, and those with a high probability of default would be denied
                    a loan. One method for selecting the property test for making each split is
                    calculating the <quote>information gain</quote>
                    <phrase audience="Computing IA" role="parenthetical">(see the sidebar <xref
                            linkend="sidebar-MeasureInfo"/>)</phrase>. This measure captures the
                    degree to which each subset contains a <quote>pure</quote> group in which every
                    applicant is classified the same, as likely repayers or likely
                    defaulters.</para>
                <para xml:id="para-r2x_52y_mw">For example, consider the chart in <xref
                        linkend="figure-7.2" xrefstyle="short"/> which is a simplified
                    representation of the bank’s historical data on loan defaults based on the
                    initial interest rate. The chart represents loans that were repaid with
                        <quote>o</quote> and those that defaulted with <quote>x.</quote> Is there an
                    interest rate that divides them into <quote>pure</quote> sets, one that contains
                    only <quote>o</quote> loans and the other that contains only  <quote>x</quote>
                    loans?</para>
                <figure xml:id="figure-7.2" label="7.2">
                    <title>Historical Data: Loan Repayment Based on Interest Rate</title>
                    <mediaobject>
                        <imageobject>
                            <imagedata fileref="figs/Figure-6.2.png" format="PNG" scalefit="1"/>
                        </imageobject>
                        <textobject>
                            <phrase role="ALT descriptive">Scatter plot chart.</phrase>
                        </textobject>
                        <caption>
                            <para audience="CORE" xml:id="para-otp_tm2_nw">The <quote>o</quote>
                                symbol represents loans that were repaid by the borrower;
                                    <quote>x</quote> represents loans on which the borrower
                                defaulted. A 6% rate (darker vertical line) best divides the loans
                                into subsets that differ in the payment outcome.</para>
                        </caption>
                    </mediaobject>
                </figure>
                <para xml:id="para-tkk_3fy_mw">You can see that no interest rate divides these into
                    pure sets. So the best that can be done is to find the interest rate that
                    divides them so that the proportions of defaulters are most different on each
                    side of the line.<footnote audience="DS" label="445" revision="4.0"
                        revisionflag="added" xml:id="endnote-0000445">
                        <para audience="DS" xml:id="para-p4b_lfy_mw">To be precise, this <quote>difference of
                                proportions</quote> calculation uses an algorithm that also uses the
                            logarithm of the proportions to calculate entropy, a measure of the
                            uncertainty in a probability distribution. An entropy of zero means that
                            the outcome can be perfectly predicted, and entropy increases as
                            outcomes are less predictable. The information gain for an attribute is
                            how much it reduces entropy after it is used to subdivide a
                            dataset.</para>
                    </footnote></para>
                <para xml:id="para-ffs_nfy_mw">This dividing line at the 6% interest rate best
                    divides those who defaulted from those who repaid their loan. Most people who
                    borrowed at 6% or greater repaid the loan, while those who took out loans at a
                    lower rate were more likely to default. This might seem counter-intuitive until
                    you learn that the lower-interest rate loans had adjustable rates that increased
                    after a few years, causing the monthly payments to increase substantially. More
                    prudent borrowers were willing to pay higher interest rates that were fixed
                    rather than adjustable to avoid radical increases in their monthly
                    payments.</para>
                <figure label="7.3" xml:id="figure-7.3">
                    <title>Probabilistic Decision Tree</title>
                    <mediaobject>
                        <imageobject>
                            <imagedata fileref="figs/ProbDecisionTree.png" format="PNG" scalefit="1"
                            />
                        </imageobject>
                        <textobject>
                            <phrase role="ALT descriptive">Flow chart. Deny loan if annual income
                                below $82k; deny loan if loan payment is above 27% of monthly
                                income; deny loan if interest rate below 6%; otherwise approve loan.
                            </phrase>
                        </textobject>
                        <caption>
                            <para audience="CORE" xml:id="para-z13_5m2_nw">In this probabilistic
                                decision tree, the sequence of property tests and the threshold
                                values in each test divide the loan applicants into categories that
                                differ in how likely they are to repay the loan.</para>
                        </caption>
                    </mediaobject>
                </figure>
                <para xml:id="para-bmj_pfy_mw">This calculation is carried out for each of the
                    attributes in the historical data set to identify the one that best divides the
                    applicants into the repaid and defaulted categories. The attributes and the
                    value that defines the decision rule can then be ordered to create a decision
                    tree similar to the rule-based one we saw in <xref linkend="section-7.5.2"
                        xrefstyle="short"/>. In our hypothetical case, it turns out that the best
                    order in which to test the properties is Income, Monthly Payment, and Interest
                    Rate, as shown in <xref linkend="figure-7.3" xrefstyle="short"/>. The end result
                    is still a set of rules, but behind each decision in the tree are probabilities
                    based on historical data that can more accurately predict whether an applicant
                    will repay or default. Thus, instead of the arbitrary cutoffs at $100,000 in
                    income and 25% for monthly payment, the bank can offer loans to people with
                    lower incomes and remain profitable doing so, because it knows from historical
                    data that $82,000 and 27% are the optimal decision points. Using the interest
                    rate in their decision process is an additional test to ensure that people can
                    afford to make loan payments even if interest rates go up.<footnote label="446"
                        revision="4.0" revisionflag="added" audience="Business"
                        xml:id="endnote-0000446">
                        <para xml:id="para-o4s_rfy_mw" audience="Business">Unfortunately, this rational data-driven
                            process for classifying loan applications as <quote>Approved</quote> or
                                <quote>Denied</quote> was abandoned during the <quote>housing
                                bubble</quote> of the early 2000s. Because lending banks could
                            quickly sell their mortgages to investment banks who bundled them into
                            mortgage-backed securities, applicants were approved without any income
                            verification for <quote>subprime</quote> loans that initially had very
                            low adjustable interest rates. Of course, when the rates increased
                            substantially a few years later, defaults and foreclosures skyrocketed.
                            This sad story is told in an informative, entertaining, but depressing
                            manner in <citetitle><quote>The Big Short</quote></citetitle>
                            <citation xml:id="cite_Lewis2010" linkend="Lewis2010">(Lewis,
                                2010)</citation> and in a 2015 movie with the same name. </para>
                    </footnote></para>
                <para xml:id="para-s52_fj5_gw" audience="DS">Because decision trees specify a
                    sequence of rules that make property tests, they are highly interpretable, which
                    makes them a very popular choice for data scientists building models much more
                    complex than the simple loan example here. But they assume that every class is a
                    conjunction of all the properties used to define them. This makes them
                    susceptible to over-fitting because if they grow very deep with many property
                    conjunctions, they capture exactly the properties that describe each member of
                    the training set, effectively memorizing the training data. In other words, they
                    capture both what is generally true beyond the set and what is particular to the
                    training set only, when the goal is to build a model that captures only what is
                    generally true. Overfitting in decision trees can be prevented by pruning back
                    the tree after it has perfectly classified the training set, or by limiting the
                    depth of the tree in advance, essentially pre-pruning it.</para>
            </section>
            <?dbfo clear ?>
            <?need 6.5cm ?>
            <section label="7.5.3.2" revision="4.0" revisionflag="added" xml:id="section-7.5.3.2">
                <title>Naïve Bayes Classifiers</title>
                <info audience="DS">
                    <itermset>
                        <indexterm zone="section-7.5.3.2">
                            <primary>Bayes’ Theorem</primary>
                        </indexterm>
                        <indexterm zone="section-7.5.3.2">
                            <primary>data science</primary>
                            <secondary>Bayes’ Theorem</secondary>
                        </indexterm>
                        <indexterm zone="section-7.5.3.2">
                            <primary>spam classification</primary>
                        </indexterm>
                    </itermset>
                </info>
                <para xml:id="para-pyd_4g5_gw">Another commonly used approach to implement a
                    classifier for probabilistic categories is called Naïve Bayes. It employs Bayes’
                    Theorem for learning the importance of a particular property for correct
                    classification. There are some common sense ideas that are embodied in Bayes’ Theorem:<itemizedlist>
                        <listitem>
                            <para xml:id="para-nmp_bm5_gw">When you have a hypothesis or prior
                                belief about the relationship between a property and a
                                classification, new evidence consistent with that belief should
                                increase your confidence. </para>
                        </listitem>
                        <listitem>
                            <para xml:id="para-sz1_dm5_gw">Contradictory evidence should reduce
                                confidence in your belief. </para>
                        </listitem>
                        <listitem>
                            <para xml:id="para-r1l_dm5_gw">If the base rate for some kind of event
                                is low, do not forget that when you make a prediction or
                                classification for a new specific instance. It is easy to be overly
                                influenced by recent information.</para>
                        </listitem>
                    </itemizedlist></para>
                <para xml:id="para-h3d_fm5_gw">Now we can translate these ideas into calculations
                    about how learning takes place. For property A and classification B, Bayes’
                    Theorem says: <informalexample>
                        <literallayout>    P (A | B) = P (B|A) P(A) / P(B) </literallayout>
                    </informalexample>The left hand side of the equation, P (A | B), is what we want
                    to estimate but can’t measure directly: the probability that A is the correct
                    classification for an item or observation that has property B. This is called
                    the conditional or posterior probability because it is estimated after seeing
                    the evidence of property B.</para>
                <para xml:id="para-okd_yyx_pw">P (B | A) is the probability that any item correctly
                    classified as A has property B. This is called the likelihood function.</para>
                <para xml:id="para-ejp_zyx_pw">P (A) and P (B) are the independent or prior
                    probabilities of A and B; what proportion of the items are classified as A? How
                    often does property B occur in some set of items?</para>
                <sidebar>
                    <title>Using Bayes’ Theorem to Calculate Conditional Probability</title>
                    <para xml:id="para-dbd_fzx_pw">Your personal library contains 60% fiction and
                        40% nonfiction books. All of the fiction books are in ebook format, and half
                        of the nonfiction books are ebooks and half are in print format. If you pick
                        a book at random and it is in ebook format, what is the probability that it
                        is nonfiction?</para>
                    <para xml:id="para-hct_fzx_pw">Bayes’ Theorem tells us that: </para>
                    <informalexample xml:id="para-mcr_szx_pw">
                        <literallayout>    P (nonfiction | ebook) = P (ebook |nonfiction) x P (nonfiction) / P (ebook).</literallayout>
                    </informalexample>
                    <para xml:id="para-ifm_gzx_pw">We know: P (ebook | nonfiction) = .5 and P
                        (nonfiction) = .4</para>
                    <para xml:id="para-bkc_hzx_pw">We compute P (ebook) using the law of total
                        probability to compute the combined probability of all the independent ways
                        in which an ebook might be sampled. In this example there are two
                        ways:</para>
                    <informalexample xml:id="para-vhy_lzx_pw">
                        <literallayout>    P (ebook) = P (ebook | nonfiction) x P (nonfiction) 
                       + P (ebook | fiction) x P (fiction)
                    = (.5 x .4) + (1 x .6) = .8</literallayout>
                    </informalexample>
                    <para xml:id="para-jrb_4zx_pw">Therefore: P (nonfiction | ebook) = (.5 x .4) /
                        .8 = .25</para>
                </sidebar>
                <para xml:id="para-l1s_nm5_gw">Now let’s apply Bayes’ Theorem to implement email
                    spam filtering. Messages are classified as SPAM or HAM (i.e., non-SPAM); the
                    former are sent to a SPAM folder, while the latter head to your inbox.<orderedlist>
                        <listitem>
                            <para xml:id="para-avm_qm5_gw">Select Properties. We start with a set of
                                properties, some from the message metadata like the sender’s email
                                address or the number of recipients, and some from the message
                                content. Every word that appears in messages can be treated as a
                                separate property<footnote xml:id="endnote-435" label="447"
                                    audience="Computing">
                                    <para audience="Computing" xml:id="para-i5w_44r_lr"><info>
                                            <itermset>
                                                <indexterm audience="Markup" zone="endnote-435">
                                                  <primary>endnote</primary>
                                                  <secondary>Computing</secondary>
                                                </indexterm>
                                                <indexterm zone="endnote-435">
                                                  <primary>computing</primary>
                                                  <secondary>machine learning methods</secondary>
                                                </indexterm>
                                                <indexterm zone="endnote-435">
                                                  <primary>computational</primary>
                                                  <secondary>agents</secondary>
                                                </indexterm>
                                            </itermset>
                                        </info>Machine learning algorithms differ in which
                                        properties they use in how they select them. A
                                        straightforward method is to run the algorithms using
                                        different sets of properties, and select the set that yields
                                        the best result. However, it can be very computationally
                                        expensive to run algorithms multiple times, especially when
                                        the number of properties is large. A faster alternative is
                                        to select or filter features based on how well they predict
                                        the classification. The information gain calculation
                                        discussed in <xref linkend="section-7.5.3.1"/> is an example
                                        of a filter method. </para>
                                    <para audience="Computing" xml:id="para-k2p_j1y_pw">Naïve Bayes classifiers make the
                                        simplifying assumption that the properties are independent,
                                        an assumption that is rarely correct, which is why the
                                        approach is called naïve. For example, a document that
                                        contains the word <quote>insurance</quote> is also likely to
                                        contain <quote>beneficiary,</quote> so their presence in
                                        messages is not independent.</para>
                                    <para audience="Computing" xml:id="para-svr_3p5_gw"
                                        >Nevertheless, even though the independence assumption is
                                        usually violated, Naive Bayes classifiers often perform very
                                        well. Furthermore, treating properties as independent means
                                        that the classifier needs much less data to train than if we
                                        had to calculate the conditional probabilities of all
                                        combination of properties. Instead, we just have to count
                                        separately the number of times each property occurs with
                                        each of the two classification outcomes.</para>
                                </footnote></para>
                        </listitem>
                        <listitem>
                            <para xml:id="para-g1p_sm5_gw">Assemble Training Data. We assemble a set
                                of email message that have been correctly assigned to the SPAM and
                                HAM categories. These labeled instances make up the training set.
                            </para>
                        </listitem>
                        <listitem>
                            <para xml:id="para-rdz_sm5_gw">Analyze the Training Data. For each
                                message, does it contain a particular property? For each message, is
                                it classified as SPAM? If a message is classified as SPAM, does it
                                contain a particular property? (These are the three probabilities on
                                the right side of the Bayes equation). </para>
                        </listitem>
                        <listitem>
                            <para xml:id="para-jc3_tm5_gw">Learn. The conditional probability (the
                                left side of the Bayes equation) is recalculated, adjusting the
                                predictive value of each property. Taken together, all of the
                                properties are now able to correctly assign (most of) the messages
                                into the categories they belonged to in the training set. </para>
                        </listitem>
                        <listitem>
                            <para xml:id="para-anb_5m5_gw">Classify. The trained classifier is now
                                ready to classify uncategorized messages to the SPAM or HAM
                                categories. </para>
                        </listitem>
                        <listitem>
                            <para xml:id="para-h4n_5m5_gw">Improve. The classifier can improve its
                                accuracy if the user gives it feedback by reclassifying SPAM
                                messages as HAM ones or vice versa. The most efficient learning
                                occurs when an algorithm uses <quote>active learning</quote>
                                techniques to choose its own training data by soliciting user
                                feedback only where it is uncertain about how to classify a message.
                                For example, the algorithm might be confident that a message with
                                    <quote>Cheap drugs</quote> in the subject line is SPAM, but if
                                the message comes from a longtime correspondent, the algorithm might
                                ask the user to confirm that the classification.<footnote
                                    xml:id="endnote-436" label="448" audience="Computing">
                                    <para audience="Computing" xml:id="para-mlx_44r_lr"><info>
                                            <itermset>
                                                <indexterm audience="Markup" zone="endnote-436">
                                                  <primary>endnote</primary>
                                                  <secondary>Computing</secondary>
                                                </indexterm>
                                                <indexterm zone="endnote-436">
                                                  <primary>computing</primary>
                                                  <secondary>machine learning methods</secondary>
                                                </indexterm>
                                                <indexterm zone="endnote-436">
                                                  <primary>active learning</primary>
                                                </indexterm>
                                            </itermset>
                                        </info>See <citation xml:id="cite_Blanzieri2009"
                                            linkend="Blanzieri2009">(Blanzieri and Bryl
                                            2009)</citation> for a review of the spam problem and
                                        the policy and technology methods for fighting it. <citation
                                            xml:id="cite_Pandey2010" linkend="Pandey2010">(Upsana
                                            and Chakravarty 2010)</citation> is somewhat more recent
                                        and more narrowly focused on text classification
                                        techniques.</para>
                                    <para audience="Computing" revision="3.0" revisionflag="added"
                                        xml:id="para-jkb_zn5_gw">A very thorough yet highly readable
                                        introduction to Active Learning is <citation
                                            linkend="Settles2012" xml:id="cite_Settles2012">(Settles
                                            2012)</citation>.</para>
                                </footnote></para>
                        </listitem>
                    </orderedlist></para>
            </section>
            <?dbfo clear ?>
            <?need 5cm ?>
            <section label="7.5.3.3" revision="4.0" revisionflag="added" xml:id="section-7.5.3.3">
                <title>Categories Created by Clustering</title>
                <info audience="DS">
                    <itermset>
                        <indexterm zone="section-7.5.3.3">
                            <primary>clustering</primary>
                        </indexterm>
                        <indexterm zone="section-7.5.3.3">
                            <primary>similarity</primary>
                            <secondary>creating clusters using</secondary>
                        </indexterm>
                        <indexterm zone="section-7.5.3.3">
                            <primary>support vector machines</primary>
                        </indexterm>
                        <indexterm zone="section-7.5.3.3">
                            <primary>machine learning</primary>
                            <secondary>support vector machines</secondary>
                        </indexterm>
                    </itermset>
                </info>
                <para xml:id="para-r3h_rp5_gw">In the previous two sections we discussed how
                    probabilistic decision trees and naïve Bayes classifiers implement categories
                    that are defined by typically shared properties and similarity. Both are
                    examples of supervised learning because they need correctly classified examples
                    as training data, and they learn the categories they are taught.</para>
                <para xml:id="para-rg1_sp5_gw">In contrast, clustering techniques are unsupervised;
                    they analyze a collection of uncategorized resources to discover statistical
                    regularities or structure among the items, creating a set of categories without
                    any labeled training data.</para>
                <para audience="CORE" xml:id="para-osj_m4r_lr"><phrase role="definition"
                        xml:id="def_clustering"><glossterm xml:id="term_clustering"
                            >Clustering</glossterm> techniques share the goal of creating meaningful
                        categories from a collection of items whose properties are hard to directly
                        perceive and evaluate, which implies that category membership cannot easily
                        be reduced to specific property tests and instead must be based on
                        similarity.</phrase> For example, with large sets of documents or behavioral
                    data, clustering techniques can find categories of documents with the same
                    topics, genre, or sentiment, or categories of people with similar habits and
                    preferences.</para>
                <para xml:id="para-vch_tp5_gw">Because clustering techniques are unsupervised, they
                    create categories based on calculations of similarity between resources,
                    maximizing the similarity of resources within a category and maximizing the
                    differences between them. These statistically-learned categories are not always
                    meaningful ones that can be named and used by people, and the choice of
                    properties and methods for calculating similarity can result in very different
                    numbers and types of categories. Some clustering techniques for text resources
                    suggest names for the clusters based on the important words in documents at the
                    center of each cluster. However, unless there is a labeled set of resources from
                    the same domain that can be used as a check to see if the clustering discovered
                    the same categories, it is up to the data analyst or information scientist to
                    make sense of the discovered clusters or topics.</para>
                <para xml:id="para-msy_tp5_gw">There are many different distance-based clustering
                    techniques, but they share three basic methods.</para>
                <itemizedlist>
                    <listitem>
                        <para audience="CORE" xml:id="para-wtj_m4r_lr">The first shared method is
                            that clustering techniques start with an initially uncategorized set of
                            items or documents that are represented in ways that enable measures of
                            inter-item similarity can be calculated. This representation is most
                            often a vector of property values or the probabilities of different
                            properties, so that items can be represented in a multidimensional space
                            and similarity calculated using a distance function like those described
                            in <xref linkend="section-7.3.6.2"/>.<footnote xml:id="endnote-374"
                                label="449" audience="Computing">
                                <para audience="Computing" xml:id="para-mvj_m4r_lr"><info>
                                        <itermset>
                                            <indexterm audience="Markup" zone="endnote-374">
                                                <primary>endnote</primary>
                                                <secondary>Computing</secondary>
                                            </indexterm>
                                            <indexterm zone="endnote-374">
                                                <primary>computing</primary>
                                                <secondary>SVM</secondary>
                                            </indexterm>
                                        </itermset>
                                    </info>In particular, documents are usually represented as
                                    vectors of frequency-weighted terms. Other approaches start more
                                    directly with the similarity measure, obtained either by direct
                                    judgments of the similarity of each pair of items or by indirect
                                    measures like the accuracy in deciding whether two sounds,
                                    colors, or images are the same or different. The assumption is
                                    that the confusability of two items reflects how similar they
                                    are.</para>
                            </footnote></para>
                    </listitem>
                    <listitem>
                        <para audience="CORE" xml:id="para-fyj_m4r_lr"><info>
                                <itermset>
                                    <indexterm zone="para-fyj_m4r_lr">
                                        <primary>machine learning</primary>
                                        <secondary>K-means clustering</secondary>
                                    </indexterm>
                                    <indexterm zone="para-fyj_m4r_lr">
                                        <primary>K-means clustering</primary>
                                    </indexterm>
                                </itermset>
                            </info>The second shared method is that categories are created by
                            putting items that are most similar into the same category. Hierarchical
                            clustering approaches start with every item in its own category. Other
                            approaches, notably one called <quote>K-means clustering,</quote> start
                            with a fixed number of K categories initialized with a randomly chosen
                            item or document from the complete set.</para>
                    </listitem>
                    <listitem>
                        <para audience="CORE" xml:id="para-szj_m4r_lr">The third shared method is
                            refining the system of categories by iterative similarity recalculation
                            each time an item is added to a category. Approaches that start with
                            every item in its own category create a hierarchical system of
                            categories by merging the two most similar categories, recomputing the
                            similarity between the new category and the remaining ones, and
                            repeating this process until all the categories are merged into a single
                            category at the root of a category tree. Techniques that start with a
                            fixed number of categories do not create new ones but instead repeatedly
                            recalculate the <quote>centroid</quote> of the category by adjusting its
                            property representation to the average of all its members after a new
                            member is added.<footnote xml:id="endnote-375" label="450"
                                audience="Computing">
                                <para audience="Computing" xml:id="para-bbk_m4r_lr"><info>
                                        <itermset>
                                            <indexterm audience="Markup" zone="endnote-375">
                                                <primary>endnote</primary>
                                                <secondary>Computing</secondary>
                                            </indexterm>
                                            <indexterm zone="endnote-375">
                                                <primary>computing</primary>
                                                <secondary>k-means clustering</secondary>
                                            </indexterm>
                                        </itermset>
                                    </info>Unlike hierarchical clustering methods that have a clear
                                    stopping rule when they create the root category, k-means
                                    clustering methods run until the centroids of the categorize
                                    stabilize. Furthermore, because the k-means algorithm is
                                    basically just hill-climbing, and the initial category
                                        <quote>seed</quote> items are random, it can easily get
                                    stuck in a local optimum. So it is desirable to try many
                                    different starting configurations for different choices of
                                    K.</para>
                            </footnote></para>
                    </listitem>
                </itemizedlist>
                <para xml:id="para-llr_cr5_gw" audience="DS">It makes sense that the algorithms that
                    create clusters or categories of similar items can be later used as classifiers
                    by using the same similarity measures to compare the unclassified items against
                    items that are labeled by category. There are different choices about which
                    items to compare with the unclassified one:<itemizedlist>
                        <listitem>
                            <para xml:id="para-yk3_fr5_gw">The centroid: a prototypical or average
                                item calculated on the properties of all the category members.
                                However, the centroid might not correspond to any actual
                                    member<phrase role="parenthetical"> (see the sidebar <xref
                                        linkend="sidebar-MedianVsAverage"/>)</phrase>, and this can
                                make it hard to interpret the classification.</para>
                        </listitem>
                        <listitem>
                            <para xml:id="para-k1g_kr5_gw">Items that actually exist: Because the
                                items in categories defined by similarity are not equally typical or
                                good members, it is more robust to test against more than one
                                exemplar. Classifiers that use this approach are called
                                nearest-neighbor techniques, and they essentially vote among
                                themselves and the majority category is assigned to the new
                                item.</para>
                        </listitem>
                        <listitem>
                            <para xml:id="para-ewd_lr5_gw">The edge cases: These are instances that
                                are closest to the boundary between two categories, so there need to
                                be at least two of them, one in each category. Because they are not
                                typical members of the category, they are the hardest to classify
                                initially, but using them in classifiers emphasizes the properties
                                that are the most discriminating. This is the approach taken by
                                support vector machines, which are not clustering algorithms but are
                                somewhat like nearest-neighbor algorithms in that they calculate the
                                similarity of an unclassified item to these edge cases. Their name
                                makes more sense if you think of the vectors that represent the
                                    <quote>edge cases</quote> being used to <quote>support</quote>
                                the category boundary, which falls between them.</para>
                        </listitem>
                    </itemizedlist></para>
            </section>
            <?dbfo clear ?>
            <?need 5cm ?>
            <section label="7.5.3.4" revision="4.0" revisionflag="added" xml:id="section-7.5.3.4">
                <title>Neural networks</title>
                <info audience="DS">
                    <itermset>
                        <indexterm zone="section-7.5.3.4">
                            <primary>deep learning</primary>
                        </indexterm>
                        <indexterm zone="section-7.5.3.4">
                            <primary>backpropagation</primary>
                        </indexterm>
                        <indexterm zone="section-7.5.3.4">
                            <primary>autoencoding</primary>
                        </indexterm>
                        <indexterm zone="section-7.5.3.4">
                            <primary>neural networks</primary>
                        </indexterm>
                        <indexterm zone="section-7.5.3.4">
                            <primary>machine learning</primary>
                            <secondary>autoencoding</secondary>
                        </indexterm>
                        <indexterm zone="section-7.5.3.4">
                            <primary>machine learning</primary>
                            <secondary>backpropagation</secondary>
                        </indexterm>
                        <indexterm zone="section-7.5.3.4">
                            <primary>machine learning</primary>
                            <secondary>neural networks</secondary>
                        </indexterm>
                        <indexterm zone="section-7.5.3.4">
                            <primary>machine learning</primary>
                            <secondary>deep learning</secondary>
                        </indexterm>
                    </itermset>
                </info>
                <para xml:id="para-ozn_135_gw">Among the best performing classifiers for
                    categorizing by similarity and probabilistic membership are those implemented
                    using neural networks, and especially those employing deep learning techniques.
                    Deep learning algorithms can learn categories from labeled training data or by
                    using autoencoding, an unsupervised learning technique that trains a neural
                    network to reconstruct its input data. However, instead of using the properties
                    that are defined in the data, deep learning algorithms devise a very large
                    number of features in hidden hierarchical layers, which makes them
                    uninterpretable by people. The key idea that made deep learning possible is the
                    use of <quote>backpropagation</quote> to adjust the weights on features by
                    working backwards from the output (the object classification produced by the
                    network) all the way back to the input. The use of deep learning to classify
                    images was mentioned in <xref linkend="section-5.4.2" xrefstyle="short"
                        />.<footnote audience="DS" xml:id="endnote-n1" revision="4.0"
                        revisionflag="added" label="451">
                        <para xml:id="para-y1j_w35_gw" audience="DS">In addition, the complex feature
                            representations of neural networks compute very precise similarity
                            measurements, which enable searches for specific images or that find
                            duplicate ones.</para>
                    </footnote></para>
            </section>
        </section>
        <?dbfo clear ?>
        <?need 5cm ?>
        <section label="7.5.4" revision="4.0" revisionflag="added" xml:id="section-7.5.4">
            <title>Implementing Goal-Based Categories</title>
            <para xml:id="para-kyf_srl_gw">Goal-based categories are highly individualized, and are
                often used just once in a very specific context. However, it is useful to consider
                that we could implement model goal-derived categories as rule-based decision trees
                by ordering the decisions to ensure that any sub-goals are satisfied according to
                their priority. We could understand the category <quote>Things to take from a burning
                house</quote> by first asking the question <quote>Are there living things in the house?</quote> because
                that might be the most important sub-goal. If the answer to that question is <quote>yes,</quote>
                we might proceed along a different path than if the answer is <quote>no.</quote> Similarly, we
                might put a higher priority on things that cannot be replaced (Grandma’s photos)
                than those that can (passport).</para>
        </section>
        <?dbfo clear ?>
        <?need 5cm ?>
        <section label="7.5.5" revision="4.0" revisionflag="added" xml:id="section-7.5.5">
            <title>Implementing Theory-Based Categories</title>
            <info audience="DS">
                <itermset>
                    <indexterm zone="section-7.5.5">
                        <primary>Gentner, Dedre</primary>
                    </indexterm>
                    <indexterm zone="section-7.5.5">
                        <primary>structure mapping</primary>
                    </indexterm>
                    <indexterm zone="section-7.5.5">
                        <primary>similarity</primary>
                        <secondary>abstract</secondary>
                    </indexterm>
                    <indexterm zone="section-7.5.5">
                        <primary>similarity</primary>
                        <secondary>relational</secondary>
                    </indexterm>
                    <indexterm zone="section-7.5.5">
                        <primary>similarity</primary>
                        <secondary>structure mapping</secondary>
                    </indexterm>
                </itermset>
            </info>
            <para audience="CORE" xml:id="para-pgs_hs1_fw"><phrase role="principle statement"
                    >Theory-based categories arise in domains in which the items to be categorized
                    are characterized by abstract or complex relationships with their features and
                    with each other. With this model an entity need not be understood as inherently
                    possessing features shared in common with another entity.  Rather, people
                    project features from one thing to another in a search for congruities between
                    things, much as clue receivers in the second round of the
                        <citetitle>Pyramid</citetitle> game search for congruities between examples
                    provided by the clue giver in order to guess the target category. For example, a
                    clue like <quote>screaming baby</quote> can suggest many categories, as can
                        <quote>parking meter.</quote> But the likely intersection of the
                    interactions one can have with babies and parking meters is that they are both
                        <quote>Things you need to feed.</quote></phrase></para>
            <para xml:id="para-pnh_ckb_hw" revision="4.0" revisionflag="added">Theory-based
                categories are created as cognitive constructs when we use analogies and classify,
                because things brought together by analogy have abstract rather than literal
                similarity. The most influential model of analogical processing is Structure
                Mapping, whose development and application has been guided by
                        <personname><firstname>Dedre</firstname>
                    <surname>Gentner</surname></personname> for over three decades. </para>
            <para xml:id="para-ick_2kb_hw">The key insight in Structure Mapping is that an analogy
                    <quote>a T is like B</quote> is created by matching relational structures and
                not properties between the base domain B and a target domain T. We take any two
                things, analyze the relational structures they contain, and align them to find
                correspondences between them. The properties of objects in the two domains need not
                match, and in fact, if too many properties match, analogy goes away and we have
                literal similarity: <itemizedlist>
                    <listitem>
                        <para xml:id="para-qxk_fkb_hw">Analogy: The hydrogen atom is like our solar
                            system </para>
                    </listitem>
                    <listitem>
                        <para xml:id="para-q21_gkb_hw">Literal Similarity: The X12 star system in
                            the Andromeda galaxy is like our solar system</para>
                    </listitem>
                </itemizedlist></para>
            <para xml:id="para-x3k_3kb_hw" revision="4.0" revisionflag="added" audience="Computing"
                >Structure Mapping theory was implemented in the Structure-Mapping Engine (SME),
                which both formalized the theory and offered a computationally-tractable algorithm
                for carrying out the process of mapping structures and drawing inferences.<footnote
                    label="452" revision="4.0" revisionflag="added" audience="CogSci"
                    xml:id="endnote-0000452">
                    <para xml:id="para-hl2_lkb_hw" audience="CogSci">Structure Mapping theory was
                        proposed in <citation linkend="Gentner1983">(Gentner 1983)</citation>, and
                        the Structure Mapping Engine followed a few years later <citation
                            linkend="Falkenhainer1989" xml:id="cite_Falkenhainer1989">(Falkenhainer
                            et al 1989)</citation>. The SME was criticized for relying on hand-coded
                        knowledge representations, a limitation overcome by <citation
                            xml:id="cite_Turney2008" linkend="Turney2008">(Turney 2008)</citation>,
                        who used text processing techniques to extract the semantic relationships
                        used by Structure Mapping. </para>
                </footnote></para>
        </section>
        <?dbfo clear ?>
        
        <!-- ###################### SECTION ########################################### -->
    </section>
    <!--              -->
    <?need 5cm ?>
    <section xml:id="section-7.6" label="7.6">
        <title>Key Points in Chapter Seven</title>

        <qandaset role="quiz">
            <qandaentry>
                <question>
                    <para audience="CORE" role="interrogative">What are
                        categories?</para>
                </question>
                <answer>
                    <para audience="CORE">Categories are <glossterm
                            linkend="gloss_equivalence_class">equivalence classes</glossterm>: sets
                        or groups of things or abstract entities that we treat the same.</para>
                    <para audience="CORE" role="parenthetical">(See <xref linkend="section-7.2"
                        />)</para>
                </answer>
            </qandaentry>
            <qandaentry>
                <question>
                    <para audience="CORE" role="interrogative">What
                        determines the size of the equivalence class?</para>
                </question>
                <answer>
                    <para audience="CORE">The size of the equivalence class
                        is determined by the properties or characteristics we consider.</para>
                    <para audience="CORE" role="parenthetical">(See <xref
                            linkend="section-7.2"/>)</para>
                </answer>
            </qandaentry>
            <qandaentry>
                <question>
                    <para audience="CORE" role="interrogative">Why do we
                        contrast cultural, individual, and institutional categorization?</para>
                </question>
                <answer>
                    <para audience="CORE">Cultural, individual, and
                        institutional categorization share some core ideas but they emphasize
                        different processes and purposes for creating categories.</para>
                    <para audience="CORE" role="parenthetical">(See <xref
                            linkend="section-7.2"/>)</para>
                </answer>
            </qandaentry>
            <qandaentry>
                <question>
                    <para audience="CORE" role="interrogative">What
                        distinguishes individual categories?</para>
                </question>
                <answer>
                    <para audience="CORE">Individual categories are created
                        by intentional activity that usually takes place in response to a specific
                        situation.</para>
                    <para audience="CORE" role="parenthetical">(See <xref
                            linkend="section-7.2.2"/>)</para>
                </answer>
            </qandaentry>
            <qandaentry>
                <question>
                    <para audience="CORE" role="interrogative">What
                        distinguishes institutional categories?</para>
                </question>
                <answer>
                    <para audience="CORE">Institutional categories are most
                        often created in abstract and information-intensive domains where
                        unambiguous and precise categories are needed.</para>
                    <para audience="CORE" role="parenthetical">(See <xref linkend="section-7.2.3"
                        />)</para>
                </answer>
            </qandaentry>
            <qandaentry>
                <question>
                    <para audience="CORE" role="interrogative">What is the
                        relation between categories and classification?</para>
                </question>
                <answer>
                    <para audience="CORE">The rigorous definition of
                        institutional categories enables <emphasis><glossterm
                                linkend="gloss_classification"
                            >classification</glossterm>,</emphasis> the systematic assignment of
                        resources to categories in an organizing system.</para>
                    <para audience="CORE" role="parenthetical">(See <xref
                            linkend="section-7.2.3"/>)</para>
                </answer>
            </qandaentry>
            <qandaentry>
                <question>
                    <para>When is it necessary to create categories by computational methods rather
                        than by people?</para>
                </question>
                <answer>
                    <para>Computational categories are created by computer programs when the number
                        of resources, or when the number of descriptions or observations associated
                        with each resource, are so large that people cannot think about them
                        effectively. </para>
                    <para>(See <xref linkend="section-7.2.5"/>)</para>
                </answer>
            </qandaentry>
            <qandaentry>
                <question>
                    <para>What is the difference between supervised and unsupervised
                        learning?</para>
                </question>
                <answer>
                    <para>In supervised learning, a machine learning program is trained by giving it
                        sample items or documents that are labeled by category. In unsupervised
                        learning, the program gets the samples but has to come up with the
                        categories on its own.</para>
                    <para>(See <xref linkend="sidebar-SupervisedAndUnsupervised"/>)</para>
                </answer>
            </qandaentry>
            
            <qandaentry>
                <question>
                    <para audience="CORE" role="interrogative">Why does it
                        matter if every resource in a collection has a sortable identifier?</para>
                </question>
                <answer>
                    <para audience="CORE" role="principle">Any collection
                        of resources with sortable identifiers (alphabetic or numeric) as an
                        associated property can benefit from using sorting order as an organizing
                        principle.</para>
                    <para audience="CORE" role="parenthetical">(See <xref
                        linkend="section-7.3.2"/>)</para>
                </answer>
            </qandaentry>
            <qandaentry>
                <question>
                    <para audience="CORE" role="interrogative">What is the
                        concern when only a single property is used to assign category
                        membership?</para>
                </question>
                <answer>
                    <para audience="CORE">If only a single property is used
                        to distinguish among some set of resources and to create the categories in
                        an organizing system, the choice of property is critical because different
                        properties often lead to different categories.</para>
                    <para audience="CORE" role="parenthetical">(See <xref
                            linkend="section-7.3.2"/>)</para>
                </answer>
            </qandaentry>
            <qandaentry>
                <question>
                    <para audience="CORE" role="interrogative">What is a
                        hierarchical category system?</para>
                </question>
                <answer>
                    <para audience="CORE">A sequence of organizing
                        decisions based on a fixed ordering of resource properties creates a
                            <emphasis>hierarchy</emphasis>, a multi-level category system.</para>
                    <para audience="CORE" role="parenthetical">(See <xref
                            linkend="section-7.3.3.1"/>)</para>
                </answer>
            </qandaentry>
            <qandaentry>
                <question>
                    <para audience="CORE" role="interrogative">What can one
                        say about any member of a classical category in terms of how it represents
                        the category?</para>
                </question>
                <answer>
                    <para audience="CORE">An important implication of
                        necessary and sufficient category definition is that every member of the
                        category is an equally good member or example of the category.</para>
                    <para audience="CORE" role="parenthetical">(See <xref
                        linkend="section-7.3.3.3"/>)</para>
                </answer>
            </qandaentry>
            <qandaentry>
                <question>
                    <para audience="CORE" role="interrogative">What is
                        aboutness?</para>
                </question>
                <answer>
                    <para audience="CORE"><phrase role="statement">For most
                            purposes, the most useful property of information resources for
                            categorizing them is their <glossterm linkend="gloss_aboutness"
                                >aboutness</glossterm>, which is not directly perceivable and which
                            is hard to characterize.</phrase></para>
                    <para audience="CORE" role="parenthetical">(See <xref
                            linkend="section-7.3.4"/>)</para>
                </answer>
            </qandaentry>
            
            <qandaentry>
                <question>
                    <para>When it is necessary to adopt a probabilistic or statistical view of
                        properties in defining categories?</para>
                </question>
                <answer>
                    <para>In domains where properties lack one or more of the characteristics of
                        separability, perceptibility, and necessity, a probabilistic or statistical
                        view of properties is needed to define categories.</para>
                    <para>(See <xref linkend="section-7.3.5"/>)</para>
                </answer>
            </qandaentry>
            
            <qandaentry>
                <question>
                    <para audience="CORE" role="interrogative">What is
                        family resemblance?</para>
                </question>
                <answer>
                    <para audience="CORE"><phrase>Sharing some but not all
                            properties is akin to <emphasis>family resemblances</emphasis> among the
                            category members.</phrase></para>
                    <para audience="CORE" role="parenthetical">(See <xref
                            linkend="section-7.3.5"/>)</para>
                </answer>
            </qandaentry>
            <qandaentry>
                <question>
                    <para audience="CORE" role="interrogative">What is
                        similarity?</para>
                </question>
                <answer>
                    <para audience="CORE"><emphasis>Similarity</emphasis>
                        is a measure of the resemblance between two things that share some
                        characteristics but are not identical. </para>
                    <para audience="CORE" role="parenthetical">(See <xref
                            linkend="section-7.3.6"/>)</para>
                </answer>
            </qandaentry>
            
            <qandaentry>
                <question>
                    <para>What are the four psychologically-motivated approaches that propose
                        different functions for computing similarity?</para>
                </question>
                <answer>
                    <para>Feature- or property-based, geometry-based, transformational, and
                        alignment- or analogy-based approaches are psychologically-motivated
                        approaches that propose different functions for computing similarity. </para>
                    <para>(See <xref linkend="section-7.3.6"/>)</para>
                </answer>
            </qandaentry>

            <qandaentry>
                <question>
                    <para audience="CORE" role="interrogative">What are
                        so-called <quote>classical categories</quote>?</para>
                </question>
                <answer>
                    <para audience="CORE">Classical categories can be
                        defined precisely with just a few <glossterm
                            linkend="gloss_sufficiency_necessity">necessary and
                            sufficient</glossterm> properties.</para>
                    <para audience="CORE" role="parenthetical">(See <xref
                            linkend="section-7.4.2"/>)</para>
                </answer>
            </qandaentry>
            <qandaentry>
                <question>
                    <para audience="CORE" role="interrogative">How does the
                        breadth of a category affect the recall/precision tradeoff?</para>
                </question>
                <answer>
                    <para audience="CORE">Broader or coarse-grained
                        categories increase <emphasis>recall</emphasis>, but lower
                            <emphasis>precision.</emphasis></para>
                    <para audience="CORE" role="parenthetical">(See <xref
                            linkend="section-7.4.3"/>)</para>
                </answer>
            </qandaentry>

            <qandaentry>
                <question>
                    <para audience="CORE" role="interrogative">What is a
                        decision tree?</para>
                </question>
                <answer>
                    <para audience="CORE"><xref linkend="def_decision_tree"
                            endterm="def_decision_tree" role="transclusion"/></para>
                    <para audience="CORE" role="parenthetical">(See <xref linkend="section-7.5.2"
                        />)</para>
                </answer>
            </qandaentry>
            <qandaentry>
                <question>
                    <para audience="CORE" role="interrogative">What is the
                        practical benefit of defining categories according to necessary and
                        sufficient features?</para>
                </question>
                <answer>
                    <para audience="CORE">The most conceptually simple and
                        straightforward implementation of categories in technologies for organizing
                        systems adopts the classical view of categories based on necessary and
                        sufficient features.</para>
                    <para audience="CORE" role="parenthetical">(See <xref
                            linkend="section-7.5.2"/>)</para>
                </answer>
            </qandaentry>
            
      <!-- NEW KEY POINT -->
<qandaentry audience="CORE" revision="3.0" revisionflag="added">
<question>
<para audience="CORE" role="interrogative">How do artificial languages like mathematical notation and programming languages enable precise specification of categories?
</para>
</question>
<answer><para audience="CORE">An artificial language expresses ideas concisely by introducing new terms or symbols that
                        represent complex ideas along with syntactic mechanisms for combining and
                        operating on them. </para>
                    <para audience="CORE" role="parenthetical">(See <xref linkend="section-7.5.2"
                        />)</para></answer>
</qandaentry>
            
            <qandaentry>
                <question>
                    <para>How do Naïve Bayes classifiers learn?</para>
                </question>
                <answer>
                    <para>Naïve Bayes classifiers learn by revising the conditional probability of
                        each property for making the correct classification after seeing the base
                        rates of the class and property in the training data and how likely it is
                        that a member of the class has the property.</para>
                    <para>(See <xref linkend="section-7.5.3.2"/>)</para>
                </answer>
            </qandaentry>
            
            <qandaentry>
                <question>
                    <para>How do clustering techniques create categories?</para>
                </question>
                <answer>
                    <para>Because clustering techniques are unsupervised, they create categories
                        based on calculations of similarity between resources, maximizing the
                        similarity of resources within a category and maximizing the differences
                        between them.</para>
                    <para>(See <xref linkend="section-7.5.3.3"/>)</para>
                </answer>
            </qandaentry>
            
        </qandaset>
        
        <!-- ###################### CHAPTER ########################################### -->
    </section>
</chapter>
